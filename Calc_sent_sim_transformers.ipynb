{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56d77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script computes pairwise sentence similarities based on transformer embeddings\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "#\n",
    "# Requires first generating the sentence embeddings using the 'Calc_embeds_transformers.ipynb' file.\n",
    "# This code generates files with a similarity score between each pair of sentences, one for file per model.\n",
    "# The file contains a single similarity score (-1 to 1) on each line.\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')\n",
    "sns.set()\n",
    "\n",
    "# load custom functions from library\n",
    "sys.path.insert(0,'D:\\\\My Code\\\\Python\\\\2022_06 Compositional Semantics Paper')\n",
    "sys.path.insert(0,'D:\\\\My Code\\\\Python\\\\2023_02 fMRI RSA Analysis')\n",
    "import sentence_embeds_processing as sep\n",
    "import voxels_processing as vp\n",
    "\n",
    "# file containing path directories\n",
    "with open(\"D:\\\\My Code\\\\Python\\\\2023_02 fMRI RSA Analysis\\\\file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ee3389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sentence_embeds_processing' from 'D:\\\\My Code\\\\Python\\\\2022_06 Compositional Semantics Paper\\\\sentence_embeds_processing.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cba17b5",
   "metadata": {},
   "source": [
    "### Load sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3280faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 Wehbe_neuro\n",
      "1 Anderson_neuro\n",
      "2 Pereira243_neuro\n",
      "3 Pereira384_neuro\n",
      "4 Alice_neuro\n",
      "5 Zhang_neuro\n",
      "6 Zhang_neuro_rev\n",
      "7 Zhang_neuro_8s\n",
      "8 Fodor2024-final108_neuro\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5206758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded Zhang_neuro_rev with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set \n",
    "\n",
    "# choose number from those printed above\n",
    "dataset_name = datasets[6]\n",
    "\n",
    "# load sentence set into dictionary depending on type\n",
    "if pairs == True:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['data_pairs_path'], pairs)\n",
    "else:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict[dataset_name+'-stim'], pairs)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset_name,'with',n,'sentences')\n",
    "\n",
    "# store in list\n",
    "sentences = []\n",
    "if pairs==True: # use this for sentence similarity pair data\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,0].flatten()))\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,1].flatten()))\n",
    "else: # use this for neuroimaging data/list of sentences\n",
    "    sentences = list(sentences_dict.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4c503e",
   "metadata": {},
   "source": [
    "### Compute model similarities (for paired datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c90951",
   "metadata": {},
   "source": [
    "This code is used for experimental sentence datasets, which have lists of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "824e9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute sentence similarities using pre-computed embeddings\n",
    "\n",
    "# Compute similarities for all sentence pairs in dataset\n",
    "sim_storage = {}\n",
    "for comp_model in vp.model_name_dict.keys():\n",
    "\n",
    "    # load pre-computed sentence embeddings for relevant dataset\n",
    "    comp_model_norml = comp_model+'_norml'\n",
    "    sim_storage[comp_model] = np.array([])\n",
    "    sim_storage[comp_model_norml] = np.array([])\n",
    "    try:\n",
    "        sentences_a = np.loadtxt(file_paths_dict['embeddings_path']+dataset_name+'_a_'+comp_model+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "        sentences_b = np.loadtxt(file_paths_dict['embeddings_path']+dataset_name+'_b_'+comp_model+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "    except OSError:\n",
    "        continue\n",
    "    \n",
    "    # normalise embeddings\n",
    "    sentences_a_norml = sep.normalise_embeddings(sentences_a)\n",
    "    sentences_b_norml = sep.normalise_embeddings(sentences_b)\n",
    "    \n",
    "    # compute and store similarities\n",
    "    for sent_id in sentences_dict.keys():\n",
    "        pair_sim = sep.cosine_sim(sentences_a[sent_id-1],sentences_b[sent_id-1])\n",
    "        pair_sim_norml = sep.cosine_sim(sentences_a_norml[sent_id-1],sentences_b_norml[sent_id-1])\n",
    "        sim_storage[comp_model] = np.append(sim_storage[comp_model],pair_sim)\n",
    "        sim_storage[comp_model_norml] = np.append(sim_storage[comp_model_norml],pair_sim_norml)\n",
    "        \n",
    "    # save similarities\n",
    "    np.savetxt(dataset_name+'_'+comp_model+'_norml_similarities.txt', sim_storage[comp_model_norml], fmt='%f')\n",
    "    np.savetxt(dataset_name+'_'+comp_model+'_similarities.txt', sim_storage[comp_model], fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute 'vocab similarity' for paired datasets (number of words shared by the two sentences)\n",
    "\n",
    "# get stop words\n",
    "stop_words = np.loadtxt(file_paths_dict['stop_words_path'], dtype='str') # list of stop words\n",
    "\n",
    "# loop over all word pairs\n",
    "word_overlaps = []\n",
    "for sent_id in sentences_dict.keys():\n",
    "    sentence_1 = sentences_dict[sent_id][0]\n",
    "    sentence_2 = sentences_dict[sent_id][1]\n",
    "    token_list_1 = sep.tokenise_sentence(sentence_1, stop_words)\n",
    "    token_list_2 = sep.tokenise_sentence(sentence_2, stop_words)\n",
    "    \n",
    "    shared_words = set(token_list_1) & set(token_list_2)\n",
    "    all_words = list(set(token_list_1+token_list_2))\n",
    "    word_overlap = len(shared_words)/len(all_words)\n",
    "    word_overlaps.append(word_overlap)\n",
    "\n",
    "# save similarities to file\n",
    "np.savetxt(dataset_name+'_vocab_similarities.txt', np.array(word_overlaps), fmt='%f')\n",
    "print('saved vocab similarities')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef1cfc23",
   "metadata": {},
   "source": [
    "### Compute model similarities (for neuro datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efd78f",
   "metadata": {},
   "source": [
    "This code is used for neuroimaging sentence datasets, which have a list of single sentences. The code therefore computes the pairwise similarity between each unique pairing of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74d9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute sentence similarities using pre-computed embeddings\n",
    "sim_storage = {}\n",
    "sent_id_pairs = list(itertools.combinations(np.arange(n), 2)) # pairs of sentences\n",
    "\n",
    "# Add storage elements for functions to be used\n",
    "# comp_models = ['mean','random','glove6b','mean_inverted','mult','conv','ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart',\n",
    "#              'ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart','dictbert','S3BERT','UAE']\n",
    "comp_models = ['mean','sentbert_mpnet','defsent_cls','amrbart','UAE']\n",
    "sentence_embeds_dict = {}\n",
    "sentence_embeds_norm_dict = {}\n",
    "\n",
    "# Compute similarities for all sentence pairs in dataset\n",
    "for comp_model in comp_models:\n",
    "    \n",
    "    # load pre-computed sentence embeddings for relevant dataset\n",
    "    comp_model_norml = comp_model+'_norml'\n",
    "    sim_storage[comp_model] = np.array([])\n",
    "    sim_storage[comp_model_norml] = np.array([])\n",
    "    try:\n",
    "        sentence_embeds = np.loadtxt(file_paths_dict['embeddings_path']+dataset_name+'_'+comp_model+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "        sentence_embeds_dict[comp_model] = sentence_embeds\n",
    "    except OSError:\n",
    "        print('Error loading',comp_model)\n",
    "        continue\n",
    "    \n",
    "    # normalise embeddings\n",
    "    sentence_embeds_norml = sep.normalise_embeddings(sentence_embeds)\n",
    "    sentence_embeds_norm_dict[comp_model] = sentence_embeds_norml\n",
    "    np.savetxt(dataset_name+'_'+comp_model+'_norml_embeddings.txt', sentence_embeds_norm_dict[comp_model], fmt='%f')\n",
    "    \n",
    "    # compute and store similarities\n",
    "    for sent_id_pair in sent_id_pairs:\n",
    "        pair_sim = sep.cosine_sim(sentence_embeds[sent_id_pair[0]],sentence_embeds[sent_id_pair[1]])\n",
    "        pair_sim_norml = sep.cosine_sim(sentence_embeds_norml[sent_id_pair[0]],sentence_embeds_norml[sent_id_pair[1]])\n",
    "        sim_storage[comp_model] = np.append(sim_storage[comp_model],pair_sim)\n",
    "        sim_storage[comp_model_norml] = np.append(sim_storage[comp_model_norml],pair_sim_norml)\n",
    "        \n",
    "    # save similarities\n",
    "    np.savetxt(dataset_name+'_'+comp_model+'_similarities.txt', sim_storage[comp_model], fmt='%f')\n",
    "    np.savetxt(dataset_name+'_'+comp_model+'_norml_similarities.txt', sim_storage[comp_model_norml], fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e0e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save random embeddings and sims\n",
    "sentence_embeds_flat = sentence_embeds_norml.flatten()\n",
    "sentence_embeds_flat_suffle = sentence_embeds_flat.copy()\n",
    "random.shuffle(sentence_embeds_flat_suffle)\n",
    "sentence_embeds_shuffle = sentence_embeds_flat_suffle.reshape(sentence_embeds_norml.shape)\n",
    "np.savetxt(dataset_name+'_mean_random_embeddings.txt', sentence_embeds_shuffle, fmt='%f')\n",
    "\n",
    "random_sims = np.array([])\n",
    "for sent_id_pair in sent_id_pairs:\n",
    "    pair_sim = sep.cosine_sim(sentence_embeds_shuffle[sent_id_pair[0]],sentence_embeds_shuffle[sent_id_pair[1]])\n",
    "    random_sims = np.append(random_sims,pair_sim)\n",
    "np.savetxt(dataset_name+'_mean_random_similarities.txt', random_sims, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac8fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
