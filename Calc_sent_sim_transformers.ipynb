{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56d77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script computes pairwise sentence similarities based on transformer embeddings\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "#\n",
    "# Requires first generating the sentence embeddings using the 'Calc_embeds_transformers.ipynb' file.\n",
    "# This code generates files with a similarity score between each pair of sentences, one for file per model.\n",
    "# The file contains a single similarity score (-1 to 1) on each line.\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# base path for all data files\n",
    "path_root = \"D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\\"\n",
    "data_pairs_path = 'Sentence Similarity Data\\\\Sentence Similarities Final\\\\'\n",
    "data_nonpaired_path = 'Neuroimaging Data\\\\'\n",
    "embeddings_path = \"Analysis Results\\Sentence Embeddings\\\\\"\n",
    "sims_path = 'Analysis Results\\Sentence Similarities\\\\'\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cba17b5",
   "metadata": {},
   "source": [
    "### Load sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3280faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 2014 Wehbe\\Stimuli\\Chapter_9_sentences_final\n",
      "1 2017 Anderson\\Stimuli\\stimuli_final\n",
      "2 2018 Pereira\\Stimuli\\stimuli_243sentences\n",
      "3 2018 Pereira\\Stimuli\\stimuli_384sentences\n",
      "4 2020 Alice Dataset\\Stimuli\\stimuli_sentences_final\n",
      "5 2020 Zhang\\Stimuli\\test_sentences_final\n",
      "6 2023 Fodor Dataset\\Fodor2023-final240\n",
      "7 2023 Fodor Dataset\\Fodor2023-final192\n",
      "8 2023 Fodor Dataset\\Fodor2023-prelim\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc2b4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded 2020 Zhang\\Stimuli\\test_sentences_final with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set (choose number from those printed above)\n",
    "dataset = datasets[5]\n",
    "sentences_dict = sep.load_set_of_sentences(dataset, path_root+data_pairs_path, path_root+data_nonpaired_path, pairs)\n",
    "full_dataset_name = sep.fix_sentence_dataset_name(dataset)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset,'with',n,'sentences')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4c503e",
   "metadata": {},
   "source": [
    "### Compute model similarities (for paired datasets, run once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c90951",
   "metadata": {},
   "source": [
    "This code is used for experimental sentence datasets, which have lists of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824e9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute sentence similarities using pre-stored embeddings\n",
    "\n",
    "# Specify compositional functions to be examined\n",
    "sim_funcs = ['mean','mult','conv','ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart',\n",
    "             'ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart']\n",
    "\n",
    "# Compute similarities for all sentence pairs in dataset\n",
    "sim_storage = {}\n",
    "for comp_func in sim_funcs:\n",
    "\n",
    "    # load pre-computed sentence embeddings for relevant dataset\n",
    "    comp_func_norml = comp_func+'_norml'\n",
    "    sim_storage[comp_func] = np.array([])\n",
    "    sim_storage[comp_func_norml] = np.array([])\n",
    "    try:\n",
    "        sentences_a = np.loadtxt(path_root+embeddings_path+dataset+'_a_'+comp_func+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "        sentences_b = np.loadtxt(path_root+embeddings_path+dataset+'_b_'+comp_func+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "    except OSError:\n",
    "        continue\n",
    "    \n",
    "    # normalise embeddings\n",
    "    sentences_a_norml = sep.normalise_embeddings(sentences_a)\n",
    "    sentences_b_norml = sep.normalise_embeddings(sentences_b)\n",
    "    \n",
    "    # compute and store similarities\n",
    "    for sent_id in sentences_dict.keys():\n",
    "        pair_sim = sep.cosine_sim(sentences_a[sent_id-1],sentences_b[sent_id-1])\n",
    "        pair_sim_norml = sep.cosine_sim(sentences_a_norml[sent_id-1],sentences_b_norml[sent_id-1])\n",
    "        sim_storage[comp_func] = np.append(sim_storage[comp_func],pair_sim)\n",
    "        sim_storage[comp_func_norml] = np.append(sim_storage[comp_func_norml],pair_sim_norml)\n",
    "        \n",
    "    # save similarities\n",
    "    np.savetxt(dataset+'_'+comp_func+'_norml_similarities.txt', sim_storage[comp_func_norml], fmt='%f')\n",
    "    np.savetxt(dataset+'_'+comp_func+'_similarities.txt', sim_storage[comp_func], fmt='%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef1cfc23",
   "metadata": {},
   "source": [
    "### Compute model similarities (neuro data, run once)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efd78f",
   "metadata": {},
   "source": [
    "This code is used for neuroimaging sentence datasets, which have a list of single sentences. The code therefore computes the pairwise similarity between each unique pairing of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74d9113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Code\\Python\\2022_06 Compositional Semantics Paper\\sentence_embeds_processing.py:248: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  new_embeddings = np.transpose(new_mean_np/std_np)\n",
      "d:\\My Code\\Python\\2022_06 Compositional Semantics Paper\\sentence_embeds_processing.py:108: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = np.dot(embed_1,embed_2)/(np.linalg.norm(embed_1)*np.linalg.norm(embed_2))\n"
     ]
    }
   ],
   "source": [
    "## Compute and save similarities for sentence experimental similarities\n",
    "sim_storage = {}\n",
    "sent_id_pairs = list(itertools.combinations(sentences_dict.keys(), 2)) # pairs of sentences\n",
    "\n",
    "# Add storage elements for functions to be used\n",
    "sim_funcs = ['mean','mult','conv','ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart',\n",
    "             'ernie_0','ernie_5','ernie_12','infersent','universal','sentbert','sentbert_mpnet','openai','defsent_mean','defsent_cls','amrbart']\n",
    "# sim_funcs = ['sentbert_mpnet']\n",
    "sentence_embeds_dict = {}\n",
    "sentence_embeds_norm_dict = {}\n",
    "\n",
    "# Compute similarities for all sentence pairs in dataset\n",
    "for comp_func in sim_funcs:\n",
    "    \n",
    "    # load pre-computed sentence embeddings for relevant dataset\n",
    "    comp_func_norml = comp_func+'_norml'\n",
    "    sim_storage[comp_func] = np.array([])\n",
    "    sim_storage[comp_func_norml] = np.array([])\n",
    "    try:\n",
    "        sentence_embeds = np.loadtxt(path_root+embeddings_path+full_dataset_name+'_'+comp_func+'_embeddings.txt',  delimiter=' ', dtype='float', encoding='utf-8')\n",
    "        sentence_embeds_dict[comp_func] = sentence_embeds\n",
    "    except OSError:\n",
    "        continue\n",
    "    \n",
    "    # normalise embeddings\n",
    "    sentence_embeds_norml = sep.normalise_embeddings(sentence_embeds)\n",
    "    sentence_embeds_norm_dict[comp_func] = sentence_embeds_norml\n",
    "    \n",
    "    # compute and store similarities\n",
    "    for sent_id_pair in sent_id_pairs:\n",
    "        pair_sim = sep.cosine_sim(sentence_embeds[sent_id_pair[0]-1],sentence_embeds[sent_id_pair[1]-1])\n",
    "        pair_sim_norml = sep.cosine_sim(sentence_embeds_norml[sent_id_pair[0]-1],sentence_embeds_norml[sent_id_pair[1]-1])\n",
    "        sim_storage[comp_func] = np.append(sim_storage[comp_func],pair_sim)\n",
    "        sim_storage[comp_func_norml] = np.append(sim_storage[comp_func_norml],pair_sim_norml)\n",
    "        \n",
    "    # save similarities\n",
    "    np.savetxt(full_dataset_name+'_'+comp_func+'_similarities.txt', sim_storage[comp_func], fmt='%f')\n",
    "    np.savetxt(full_dataset_name+'_'+comp_func+'_norml_similarities.txt', sim_storage[comp_func_norml], fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2bb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
