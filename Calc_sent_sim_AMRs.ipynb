{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script computes the Smatch, WWLK, and AMR similarity between the AMR parses of two sentences \n",
    "# Run using spring environment with Python 3.8\n",
    "# James Fodor 2023\n",
    "#\n",
    "# This code requires the input sentences have been parsed using the 'Calc_parse_AMRs.ipynb' file.\n",
    "# Three types of similarities are computed:\n",
    "# 1. Smatch similarity: a standard graph similarity measure\n",
    "# 2. AMR-sim: this is a custom measure developed for this project, see Fodor 2023 for details.\n",
    "# 3. WLK-Wasser: incorporates both graph distance and graded word similarity, \n",
    "#                see https://github.com/flipz357/weisfeiler-leman-amr-metrics/blob/main/README.md\n",
    "\n",
    "\n",
    "# load libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import itertools\n",
    "import sentence_embeds_processing as sep\n",
    "\n",
    "from amrlib.evaluate.smatch_enhanced import match_pair # for smatch score\n",
    "\n",
    "\n",
    "# base path for all data files\n",
    "path_root = \"D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\\"\n",
    "data_pairs_path = path_root+'\\\\Sentence Similarity Data\\\\Sentence Similarities Final\\\\'\n",
    "data_nonpaired_path = path_root+'\\\\Neuroimaging Data\\\\'\n",
    "sims_path = 'Analysis Results\\Sentence Similarities\\\\'\n",
    "parses_path = 'Analysis Results\\Sentence Parses\\\\'\n",
    "\n",
    "# load nltk lemmatizer\n",
    "from nltk.data import path\n",
    "path.append(path_root+\"\\Frames and Structured Data\\\\FrameNet\\\\nltk_data\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentence datasets, word embeddings, and AMR parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 2014 Wehbe\\Stimuli\\Chapter_9_sentences_final\n",
      "1 2017 Anderson\\Stimuli\\stimuli_final\n",
      "2 2018 Pereira\\Stimuli\\stimuli_243sentences\n",
      "3 2018 Pereira\\Stimuli\\stimuli_384sentences\n",
      "4 2020 Alice Dataset\\Stimuli\\stimuli_sentences_final\n",
      "5 2020 Zhang\\Stimuli\\test_sentences_final\n",
      "6 2023 Fodor Dataset\\Fodor2023-final240\n",
      "7 2023 Fodor Dataset\\Fodor2023-final192\n",
      "8 2023 Fodor Dataset\\Fodor2023-prelim\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired sentences data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded 2020 Zhang\\Stimuli\\test_sentences_final with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set (choose number from those printed above)\n",
    "dataset = datasets[5]\n",
    "sentences_dict = sep.load_set_of_sentences(dataset, data_pairs_path, data_nonpaired_path, pairs)\n",
    "full_dataset_name = sep.fix_sentence_dataset_name(dataset)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset,'with',n,'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load word embeddings and AMR parses\n",
    "\n",
    "# Load ConceptNet embeddings\n",
    "model_address = path_root+'\\Word Embeddings\\ConceptNet Embeddings\\\\numberbatch-en.txt'\n",
    "conceptnet_embeds = sep.import_word_model(model_address)\n",
    "\n",
    "# Load AMR parsed sentence data\n",
    "with open(path_root+parses_path+'\\AMR Parsing\\\\'+full_dataset_name+\"_AMR_parse.json\",'r') as file:\n",
    "    AMR_graph_storage = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and save AMR and Smatch similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to perform AMR parsing and calculate AMR similarity\n",
    "\n",
    "# Function to convert a string parse tree of a sentence to a dictionary of roles and values\n",
    "def parse_tree_to_dict(parse_tree): \n",
    "    # extract parse tree only\n",
    "    parse = parse_tree.split('.')[1] # uses the period in the sentence to separate it from the parse tree\n",
    "    parse_list = parse.split('\\n')\n",
    "    parse_list[1]=':MAIN '+parse_list[1]\n",
    "\n",
    "    # construct a list of parse tree terms\n",
    "    new_parse_list = []\n",
    "    tab_length = 6 # length of indentation in parse tree\n",
    "    last_element_of_level_n = {0:'MAIN'}\n",
    "    for line in parse_list:\n",
    "        # skip any blank lines\n",
    "        if line=='': \n",
    "            continue\n",
    "        \n",
    "        # extract terms\n",
    "        leading_ws = line.split(':')[0] # get leading white space\n",
    "        line_no_ws = line.split(':')[1] # get terms following ws\n",
    "        \n",
    "        # calculate current level in parse tree\n",
    "        if len(leading_ws)%tab_length != 0:\n",
    "            list_level = 0\n",
    "        else:\n",
    "            list_level = int(len(leading_ws)/6) \n",
    "        \n",
    "        # extract list item at current parse tree level\n",
    "        header_string = ''\n",
    "        for i in range(0,list_level):\n",
    "            header_string = header_string+'.'+last_element_of_level_n[i]\n",
    "        header_string = header_string+'.'\n",
    "        new_parse_list.append(str(header_string)+line_no_ws)\n",
    "        \n",
    "        # append parse tree item to list\n",
    "        last_element_of_level_n[list_level] = line_no_ws.split(' ')[0]\n",
    "        \n",
    "    # turn parse tree list into dictionary\n",
    "    parse_tree_dict = {}\n",
    "    for item in new_parse_list:\n",
    "        item_list = item.split(' ')\n",
    "        parse_tree_dict[item_list[0]] = item_list[-1]\n",
    "        \n",
    "    return parse_tree_dict\n",
    "\n",
    "\n",
    "# Function to calculate AMR parse similarity\n",
    "def AMR_graph_sim(graphs_pair):\n",
    "    semantic_filler_sims = {}\n",
    "    graph_dict_1 = parse_tree_to_dict(graphs_pair[0])\n",
    "    graph_dict_2 = parse_tree_to_dict(graphs_pair[1])\n",
    "    total_num_roles = len(graph_dict_1.keys())+len(graph_dict_2.keys())\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for role in graph_dict_1.keys():\n",
    "        if role in graph_dict_2.keys():\n",
    "            filler_1 = graph_dict_1[role].strip('\\')') # get filler for role\n",
    "            filler_2 = graph_dict_2[role].strip('\\')')\n",
    "            \n",
    "            if filler_1.find('-'):\n",
    "                filler_1 = filler_1.split('-')[0] # remove sense number for verbs and lemmatise\n",
    "                filler_1 = lemmatizer.lemmatize(filler_1,'v')\n",
    "            if filler_2.find('-'):\n",
    "                filler_2 = filler_2.split('-')[0]\n",
    "                filler_2 = lemmatizer.lemmatize(filler_2,'v')\n",
    "            \n",
    "            try: # when embeddings are available\n",
    "                word_embedding_1 = conceptnet_embeds[filler_1] # get embeddings for filler\n",
    "                word_embedding_2 = conceptnet_embeds[filler_2]\n",
    "                semantic_filler_sims[(role,filler_1,filler_2)] = sep.cosine_sim(word_embedding_1, word_embedding_2) # store in dict\n",
    "            except KeyError: # if the word doesn't have embeddings\n",
    "                if filler_1==filler_2:\n",
    "                    semantic_filler_sims[(role,filler_1,filler_2)] = 1\n",
    "                else:\n",
    "                    semantic_filler_sims[(role,filler_1,filler_2)] = 0\n",
    "        \n",
    "    overall_sim = 2*np.sum(list(semantic_filler_sims.values()))/total_num_roles # average rolewise similarity\n",
    "    return (overall_sim,semantic_filler_sims)\n",
    "\n",
    "\n",
    "# Calculate smatch similarity\n",
    "def smatch_sim(graph_pair):\n",
    "    out = match_pair((graph_pair[0].split('.')[1],graph_pair[1].split('.')[1]))\n",
    "    return out[0]/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "## Compute AMR and Smatch similarities.\n",
    "# Note: If this code causes errors its almost certainly because one or more sentences don't end with a full stop.\n",
    "\n",
    "# Prepare storage array for sentence similarities\n",
    "sim_storage = {}\n",
    "sim_funcs = ['AMR','smatch']\n",
    "for function in sim_funcs:\n",
    "    sim_storage[function] = np.array([])\n",
    "    \n",
    "# Compute similarities for all sentence pairs in dataset\n",
    "if pairs==True:\n",
    "    for sentence_id in AMR_graph_storage.keys():\n",
    "        AMR_parse_trees = AMR_graph_storage[sentence_id][2:]\n",
    "        \n",
    "        # add periods after a sentence if needed\n",
    "        if AMR_parse_trees[0].find('.')==-1: \n",
    "            AMR_parse_trees[0] = AMR_parse_trees[0].replace('\\n','.\\n',1)\n",
    "        if AMR_parse_trees[1].find('.')==-1:\n",
    "            AMR_parse_trees[1] = AMR_parse_trees[1].replace('\\n','.\\n',1)\n",
    "        \n",
    "        # compute and store similarities\n",
    "        sim_storage['AMR'] = np.append(sim_storage['AMR'],AMR_graph_sim(AMR_parse_trees)[0])\n",
    "        sim_storage['smatch'] = np.append(sim_storage['smatch'],smatch_sim(AMR_parse_trees))\n",
    "\n",
    "# Compute similarities for all pairwise comparisons for lists of sentences (neuro)\n",
    "elif pairs==False:\n",
    "    sent_id_pairs = list(itertools.combinations(AMR_graph_storage.keys(), 2))\n",
    "    for sentence_id_pair in sent_id_pairs:\n",
    "        sent_id_1 = sentence_id_pair[0]\n",
    "        sent_id_2 = sentence_id_pair[1]\n",
    "        AMR_parse_tree_1 = AMR_graph_storage[sent_id_1][1][0]\n",
    "        AMR_parse_tree_2 = AMR_graph_storage[sent_id_2][1][0]\n",
    "        AMR_parse_trees = [AMR_parse_tree_1,AMR_parse_tree_2]\n",
    "        sim_storage['AMR'] = np.append(sim_storage['AMR'],AMR_graph_sim(AMR_parse_trees)[0])\n",
    "        sim_storage['smatch'] = np.append(sim_storage['smatch'],smatch_sim(AMR_parse_trees))\n",
    "    \n",
    "# Save results\n",
    "for sim_type in sim_funcs:\n",
    "    np.savetxt(full_dataset_name+'_'+sim_type+'_similarities.txt', sim_storage[sim_type], fmt='%f')\n",
    "print('saved')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and save WLK Wasser similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare set of sentence pairs AMR graphs (only run when using neuro data)\n",
    "if pairs==False:\n",
    "    graph_pair_list = []\n",
    "    sent_id_pairs = list(itertools.combinations(AMR_graph_storage.keys(), 2))\n",
    "    for index,id_pair in enumerate(sent_id_pairs):\n",
    "        graph_1 = AMR_graph_storage[id_pair[0]][1][0].replace('::snt', '::snt-'+str(index+1))+'\\n' # need to adjust naming\n",
    "        graph_2 = AMR_graph_storage[id_pair[1]][1][0].replace('::snt', '::snt-'+str(index+1))+'\\n' \n",
    "        graph_pair_list.append([graph_1,graph_2])\n",
    "        \n",
    "    WLK_sent_a_set = np.array(graph_pair_list)[:,0]\n",
    "    WLK_sent_b_set = np.array(graph_pair_list)[:,1]\n",
    "\n",
    "    np.savetxt(full_dataset_name+\"_WLK_sent_a_AMR_parse.txt\", WLK_sent_a_set, fmt='%s')\n",
    "    np.savetxt(full_dataset_name+\"_WLK_sent_b_AMR_parse.txt\", WLK_sent_b_set, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zhang_neuro_WLK_sent_a_AMR_parse.txt',\n",
       " 'Zhang_neuro_WLK_sent_b_AMR_parse.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get filenames for the code in the cell below (first two .txt files)\n",
    "glob.glob('*_AMR_parse.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLK sim:\n"
     ]
    }
   ],
   "source": [
    "# Compute WLK similarity of graphs\n",
    "# Use the filenames printed above to adjust the call below.\n",
    "# Note: the two .txt files containing AMR parses need to have a sent id for each (e.g. ::snt-1)\n",
    "\n",
    "print('WLK sim:')\n",
    "!python \"AMR_wlk_wasser\\main_wlk_wasser.py\" -a \"Zhang_neuro_WLK_sent_a_AMR_parse.txt\" -b \"Zhang_neuro_WLK_sent_b_AMR_parse.txt\" > \"Zhang_neuro_WLK_Wasser_similarities.txt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd19f865deb76414f800cd5e170dbd2fd2287196326fe3642087e61bd5d12a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
