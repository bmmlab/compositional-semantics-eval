{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code computes S3BERT embeddings and saves similarities between sentence pairs\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "# \n",
    "# Requires S3BERT code from https://github.com/flipz357/S3BERT\n",
    "\n",
    "# load libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# base path for all data files\n",
    "path_root = \"D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\\"\n",
    "data_pairs_path = path_root+'\\\\Sentence Similarity Data\\\\Sentence Similarities Final\\\\'\n",
    "data_nonpaired_path = path_root+'\\\\Neuroimaging Data\\\\'\n",
    "embeddings_path = \"Analysis Results\\Sentence Embeddings\\\\\"\n",
    "sims_path = 'Analysis Results\\Sentence Similarities\\\\'\n",
    "\n",
    "# load S3BERT code\n",
    "sys.path.append(\"./S3BERT_main/\")\n",
    "import config\n",
    "import prediction_helpers as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 2014 Wehbe\\Stimuli\\Chapter_9_sentences_final\n",
      "1 2017 Anderson\\Stimuli\\stimuli_final\n",
      "2 2018 Pereira\\Stimuli\\stimuli_243sentences\n",
      "3 2018 Pereira\\Stimuli\\stimuli_384sentences\n",
      "4 2020 Alice Dataset\\Stimuli\\stimuli_sentences_final\n",
      "5 2020 Zhang\\Stimuli\\test_sentences_final\n",
      "6 2023 Fodor Dataset\\Fodor2023-final240\n",
      "7 2023 Fodor Dataset\\Fodor2023-final192\n",
      "8 2023 Fodor Dataset\\Fodor2023-prelim\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded 2020 Zhang\\Stimuli\\test_sentences_final with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set (choose number from those printed above)\n",
    "dataset = datasets[5]\n",
    "sentences_dict = sep.load_set_of_sentences(dataset, data_pairs_path, data_nonpaired_path, pairs)\n",
    "full_dataset_name = sep.fix_sentence_dataset_name(dataset)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset,'with',n,'sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for paired (behavioural) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used for experimental sentence datasets, which have lists of sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute similarities for experimental datasets\n",
    "\n",
    "# load S3BERT model\n",
    "model = SentenceTransformer(\"S3BERT_main\\s3bert_all-mpnet-base-v2\", device=\"cpu\")\n",
    "\n",
    "# sentences\n",
    "sentences_a = np.array(list(sentences_dict.values()))[:,0]\n",
    "sentences_b = np.array(list(sentences_dict.values()))[:,1]\n",
    "\n",
    "# encode with s3bert\n",
    "sentences_a_encoded = model.encode(sentences_a)\n",
    "sentences_b_encoded = model.encode(sentences_b)\n",
    "\n",
    "# normalise\n",
    "sentences_a_norml = sep.normalise_embeddings(sentences_a_encoded)\n",
    "sentences_b_norml = sep.normalise_embeddings(sentences_b_encoded)\n",
    "\n",
    "# get similarity scores of different features\n",
    "similarities = ph.get_preds(sentences_a_encoded, sentences_b_encoded, biases=None, n=config.N, dim=config.FEATURE_DIM)\n",
    "similarities_norml = ph.get_preds(sentences_a_norml, sentences_b_norml, biases=None, n=config.N, dim=config.FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save global S3BERT similarities and embeddings\n",
    "global_sims = similarities[:,0]\n",
    "global_sims_norml = similarities_norml[:,0]\n",
    "np.savetxt(dataset+'_S3BERT_similarities.txt', global_sims, fmt='%f')\n",
    "np.savetxt(dataset+'_S3BERT_norml_similarities.txt', global_sims_norml, fmt='%f')\n",
    "np.savetxt(dataset+'_a_S3BERT_embeddings.txt', sentences_a_encoded, fmt='%f')\n",
    "np.savetxt(dataset+'_b_S3BERT_embeddings.txt', sentences_b_encoded, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'global': 0.8568320870399475, 'Concepts ': 0.8519608974456787, 'Frames ': 0.7636246085166931, 'Named Ent. ': 0.8424161076545715, 'Negations ': 0.9437075853347778, 'Reentrancies ': 0.9733306169509888, 'SRL ': 0.6547356843948364, 'Smatch ': 0.978515088558197, 'Unlabeled ': 0.8995557427406311, 'max_indegree_sim': 0.8576575517654419, 'max_outdegree_sim': 0.9412575960159302, 'max_degree_sim': 0.6852121949195862, 'root_sim': 0.8155668377876282, 'quant_sim': 0.9001861214637756, 'score_wlk': 0.840286910533905, 'score_wwlk': 0.8543912172317505, 'residual': 0.8567565083503723, 'sent_a': 'I remained under the banyan tree, exhausted by my daily ritual of dragooning the men every two hours.', 'sent_b': 'I remained under the banyan tree, exhausted by my daily ritual of herding the cats every two hours.'}\n",
      "{'global': 0.8202927112579346, 'Concepts ': 0.7236304879188538, 'Frames ': 0.6475898027420044, 'Named Ent. ': 0.7386634349822998, 'Negations ': 0.8733257055282593, 'Reentrancies ': 0.29368606209754944, 'SRL ': 0.2730924189090729, 'Smatch ': 0.8313414454460144, 'Unlabeled ': -0.14553482830524445, 'max_indegree_sim': 0.6938204765319824, 'max_outdegree_sim': 0.8812499642372131, 'max_degree_sim': 0.9005864858627319, 'root_sim': 0.7364136576652527, 'quant_sim': 0.8547233939170837, 'score_wlk': 0.7446373701095581, 'score_wwlk': 0.7180577516555786, 'residual': 0.8320602774620056, 'sent_a': 'In the US, it will depend on the school.', 'sent_b': 'It really depends on the school and the program.'}\n",
      "{'global': 0.0663418397307396, 'Concepts ': 0.26988285779953003, 'Frames ': 0.3650836944580078, 'Named Ent. ': 0.4117499589920044, 'Negations ': 0.14890463650226593, 'Reentrancies ': 0.49242258071899414, 'SRL ': -0.05503693222999573, 'Smatch ': -0.16507533192634583, 'Unlabeled ': 0.31546592712402344, 'max_indegree_sim': 0.3393804132938385, 'max_outdegree_sim': 0.01808113045990467, 'max_degree_sim': 0.27914485335350037, 'root_sim': 0.3565725088119507, 'quant_sim': 0.5329641699790955, 'score_wlk': 0.27997422218322754, 'score_wwlk': -0.07370971143245697, 'residual': 0.04065963253378868, 'sent_a': \"There's also what the string is made of.\", 'sent_b': 'There is also a Youtube-Version of the film.'}\n",
      "{'global': 0.4515317976474762, 'Concepts ': 0.49426090717315674, 'Frames ': 0.5136300325393677, 'Named Ent. ': 0.33065205812454224, 'Negations ': 0.26643145084381104, 'Reentrancies ': 0.34673619270324707, 'SRL ': 0.5512763857841492, 'Smatch ': 0.763546347618103, 'Unlabeled ': 0.7331702709197998, 'max_indegree_sim': 0.3979688584804535, 'max_outdegree_sim': 0.1523093730211258, 'max_degree_sim': 0.40480345487594604, 'root_sim': 0.5262381434440613, 'quant_sim': 0.6762409806251526, 'score_wlk': 0.8502296805381775, 'score_wwlk': 0.2820054292678833, 'residual': 0.43751761317253113, 'sent_a': 'You also imply you may not be paid if they cannot place you with a client.', 'sent_b': 'You can do it, but you might not be a professor.'}\n",
      "{'global': 0.4735051095485687, 'Concepts ': 0.558397650718689, 'Frames ': 0.4251399040222168, 'Named Ent. ': 0.4467579424381256, 'Negations ': 0.3347189128398895, 'Reentrancies ': 0.3487217426300049, 'SRL ': 0.5896735787391663, 'Smatch ': 0.21964150667190552, 'Unlabeled ': 0.6997800469398499, 'max_indegree_sim': 0.6987881660461426, 'max_outdegree_sim': 0.23332996666431427, 'max_degree_sim': 0.24582280218601227, 'root_sim': 0.4734610617160797, 'quant_sim': 0.4924663305282593, 'score_wlk': 0.35166531801223755, 'score_wwlk': 0.2564069330692291, 'residual': 0.4792286157608032, 'sent_a': 'I did this one time as well.', 'sent_b': 'I have this habit as well.'}\n",
      "{'global': 0.2003289759159088, 'Concepts ': 0.19161231815814972, 'Frames ': 0.7095224261283875, 'Named Ent. ': -0.11539178341627121, 'Negations ': 0.11873925477266312, 'Reentrancies ': 0.39222460985183716, 'SRL ': 0.49098485708236694, 'Smatch ': 0.20186102390289307, 'Unlabeled ': -0.03654901683330536, 'max_indegree_sim': 0.6960012316703796, 'max_outdegree_sim': 0.752860426902771, 'max_degree_sim': 0.5911090970039368, 'root_sim': 0.3876374065876007, 'quant_sim': 0.21733899414539337, 'score_wlk': 0.3807334303855896, 'score_wwlk': 0.5480067729949951, 'residual': 0.17419999837875366, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'You may want it, but the process given to you is what you have to work within.'}\n",
      "{'global': 0.9645445346832275, 'Concepts ': 0.9643332362174988, 'Frames ': 0.9612810611724854, 'Named Ent. ': 0.9704822301864624, 'Negations ': 0.927207350730896, 'Reentrancies ': 0.9801792502403259, 'SRL ': 0.9805558919906616, 'Smatch ': 0.9845426082611084, 'Unlabeled ': 0.9897064566612244, 'max_indegree_sim': 0.8949257135391235, 'max_outdegree_sim': 0.9294470548629761, 'max_degree_sim': 0.9563202261924744, 'root_sim': 0.9754348993301392, 'quant_sim': 0.961847186088562, 'score_wlk': 0.9551449418067932, 'score_wwlk': 0.9634296894073486, 'residual': 0.9632614850997925, 'sent_a': 'You do not need to worry.', 'sent_b': \"You don't have to worry.\"}\n",
      "{'global': 0.6573895215988159, 'Concepts ': 0.740531325340271, 'Frames ': 0.8691100478172302, 'Named Ent. ': 0.715935230255127, 'Negations ': 0.5021775364875793, 'Reentrancies ': 0.7577586770057678, 'SRL ': 0.7464112639427185, 'Smatch ': 0.9397060871124268, 'Unlabeled ': 0.2454136610031128, 'max_indegree_sim': 0.5681626200675964, 'max_outdegree_sim': 0.584459125995636, 'max_degree_sim': 0.7893553972244263, 'root_sim': 0.7410548329353333, 'quant_sim': 0.5838626027107239, 'score_wlk': 0.8209636211395264, 'score_wwlk': 0.7131891250610352, 'residual': 0.6521663665771484, 'sent_a': 'You should do it.', 'sent_b': 'You should do what it says.'}\n",
      "{'global': 0.8066172003746033, 'Concepts ': 0.8044451475143433, 'Frames ': 0.7222521305084229, 'Named Ent. ': 0.7902593016624451, 'Negations ': 0.3980950713157654, 'Reentrancies ': 0.6100928783416748, 'SRL ': 0.6650117039680481, 'Smatch ': 0.5473803281784058, 'Unlabeled ': 0.6670602560043335, 'max_indegree_sim': 0.8720057606697083, 'max_outdegree_sim': 0.7559652924537659, 'max_degree_sim': 0.5606377124786377, 'root_sim': 0.833054780960083, 'quant_sim': 0.6247855424880981, 'score_wlk': 0.9115267395973206, 'score_wwlk': 0.5826722979545593, 'residual': 0.8245564699172974, 'sent_a': 'You should just ask your boss what he wants you to do.', 'sent_b': \"You should listen to your boss, because you're not paid to tell the boss what to do.\"}\n",
      "{'global': 0.25170114636421204, 'Concepts ': 0.42033615708351135, 'Frames ': 0.22804036736488342, 'Named Ent. ': 0.3640579283237457, 'Negations ': 0.02041308581829071, 'Reentrancies ': 0.6594475507736206, 'SRL ': 0.5533896088600159, 'Smatch ': 0.7843205332756042, 'Unlabeled ': 0.6315222978591919, 'max_indegree_sim': 0.4371066093444824, 'max_outdegree_sim': 0.7365095019340515, 'max_degree_sim': 0.4425596296787262, 'root_sim': 0.5874714255332947, 'quant_sim': 0.5617384910583496, 'score_wlk': 0.500444769859314, 'score_wwlk': 0.7113368511199951, 'residual': 0.2050604522228241, 'sent_a': \"You need to read a lot to know what you like and what you don't.\", 'sent_b': 'You have to decide how much you want to demand, and what unmet demands you can live with.'}\n",
      "{'global': 0.3848375976085663, 'Concepts ': 0.3185575008392334, 'Frames ': 0.5622625946998596, 'Named Ent. ': 0.18474623560905457, 'Negations ': 0.4589233994483948, 'Reentrancies ': 0.9048794507980347, 'SRL ': 0.2073158472776413, 'Smatch ': 0.4548841714859009, 'Unlabeled ': 0.6071128249168396, 'max_indegree_sim': 0.5479971170425415, 'max_outdegree_sim': 0.23678909242153168, 'max_degree_sim': -0.016907967627048492, 'root_sim': -0.14500859379768372, 'quant_sim': 0.4370589852333069, 'score_wlk': 0.6096728444099426, 'score_wwlk': 0.08450806140899658, 'residual': 0.3917527496814728, 'sent_a': 'It depends on what you want to have in your tank.', 'sent_b': 'i think it depends what you want:'}\n",
      "{'global': 0.6125894784927368, 'Concepts ': 0.7000157833099365, 'Frames ': 0.8067710995674133, 'Named Ent. ': 0.26656004786491394, 'Negations ': 0.7830484509468079, 'Reentrancies ': 0.8359605073928833, 'SRL ': 0.7443633079528809, 'Smatch ': -0.12638656795024872, 'Unlabeled ': 0.5051570534706116, 'max_indegree_sim': 0.6578890085220337, 'max_outdegree_sim': 0.17516478896141052, 'max_degree_sim': 0.65167236328125, 'root_sim': 0.7858893275260925, 'quant_sim': 0.6029046177864075, 'score_wlk': 0.8027423024177551, 'score_wwlk': 0.8365010619163513, 'residual': 0.6179955005645752, 'sent_a': 'You can do it, too.', 'sent_b': 'Yes, you can do it.'}\n",
      "{'global': 0.5006075501441956, 'Concepts ': 0.770241916179657, 'Frames ': 0.6972028613090515, 'Named Ent. ': 0.1718294471502304, 'Negations ': 0.6719425320625305, 'Reentrancies ': 0.7316879630088806, 'SRL ': 0.6383453011512756, 'Smatch ': -0.2058791071176529, 'Unlabeled ': 0.011771121062338352, 'max_indegree_sim': 0.26906752586364746, 'max_outdegree_sim': -0.07674223929643631, 'max_degree_sim': 0.6305025815963745, 'root_sim': 0.7616814970970154, 'quant_sim': 0.4391768276691437, 'score_wlk': 0.6804953217506409, 'score_wwlk': 0.6012259125709534, 'residual': 0.4992736577987671, 'sent_a': 'You should do it.', 'sent_b': 'You can do it, too.'}\n",
      "{'global': 0.5536214709281921, 'Concepts ': 0.6757559776306152, 'Frames ': 0.7889230251312256, 'Named Ent. ': 0.20153191685676575, 'Negations ': 0.28815314173698425, 'Reentrancies ': 0.5731105208396912, 'SRL ': 0.42863813042640686, 'Smatch ': 0.3535804748535156, 'Unlabeled ': 0.7420359253883362, 'max_indegree_sim': 0.7215152978897095, 'max_outdegree_sim': 0.8354273438453674, 'max_degree_sim': 0.5623999834060669, 'root_sim': 0.6303071975708008, 'quant_sim': 0.40373343229293823, 'score_wlk': 0.8236512541770935, 'score_wwlk': 0.7949065566062927, 'residual': 0.5447571277618408, 'sent_a': 'You have to decide what you want to get out of this.', 'sent_b': 'You have to know what you want to do.'}\n",
      "{'global': 0.9029917120933533, 'Concepts ': 0.8640117049217224, 'Frames ': 0.7652484774589539, 'Named Ent. ': 0.9195272922515869, 'Negations ': 0.9479376077651978, 'Reentrancies ': 0.906755805015564, 'SRL ': 0.8768799901008606, 'Smatch ': 0.7640243768692017, 'Unlabeled ': 0.5553227066993713, 'max_indegree_sim': 0.9114910960197449, 'max_outdegree_sim': 0.9405884146690369, 'max_degree_sim': 0.9380053877830505, 'root_sim': 0.817380964756012, 'quant_sim': 0.9001636505126953, 'score_wlk': 0.8956302404403687, 'score_wwlk': 0.8796314001083374, 'residual': 0.9056930541992188, 'sent_a': 'I have few suggestions for you:', 'sent_b': 'I have two suggestions for you:'}\n",
      "{'global': 0.3231346011161804, 'Concepts ': -0.04120313376188278, 'Frames ': 0.5967397093772888, 'Named Ent. ': -0.03690819442272186, 'Negations ': 0.5294374823570251, 'Reentrancies ': 0.3487079441547394, 'SRL ': 0.16227741539478302, 'Smatch ': 0.05959450453519821, 'Unlabeled ': 0.27031368017196655, 'max_indegree_sim': 0.7624356150627136, 'max_outdegree_sim': 0.24929428100585938, 'max_degree_sim': 0.3069061040878296, 'root_sim': 0.17390048503875732, 'quant_sim': 0.4194372892379761, 'score_wlk': 0.2426101118326187, 'score_wwlk': 0.2957921624183655, 'residual': 0.32359084486961365, 'sent_a': 'You want to start in the room that is the largest to make sure you have the straightest start.', 'sent_b': 'You will have to start with the clinic., and maybe move on to the insurance company.'}\n",
      "{'global': 0.5909692049026489, 'Concepts ': 0.6760615110397339, 'Frames ': 0.43224653601646423, 'Named Ent. ': 0.06000394746661186, 'Negations ': 0.6595361828804016, 'Reentrancies ': 0.8787102699279785, 'SRL ': 0.6713098287582397, 'Smatch ': -0.5376277565956116, 'Unlabeled ': 0.11325447261333466, 'max_indegree_sim': 0.387162983417511, 'max_outdegree_sim': 0.5572773814201355, 'max_degree_sim': 0.6589677929878235, 'root_sim': 0.28794506192207336, 'quant_sim': 0.6579609513282776, 'score_wlk': 0.7750206589698792, 'score_wwlk': 0.541991651058197, 'residual': 0.6174872517585754, 'sent_a': \"if you don't want to derail the meeting, but the key is to speak up.\", 'sent_b': 'The key thing you need to do in this meeting is listen.'}\n",
      "{'global': 0.6030159592628479, 'Concepts ': 0.5809022188186646, 'Frames ': 0.7511188983917236, 'Named Ent. ': 0.5196918845176697, 'Negations ': 0.1668841540813446, 'Reentrancies ': 0.5304727554321289, 'SRL ': 0.7288473844528198, 'Smatch ': 0.4799567461013794, 'Unlabeled ': 0.5726054310798645, 'max_indegree_sim': 0.008223469369113445, 'max_outdegree_sim': 0.3119881749153137, 'max_degree_sim': 0.5571180582046509, 'root_sim': 0.3850937485694885, 'quant_sim': 0.5724096298217773, 'score_wlk': 0.7631713151931763, 'score_wwlk': 0.4845241904258728, 'residual': 0.6150064468383789, 'sent_a': 'Unfortunately the answer to your question is we simply do not know.', 'sent_b': 'My answer to your question is \"Probably Not\".'}\n",
      "{'global': 0.5478153228759766, 'Concepts ': 0.8578373789787292, 'Frames ': 0.6971132755279541, 'Named Ent. ': 0.49854665994644165, 'Negations ': 0.6438828110694885, 'Reentrancies ': 0.898033857345581, 'SRL ': 0.8208577036857605, 'Smatch ': 0.4048045873641968, 'Unlabeled ': 0.3608245551586151, 'max_indegree_sim': 0.6379251480102539, 'max_outdegree_sim': 0.45881474018096924, 'max_degree_sim': 0.4347989857196808, 'root_sim': 0.7720398902893066, 'quant_sim': 0.49084222316741943, 'score_wlk': 0.8908994793891907, 'score_wwlk': 0.5038137435913086, 'residual': 0.5375639200210571, 'sent_a': 'As soon as possible.', 'sent_b': 'Start them as early as possible.'}\n",
      "{'global': 0.30404558777809143, 'Concepts ': 0.2140602469444275, 'Frames ': 0.430866003036499, 'Named Ent. ': -0.27043741941452026, 'Negations ': 0.09720166027545929, 'Reentrancies ': 0.7523811459541321, 'SRL ': 0.5496414303779602, 'Smatch ': -0.07886870950460434, 'Unlabeled ': 0.876682460308075, 'max_indegree_sim': 0.4028469920158386, 'max_outdegree_sim': 0.6506419777870178, 'max_degree_sim': 0.5590390563011169, 'root_sim': 0.33423343300819397, 'quant_sim': 0.4405517280101776, 'score_wlk': 0.6237719058990479, 'score_wwlk': 0.24519503116607666, 'residual': 0.29263928532600403, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'It depends on what you want to do next, and where you want to do it.'}\n",
      "{'global': 0.9397512078285217, 'Concepts ': 0.8963304758071899, 'Frames ': 0.9064229726791382, 'Named Ent. ': 0.9459168314933777, 'Negations ': 0.9520452618598938, 'Reentrancies ': 0.9704346060752869, 'SRL ': 0.9390822052955627, 'Smatch ': 0.9299173355102539, 'Unlabeled ': 0.826198935508728, 'max_indegree_sim': 0.9554297924041748, 'max_outdegree_sim': 0.8837405443191528, 'max_degree_sim': 0.9468246102333069, 'root_sim': 0.9619967937469482, 'quant_sim': 0.917334794998169, 'score_wlk': 0.9542032480239868, 'score_wwlk': 0.926497757434845, 'residual': 0.9418734908103943, 'sent_a': 'The answer to both questions is: Yes.', 'sent_b': 'The answer to both of your questions is yes.'}\n",
      "{'global': 0.6614713072776794, 'Concepts ': 0.8300618529319763, 'Frames ': 0.6064673066139221, 'Named Ent. ': 0.886543333530426, 'Negations ': 0.39121904969215393, 'Reentrancies ': 0.5962246060371399, 'SRL ': 0.5462979078292847, 'Smatch ': 0.12850919365882874, 'Unlabeled ': 0.6897759437561035, 'max_indegree_sim': 0.45770391821861267, 'max_outdegree_sim': 0.6530666947364807, 'max_degree_sim': 0.17846235632896423, 'root_sim': 0.8052452802658081, 'quant_sim': 0.4788939654827118, 'score_wlk': 0.45984357595443726, 'score_wwlk': 0.532416045665741, 'residual': 0.6737775206565857, 'sent_a': 'To give this an answer:', 'sent_b': \"I'll answer this question:\"}\n",
      "{'global': 0.6886667609214783, 'Concepts ': 0.6594467759132385, 'Frames ': 0.38395124673843384, 'Named Ent. ': 0.6409066915512085, 'Negations ': 0.4095079302787781, 'Reentrancies ': 0.7769235968589783, 'SRL ': 0.8090407848358154, 'Smatch ': 0.6385987997055054, 'Unlabeled ': 0.8776010870933533, 'max_indegree_sim': 0.6332844495773315, 'max_outdegree_sim': 0.7223455905914307, 'max_degree_sim': 0.6273695826530457, 'root_sim': 0.5417774319648743, 'quant_sim': 0.6868164539337158, 'score_wlk': 0.7930048108100891, 'score_wwlk': 0.5072520971298218, 'residual': 0.6948347091674805, 'sent_a': 'Unfortunately the answer to your question is we simply do not know.', 'sent_b': \"Sorry, I don't know the answer to your question.\"}\n",
      "{'global': 0.9354745149612427, 'Concepts ': 0.9734597206115723, 'Frames ': 0.961514413356781, 'Named Ent. ': 0.9730254411697388, 'Negations ': 0.9417088031768799, 'Reentrancies ': 0.9286015033721924, 'SRL ': 0.899068295955658, 'Smatch ': 0.9866726398468018, 'Unlabeled ': 0.9598690867424011, 'max_indegree_sim': 0.9607641696929932, 'max_outdegree_sim': 0.8867900967597961, 'max_degree_sim': 0.8923583626747131, 'root_sim': 0.9749135971069336, 'quant_sim': 0.9064524173736572, 'score_wlk': 0.9531413912773132, 'score_wwlk': 0.7157188653945923, 'residual': 0.9359319806098938, 'sent_a': 'The rule - When in doubt throw it out!', 'sent_b': 'I always go by the rule \"When in doubt, throw it out!'}\n",
      "{'global': 0.7428619861602783, 'Concepts ': 0.9077042937278748, 'Frames ': 0.6103026270866394, 'Named Ent. ': 0.705881655216217, 'Negations ': 0.8736064434051514, 'Reentrancies ': 0.6926473379135132, 'SRL ': 0.7420027852058411, 'Smatch ': 0.21016742289066315, 'Unlabeled ': 0.4096719026565552, 'max_indegree_sim': 0.7031661868095398, 'max_outdegree_sim': 0.5903745889663696, 'max_degree_sim': 0.7016698122024536, 'root_sim': 0.38278284668922424, 'quant_sim': 0.8053334951400757, 'score_wlk': 0.7521238327026367, 'score_wwlk': 0.5910066962242126, 'residual': 0.7626997828483582, 'sent_a': 'This is not a good idea.', 'sent_b': 'This sound like a very bad idea.'}\n",
      "{'global': 0.27350419759750366, 'Concepts ': 0.3703654408454895, 'Frames ': 0.24405498802661896, 'Named Ent. ': -0.014265313744544983, 'Negations ': 0.38551759719848633, 'Reentrancies ': 0.47590017318725586, 'SRL ': -0.05956670269370079, 'Smatch ': 0.27789416909217834, 'Unlabeled ': 0.7272613048553467, 'max_indegree_sim': 0.5280224680900574, 'max_outdegree_sim': 0.2690054178237915, 'max_degree_sim': 0.39913299679756165, 'root_sim': 0.8294596076011658, 'quant_sim': 0.6002843976020813, 'score_wlk': 0.39496487379074097, 'score_wwlk': 0.3582383692264557, 'residual': 0.26441633701324463, 'sent_a': \"Yes, it's probably a good idea to renew your passport.\", 'sent_b': \"It's a good idea.\"}\n",
      "{'global': 0.9629238247871399, 'Concepts ': 0.8903416991233826, 'Frames ': 0.9518986940383911, 'Named Ent. ': 0.9418877959251404, 'Negations ': 0.9644140601158142, 'Reentrancies ': 0.9856624007225037, 'SRL ': 0.9526392221450806, 'Smatch ': 0.8573827743530273, 'Unlabeled ': 0.9210941195487976, 'max_indegree_sim': 0.9264068603515625, 'max_outdegree_sim': 0.84493488073349, 'max_degree_sim': 0.9104654788970947, 'root_sim': 0.9499739408493042, 'quant_sim': 0.9424053430557251, 'score_wlk': 0.8799737095832825, 'score_wwlk': 0.9601147770881653, 'residual': 0.9658888578414917, 'sent_a': 'It probably depends on the cut of meat.', 'sent_b': \"It depends on the meat and how it's cut.\"}\n",
      "{'global': 0.3471294641494751, 'Concepts ': 0.45098963379859924, 'Frames ': 0.18599817156791687, 'Named Ent. ': 0.2357315570116043, 'Negations ': 0.30960896611213684, 'Reentrancies ': 0.7753747701644897, 'SRL ': 0.560030460357666, 'Smatch ': 0.5470207333564758, 'Unlabeled ': 0.834145188331604, 'max_indegree_sim': 0.22151464223861694, 'max_outdegree_sim': 0.23495221138000488, 'max_degree_sim': 0.8173000812530518, 'root_sim': -0.0425848513841629, 'quant_sim': 0.06908876448869705, 'score_wlk': 0.7073912620544434, 'score_wwlk': 0.39220577478408813, 'residual': 0.33748483657836914, 'sent_a': \"It's not a good idea.\", 'sent_b': \"It's a good question.\"}\n",
      "{'global': 0.486187607049942, 'Concepts ': 0.5648437738418579, 'Frames ': 0.6649269461631775, 'Named Ent. ': 0.3603736460208893, 'Negations ': 0.22434364259243011, 'Reentrancies ': 0.7455359101295471, 'SRL ': 0.42690521478652954, 'Smatch ': 0.5094722509384155, 'Unlabeled ': 0.5104972720146179, 'max_indegree_sim': 0.3198448121547699, 'max_outdegree_sim': -0.052596814930438995, 'max_degree_sim': 0.7907724380493164, 'root_sim': 0.48189035058021545, 'quant_sim': 0.8718994855880737, 'score_wlk': 0.6437810063362122, 'score_wwlk': 0.6678465008735657, 'residual': 0.48338544368743896, 'sent_a': \"It's pretty much up to you.\", 'sent_b': \"It's much better to ask.\"}\n",
      "{'global': 0.43047818541526794, 'Concepts ': 0.3842732012271881, 'Frames ': 0.36952444911003113, 'Named Ent. ': -0.001164495712146163, 'Negations ': 0.06462901085615158, 'Reentrancies ': 0.6008771061897278, 'SRL ': 0.3155258297920227, 'Smatch ': 0.5773229598999023, 'Unlabeled ': -0.29889804124832153, 'max_indegree_sim': 0.047141872346401215, 'max_outdegree_sim': -0.11749160289764404, 'max_degree_sim': -0.0026783859357237816, 'root_sim': 0.2735801339149475, 'quant_sim': 0.14990627765655518, 'score_wlk': 0.6428530216217041, 'score_wwlk': 0.6239870190620422, 'residual': 0.4519919455051422, 'sent_a': 'Yes, there is a reason for it.', 'sent_b': 'Yes, that is exactly what it means.'}\n",
      "{'global': 0.6862926483154297, 'Concepts ': 0.5381867289543152, 'Frames ': 0.5025572776794434, 'Named Ent. ': 0.2770666778087616, 'Negations ': 0.6352826952934265, 'Reentrancies ': 0.5668348670005798, 'SRL ': 0.7588326334953308, 'Smatch ': 0.9276043772697449, 'Unlabeled ': 0.8485040664672852, 'max_indegree_sim': 0.3695712089538574, 'max_outdegree_sim': 0.44546645879745483, 'max_degree_sim': 0.6753320097923279, 'root_sim': 0.8189024329185486, 'quant_sim': 0.3025972545146942, 'score_wlk': 0.8452355861663818, 'score_wwlk': 0.6516134738922119, 'residual': 0.689750611782074, 'sent_a': 'Have you tried asking your employees?', 'sent_b': 'Have you tried asking?'}\n",
      "{'global': 0.8230673670768738, 'Concepts ': 0.8253465294837952, 'Frames ': 0.8301549553871155, 'Named Ent. ': 0.4743916690349579, 'Negations ': 0.9088521599769592, 'Reentrancies ': 0.9308794736862183, 'SRL ': 0.812233030796051, 'Smatch ': 0.6699178814888, 'Unlabeled ': -0.1824687421321869, 'max_indegree_sim': 0.8971334099769592, 'max_outdegree_sim': 0.8650807738304138, 'max_degree_sim': 0.8466623425483704, 'root_sim': 0.6497544050216675, 'quant_sim': 0.6982734799385071, 'score_wlk': 0.767616331577301, 'score_wwlk': 0.8125655651092529, 'residual': 0.8301255106925964, 'sent_a': 'You guys are making this all WAAAAAY too complicated.', 'sent_b': 'You are making this too complicated.'}\n",
      "{'global': 0.47967976331710815, 'Concepts ': 0.7600634098052979, 'Frames ': 0.6095204949378967, 'Named Ent. ': 0.34338700771331787, 'Negations ': 0.40752285718917847, 'Reentrancies ': 0.7756593227386475, 'SRL ': 0.8247286081314087, 'Smatch ': 0.6056635975837708, 'Unlabeled ': 0.8163440227508545, 'max_indegree_sim': 0.4887511730194092, 'max_outdegree_sim': 0.13846281170845032, 'max_degree_sim': 0.7891640663146973, 'root_sim': 0.4938800036907196, 'quant_sim': 0.6493989825248718, 'score_wlk': 0.7164496183395386, 'score_wwlk': 0.6401045918464661, 'residual': 0.45046746730804443, 'sent_a': \"You don't have to know.\", 'sent_b': 'You have no need to do anything.'}\n",
      "{'global': 0.2369547039270401, 'Concepts ': 0.30302006006240845, 'Frames ': 0.44602760672569275, 'Named Ent. ': -0.4858816862106323, 'Negations ': 0.3843404948711395, 'Reentrancies ': 0.4566355347633362, 'SRL ': 0.3492620289325714, 'Smatch ': -0.37229764461517334, 'Unlabeled ': 0.23548860847949982, 'max_indegree_sim': 0.5454733967781067, 'max_outdegree_sim': 0.2975722551345825, 'max_degree_sim': 0.13171109557151794, 'root_sim': 0.138248473405838, 'quant_sim': 0.606660783290863, 'score_wlk': 0.037674371153116226, 'score_wwlk': 0.24066407978534698, 'residual': 0.24312293529510498, 'sent_a': 'There are two things to consider:', 'sent_b': 'There are two possible causes for this:'}\n",
      "{'global': 0.2964019775390625, 'Concepts ': 0.40186306834220886, 'Frames ': 0.3954618573188782, 'Named Ent. ': 0.48507168889045715, 'Negations ': 0.4560924768447876, 'Reentrancies ': 0.6452941298484802, 'SRL ': 0.12111497670412064, 'Smatch ': 0.17049895226955414, 'Unlabeled ': 0.003258360316976905, 'max_indegree_sim': 0.03847133740782738, 'max_outdegree_sim': -0.061727169901132584, 'max_degree_sim': 0.5610517263412476, 'root_sim': 0.018198028206825256, 'quant_sim': 0.6962630748748779, 'score_wlk': 0.26956304907798767, 'score_wwlk': 0.4500196576118469, 'residual': 0.29041996598243713, 'sent_a': 'Work into it slowly.', 'sent_b': 'It seems to work.'}\n",
      "{'global': 0.8572407960891724, 'Concepts ': 0.6926226615905762, 'Frames ': 0.9122534990310669, 'Named Ent. ': 0.8404306173324585, 'Negations ': 0.9331461191177368, 'Reentrancies ': 0.7529651522636414, 'SRL ': 0.8337244391441345, 'Smatch ': 0.6247624158859253, 'Unlabeled ': 0.8196152448654175, 'max_indegree_sim': 0.4948224425315857, 'max_outdegree_sim': 0.9215118288993835, 'max_degree_sim': 0.8963513970375061, 'root_sim': 0.9419470429420471, 'quant_sim': 0.8916139006614685, 'score_wlk': 0.9148836135864258, 'score_wwlk': 0.7894912958145142, 'residual': 0.8583582043647766, 'sent_a': 'You can buy it on amazon for $5.', 'sent_b': 'You can buy it on ebay for $25 and up.'}\n",
      "{'global': 0.5595585703849792, 'Concepts ': 0.7709507346153259, 'Frames ': 0.6668238639831543, 'Named Ent. ': 0.09639761596918106, 'Negations ': 0.26872557401657104, 'Reentrancies ': 0.7169537544250488, 'SRL ': 0.6781765818595886, 'Smatch ': 0.6069963574409485, 'Unlabeled ': -0.15153032541275024, 'max_indegree_sim': 0.576818585395813, 'max_outdegree_sim': 0.001914410968311131, 'max_degree_sim': 0.6384699940681458, 'root_sim': -0.1363559067249298, 'quant_sim': 0.43452131748199463, 'score_wlk': 0.35108816623687744, 'score_wwlk': 0.5113236308097839, 'residual': 0.5781729221343994, 'sent_a': 'The coffee simply picks up the aluminum from the pot, as the coffee is acidic.', 'sent_b': 'One idea is cleaning the coffee residue from the coffee pot.'}\n",
      "{'global': 0.34535926580429077, 'Concepts ': 0.11590708047151566, 'Frames ': -0.13036787509918213, 'Named Ent. ': 0.05199705436825752, 'Negations ': 0.6199287176132202, 'Reentrancies ': 0.4788786470890045, 'SRL ': 0.5496346354484558, 'Smatch ': 0.5449438691139221, 'Unlabeled ': 0.6124648451805115, 'max_indegree_sim': 0.7428415417671204, 'max_outdegree_sim': 0.4489355683326721, 'max_degree_sim': 0.5149467587471008, 'root_sim': 0.2910201847553253, 'quant_sim': 0.35019493103027344, 'score_wlk': 0.43867748975753784, 'score_wwlk': 0.23855742812156677, 'residual': 0.34168657660484314, 'sent_a': 'There are two ways to start with: plunging and dripping.', 'sent_b': 'There are two traditional ways to bend wood:'}\n",
      "{'global': 0.423138290643692, 'Concepts ': 0.3763183355331421, 'Frames ': 0.3482546806335449, 'Named Ent. ': 0.6750946640968323, 'Negations ': 0.5341991186141968, 'Reentrancies ': 0.7171987891197205, 'SRL ': 0.7326721549034119, 'Smatch ': -0.37772801518440247, 'Unlabeled ': -0.2663115859031677, 'max_indegree_sim': 0.4775978922843933, 'max_outdegree_sim': 0.6227989792823792, 'max_degree_sim': 0.17716889083385468, 'root_sim': 0.5229200124740601, 'quant_sim': 0.04847190901637077, 'score_wlk': 0.5928765535354614, 'score_wwlk': 0.5258012413978577, 'residual': 0.4246116876602173, 'sent_a': 'You might have to try a variety before you find one that clicks with him.', 'sent_b': 'My advice would be to try a variety of coffees that you can afford and find one you like.'}\n",
      "{'global': 0.9573907852172852, 'Concepts ': 0.9578436613082886, 'Frames ': 0.9672145247459412, 'Named Ent. ': 0.9760419726371765, 'Negations ': 0.9276599884033203, 'Reentrancies ': 0.9595335125923157, 'SRL ': 0.7711741328239441, 'Smatch ': 0.979243814945221, 'Unlabeled ': 0.9788622856140137, 'max_indegree_sim': 0.9606673717498779, 'max_outdegree_sim': 0.9514017105102539, 'max_degree_sim': 0.9436071515083313, 'root_sim': 0.9014087319374084, 'quant_sim': 0.9739976525306702, 'score_wlk': 0.9599413275718689, 'score_wwlk': 0.9652857780456543, 'residual': 0.956732988357544, 'sent_a': \"It's not a good idea.\", 'sent_b': 'It is not a good idea.'}\n",
      "{'global': 0.2066899985074997, 'Concepts ': 0.03414890170097351, 'Frames ': 0.6546396017074585, 'Named Ent. ': 0.15871605277061462, 'Negations ': -0.0922083705663681, 'Reentrancies ': 0.6275798678398132, 'SRL ': 0.5070331692695618, 'Smatch ': -0.13704870641231537, 'Unlabeled ': -0.307689368724823, 'max_indegree_sim': 0.24699048697948456, 'max_outdegree_sim': 0.24043266475200653, 'max_degree_sim': 0.4410046935081482, 'root_sim': 0.5402238965034485, 'quant_sim': 0.3996851444244385, 'score_wlk': 0.107961505651474, 'score_wwlk': 0.4103330075740814, 'residual': 0.20779524743556976, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'They can, but the way to do it depends on what you have available.'}\n",
      "{'global': 0.674819827079773, 'Concepts ': 0.8331177234649658, 'Frames ': 0.37661367654800415, 'Named Ent. ': 0.5482840538024902, 'Negations ': 0.7689011693000793, 'Reentrancies ': 0.853069007396698, 'SRL ': 0.7852311134338379, 'Smatch ': 0.9731032848358154, 'Unlabeled ': 0.8832579851150513, 'max_indegree_sim': 0.9244979023933411, 'max_outdegree_sim': 0.8616671562194824, 'max_degree_sim': 0.6270948648452759, 'root_sim': 0.7356992363929749, 'quant_sim': 0.5887904167175293, 'score_wlk': 0.8624336123466492, 'score_wwlk': 0.7054270505905151, 'residual': 0.6514390707015991, 'sent_a': 'Yes, you should mention your experience.', 'sent_b': 'Yes, you should mention it.'}\n",
      "{'global': 0.466050386428833, 'Concepts ': 0.6041848659515381, 'Frames ': 0.47806575894355774, 'Named Ent. ': 0.2644789516925812, 'Negations ': 0.48960673809051514, 'Reentrancies ': 0.9091063737869263, 'SRL ': 0.3936000168323517, 'Smatch ': 0.010875838808715343, 'Unlabeled ': 0.3598673343658447, 'max_indegree_sim': 0.5999787449836731, 'max_outdegree_sim': 0.20019946992397308, 'max_degree_sim': 0.5091160535812378, 'root_sim': 0.3491573631763458, 'quant_sim': 0.40844419598579407, 'score_wlk': 0.3941405117511749, 'score_wwlk': 0.562396764755249, 'residual': 0.48477408289909363, 'sent_a': 'From what I understand this is what you can do :', 'sent_b': 'Can you do this?'}\n",
      "{'global': 0.9360877275466919, 'Concepts ': 0.969804584980011, 'Frames ': 0.941573977470398, 'Named Ent. ': 0.9150766730308533, 'Negations ': 0.9394629597663879, 'Reentrancies ': 0.9143852591514587, 'SRL ': 0.917670488357544, 'Smatch ': 0.8582319617271423, 'Unlabeled ': 0.8676361441612244, 'max_indegree_sim': 0.9540578126907349, 'max_outdegree_sim': 0.9398189187049866, 'max_degree_sim': 0.941798210144043, 'root_sim': 0.935860812664032, 'quant_sim': 0.8975731730461121, 'score_wlk': 0.9198445081710815, 'score_wwlk': 0.975517749786377, 'residual': 0.9363989233970642, 'sent_a': 'Take a look at these:', 'sent_b': 'Take a look at this: '}\n",
      "{'global': 0.34353914856910706, 'Concepts ': 0.7031281590461731, 'Frames ': 0.2945306599140167, 'Named Ent. ': 0.6035304665565491, 'Negations ': -0.16064871847629547, 'Reentrancies ': 0.7747554183006287, 'SRL ': 0.4736083149909973, 'Smatch ': 0.5233736038208008, 'Unlabeled ': 0.24439606070518494, 'max_indegree_sim': 0.6600075364112854, 'max_outdegree_sim': 0.570158064365387, 'max_degree_sim': 0.42428845167160034, 'root_sim': 0.31868401169776917, 'quant_sim': 0.12936480343341827, 'score_wlk': 0.611396074295044, 'score_wwlk': 0.6839475035667419, 'residual': 0.33368027210235596, 'sent_a': \"I'd say it primarily depends on two things:\", 'sent_b': \"I'd say it depends on the ultimate outcome you want?\"}\n",
      "{'global': 0.6760794520378113, 'Concepts ': 0.7165370583534241, 'Frames ': 0.5885804295539856, 'Named Ent. ': 0.6952241659164429, 'Negations ': 0.5924363136291504, 'Reentrancies ': 0.7937718033790588, 'SRL ': 0.5117197632789612, 'Smatch ': 0.41900041699409485, 'Unlabeled ': -0.02043202519416809, 'max_indegree_sim': 0.8383539319038391, 'max_outdegree_sim': 0.5100967288017273, 'max_degree_sim': 0.6261795163154602, 'root_sim': 0.5220790505409241, 'quant_sim': 0.7384529113769531, 'score_wlk': 0.21177871525287628, 'score_wwlk': 0.735734224319458, 'residual': 0.6902257204055786, 'sent_a': 'Some of what you can do:', 'sent_b': 'Not much you can do besides:'}\n",
      "{'global': 0.8276053071022034, 'Concepts ': 0.8191145658493042, 'Frames ': 0.8059364557266235, 'Named Ent. ': 0.7593302130699158, 'Negations ': 0.7619479298591614, 'Reentrancies ': 0.9014958143234253, 'SRL ': 0.8281336426734924, 'Smatch ': 0.9101887941360474, 'Unlabeled ': 0.9363855123519897, 'max_indegree_sim': 0.8468266129493713, 'max_outdegree_sim': 0.7220398783683777, 'max_degree_sim': 0.7860681414604187, 'root_sim': 0.7210941314697266, 'quant_sim': 0.8603687286376953, 'score_wlk': 0.8487968444824219, 'score_wwlk': 0.6028655171394348, 'residual': 0.8263232111930847, 'sent_a': 'This is not a good idea.', 'sent_b': 'But it is not a good idea.'}\n",
      "{'global': 0.11797390878200531, 'Concepts ': 0.3733169436454773, 'Frames ': -0.08518808335065842, 'Named Ent. ': -0.18064558506011963, 'Negations ': 0.23738284409046173, 'Reentrancies ': 0.052632514387369156, 'SRL ': -0.17827512323856354, 'Smatch ': -0.31737494468688965, 'Unlabeled ': 0.12907396256923676, 'max_indegree_sim': 0.11201994866132736, 'max_outdegree_sim': -0.03560244292020798, 'max_degree_sim': 0.2131187468767166, 'root_sim': -0.07814005017280579, 'quant_sim': 0.2508702278137207, 'score_wlk': 0.31535604596138, 'score_wwlk': 0.4633749723434448, 'residual': 0.12257935851812363, 'sent_a': \"The answers so far are already good, but I'd like to add a map for Switzerland:\", 'sent_b': \"You have a lot of answers already, but I'd like to add Curries as another solutions.\"}\n",
      "{'global': 0.9023385643959045, 'Concepts ': 0.8629804849624634, 'Frames ': 0.884878396987915, 'Named Ent. ': 0.868630051612854, 'Negations ': 0.8852981328964233, 'Reentrancies ': 0.9712945222854614, 'SRL ': 0.7582857012748718, 'Smatch ': 0.7649732232093811, 'Unlabeled ': 0.7807522416114807, 'max_indegree_sim': 0.8354094624519348, 'max_outdegree_sim': 0.7374248504638672, 'max_degree_sim': 0.8133007287979126, 'root_sim': 0.7964111566543579, 'quant_sim': 0.9124801754951477, 'score_wlk': 0.856349527835846, 'score_wwlk': 0.8567181825637817, 'residual': 0.910307765007019, 'sent_a': 'I was in a similar situation.', 'sent_b': 'I had a similar situation.'}\n",
      "{'global': 0.8985220193862915, 'Concepts ': 0.9487859606742859, 'Frames ': 0.9433886408805847, 'Named Ent. ': 0.8525854349136353, 'Negations ': 0.9060167074203491, 'Reentrancies ': 0.9456774592399597, 'SRL ': 0.8759781718254089, 'Smatch ': 0.9208510518074036, 'Unlabeled ': 0.8536478877067566, 'max_indegree_sim': 0.8750132918357849, 'max_outdegree_sim': 0.9156107902526855, 'max_degree_sim': 0.8803555965423584, 'root_sim': 0.7701318860054016, 'quant_sim': 0.972694993019104, 'score_wlk': 0.8437933325767517, 'score_wwlk': 0.9408906698226929, 'residual': 0.8997615575790405, 'sent_a': \"I've had this same problem.\", 'sent_b': 'I had this same problem.'}\n",
      "{'global': 0.26897644996643066, 'Concepts ': 0.3455358147621155, 'Frames ': -0.010548186488449574, 'Named Ent. ': 0.0668911561369896, 'Negations ': 0.31874674558639526, 'Reentrancies ': 0.5986387133598328, 'SRL ': 0.5895480513572693, 'Smatch ': 0.6878565549850464, 'Unlabeled ': 0.786753237247467, 'max_indegree_sim': 0.6476898193359375, 'max_outdegree_sim': 0.11175737529993057, 'max_degree_sim': 0.2644050121307373, 'root_sim': 0.6910458207130432, 'quant_sim': 0.31425055861473083, 'score_wlk': 0.21228758990764618, 'score_wwlk': 0.12821677327156067, 'residual': 0.24401375651359558, 'sent_a': 'There is no maximum.', 'sent_b': 'There is no quarantine period.'}\n",
      "{'global': 0.14378084242343903, 'Concepts ': -0.1623803675174713, 'Frames ': 0.42519426345825195, 'Named Ent. ': 0.027734708040952682, 'Negations ': 0.2841680645942688, 'Reentrancies ': -0.5666041970252991, 'SRL ': -0.37564435601234436, 'Smatch ': 0.40372031927108765, 'Unlabeled ': 0.7421700358390808, 'max_indegree_sim': 0.17593708634376526, 'max_outdegree_sim': 0.17053718864917755, 'max_degree_sim': -0.29195404052734375, 'root_sim': 0.057522665709257126, 'quant_sim': 0.25486981868743896, 'score_wlk': 0.260791540145874, 'score_wwlk': -0.12901046872138977, 'residual': 0.1509184092283249, 'sent_a': 'I am not sure this is the right site for the question.', 'sent_b': 'I am not sure this question would have made much sense to the Romans themselves.'}\n",
      "{'global': 0.44204238057136536, 'Concepts ': 0.4562975764274597, 'Frames ': 0.3343241214752197, 'Named Ent. ': 0.12894465029239655, 'Negations ': 0.3223455250263214, 'Reentrancies ': 0.7689531445503235, 'SRL ': 0.44227924942970276, 'Smatch ': 0.7253199219703674, 'Unlabeled ': 0.46154114603996277, 'max_indegree_sim': 0.5725317001342773, 'max_outdegree_sim': 0.1622501164674759, 'max_degree_sim': 0.5807076692581177, 'root_sim': 0.13728849589824677, 'quant_sim': 0.5672324895858765, 'score_wlk': 0.6298165917396545, 'score_wwlk': 0.47126683592796326, 'residual': 0.4509519934654236, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'In other words, it depends on where you go, when you do there and how.'}\n",
      "{'global': 0.09144847840070724, 'Concepts ': -0.3109384775161743, 'Frames ': 0.16732513904571533, 'Named Ent. ': 0.11452659964561462, 'Negations ': -0.07577484846115112, 'Reentrancies ': 0.6333873271942139, 'SRL ': 0.7166668176651001, 'Smatch ': 0.1154126301407814, 'Unlabeled ': -0.31551074981689453, 'max_indegree_sim': -0.2658945918083191, 'max_outdegree_sim': 0.2178829461336136, 'max_degree_sim': 0.43082383275032043, 'root_sim': 0.2042267918586731, 'quant_sim': 0.3177296221256256, 'score_wlk': 0.22875013947486877, 'score_wwlk': 0.29790937900543213, 'residual': 0.09108223021030426, 'sent_a': \"You need to read a lot to know what you like and what you don't.\", 'sent_b': \"Yes, you should create a portfolio site to showcase what you can do and what you've done.\"}\n",
      "{'global': 0.21812716126441956, 'Concepts ': 0.14855565130710602, 'Frames ': -0.009784416295588017, 'Named Ent. ': -0.24476881325244904, 'Negations ': 0.16173048317432404, 'Reentrancies ': 0.5289715528488159, 'SRL ': 0.083511583507061, 'Smatch ': 0.43844473361968994, 'Unlabeled ': 0.6678627133369446, 'max_indegree_sim': 0.24322952330112457, 'max_outdegree_sim': 0.2151612639427185, 'max_degree_sim': 0.3755059838294983, 'root_sim': 0.4462147057056427, 'quant_sim': 0.3712759017944336, 'score_wlk': 0.4094609320163727, 'score_wwlk': 0.31070709228515625, 'residual': 0.20787373185157776, 'sent_a': 'You are not disclosing key info.', 'sent_b': 'No you are not.'}\n",
      "{'global': 0.6127102971076965, 'Concepts ': 0.5193670392036438, 'Frames ': 0.8184200525283813, 'Named Ent. ': 0.5997198820114136, 'Negations ': 0.6520156860351562, 'Reentrancies ': 0.9342491626739502, 'SRL ': 0.48567837476730347, 'Smatch ': 0.7579653263092041, 'Unlabeled ': 0.43235647678375244, 'max_indegree_sim': 0.8141820430755615, 'max_outdegree_sim': 0.5640736818313599, 'max_degree_sim': 0.5628752112388611, 'root_sim': 0.6927590370178223, 'quant_sim': 0.6444075703620911, 'score_wlk': 0.7034761905670166, 'score_wwlk': 0.5620172619819641, 'residual': 0.6086713075637817, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': \"I guess it depends on what you're going to do.\"}\n",
      "{'global': 0.19894541800022125, 'Concepts ': 0.3082560896873474, 'Frames ': 0.5456909537315369, 'Named Ent. ': -0.09957060217857361, 'Negations ': -0.09845605492591858, 'Reentrancies ': 0.6667660474777222, 'SRL ': 0.6911153197288513, 'Smatch ': -0.30808496475219727, 'Unlabeled ': 0.7086317539215088, 'max_indegree_sim': 0.46817708015441895, 'max_outdegree_sim': 0.39211031794548035, 'max_degree_sim': 0.1183963343501091, 'root_sim': 0.35166123509407043, 'quant_sim': 0.35455477237701416, 'score_wlk': 0.23255504667758942, 'score_wwlk': 0.485203355550766, 'residual': 0.18312683701515198, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'It is his job to see that you have what you need to do your job.'}\n",
      "{'global': 0.6654213070869446, 'Concepts ': 0.46689605712890625, 'Frames ': 0.7193577885627747, 'Named Ent. ': 0.7080349326133728, 'Negations ': 0.5764865875244141, 'Reentrancies ': 0.8525914549827576, 'SRL ': 0.6643332242965698, 'Smatch ': 0.8065892457962036, 'Unlabeled ': 0.5409703254699707, 'max_indegree_sim': 0.8830640316009521, 'max_outdegree_sim': 0.6864858865737915, 'max_degree_sim': 0.689226508140564, 'root_sim': 0.9162996411323547, 'quant_sim': 0.7437362670898438, 'score_wlk': 0.7469981908798218, 'score_wwlk': 0.5517369508743286, 'residual': 0.6595537066459656, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'It depends on what you want to achieve.'}\n",
      "{'global': 0.3040309548377991, 'Concepts ': 0.4323425590991974, 'Frames ': 0.17965096235275269, 'Named Ent. ': -0.10501518100500107, 'Negations ': 0.4181588590145111, 'Reentrancies ': 0.785470724105835, 'SRL ': 0.4375685155391693, 'Smatch ': 0.49871739745140076, 'Unlabeled ': 0.3465510606765747, 'max_indegree_sim': 0.5756256580352783, 'max_outdegree_sim': 0.13106942176818848, 'max_degree_sim': 0.01887766271829605, 'root_sim': 0.29885348677635193, 'quant_sim': 0.18472997844219208, 'score_wlk': 0.6440445184707642, 'score_wwlk': 0.3752557337284088, 'residual': 0.302903950214386, 'sent_a': 'This is a problem that the professor has to deal with.', 'sent_b': 'This is a big problem.'}\n",
      "{'global': 0.5394770503044128, 'Concepts ': 0.7219639420509338, 'Frames ': 0.7532452940940857, 'Named Ent. ': 0.5903339385986328, 'Negations ': 0.7265851497650146, 'Reentrancies ': 0.8470715880393982, 'SRL ': 0.6255624890327454, 'Smatch ': 0.7194496989250183, 'Unlabeled ': 0.3004220426082611, 'max_indegree_sim': 0.4996986985206604, 'max_outdegree_sim': 0.2771126925945282, 'max_degree_sim': 0.7257698178291321, 'root_sim': 0.3638014495372772, 'quant_sim': 0.6551890969276428, 'score_wlk': 0.7395667433738708, 'score_wwlk': 0.2378854602575302, 'residual': 0.5303235650062561, 'sent_a': 'This is a very unusual request.', 'sent_b': 'This sounds a bit unusual.'}\n",
      "{'global': 0.34839972853660583, 'Concepts ': 0.4350663423538208, 'Frames ': 0.43910980224609375, 'Named Ent. ': -0.07623907178640366, 'Negations ': 0.5527410507202148, 'Reentrancies ': 0.13147573173046112, 'SRL ': 0.11334946006536484, 'Smatch ': 0.3729363977909088, 'Unlabeled ': -0.754266083240509, 'max_indegree_sim': -0.055073898285627365, 'max_outdegree_sim': -0.3917125463485718, 'max_degree_sim': 0.5843877196311951, 'root_sim': 0.4059737026691437, 'quant_sim': 0.14500713348388672, 'score_wlk': 0.3836134076118469, 'score_wwlk': 0.5133594870567322, 'residual': 0.35650527477264404, 'sent_a': 'It very much depends on the grant in question.', 'sent_b': 'I think it depends very much on the area.'}\n",
      "{'global': 0.6795861721038818, 'Concepts ': 0.681321918964386, 'Frames ': 0.646328866481781, 'Named Ent. ': 0.6322365999221802, 'Negations ': 0.7124097347259521, 'Reentrancies ': 0.6101526618003845, 'SRL ': 0.8629540801048279, 'Smatch ': 0.5692337155342102, 'Unlabeled ': 0.5928065180778503, 'max_indegree_sim': 0.766152024269104, 'max_outdegree_sim': 0.7852509021759033, 'max_degree_sim': 0.5638127326965332, 'root_sim': 0.7932122945785522, 'quant_sim': 0.7898824214935303, 'score_wlk': 0.4783957600593567, 'score_wwlk': 0.5552292466163635, 'residual': 0.6809093356132507, 'sent_a': 'I have the same thing.', 'sent_b': 'I have had the same problem.'}\n",
      "{'global': 0.2450849860906601, 'Concepts ': -0.1359589695930481, 'Frames ': 0.21894945204257965, 'Named Ent. ': 0.055816102772951126, 'Negations ': 0.15526914596557617, 'Reentrancies ': 0.8719152808189392, 'SRL ': 0.33372801542282104, 'Smatch ': -0.11217011511325836, 'Unlabeled ': 0.7270398139953613, 'max_indegree_sim': 0.10660924017429352, 'max_outdegree_sim': 0.07022137194871902, 'max_degree_sim': 0.21385733783245087, 'root_sim': 0.5358898043632507, 'quant_sim': 0.0013666223967447877, 'score_wlk': 0.1443546861410141, 'score_wwlk': 0.35162726044654846, 'residual': 0.25726091861724854, 'sent_a': 'No it does not affect your ratings.', 'sent_b': 'No it is not.'}\n",
      "{'global': 0.8452837467193604, 'Concepts ': 0.732535719871521, 'Frames ': 0.6923742890357971, 'Named Ent. ': 0.7824491262435913, 'Negations ': 0.7975602149963379, 'Reentrancies ': 0.7429503798484802, 'SRL ': 0.8172720074653625, 'Smatch ': 0.9646520614624023, 'Unlabeled ': 0.9526281356811523, 'max_indegree_sim': 0.6445472836494446, 'max_outdegree_sim': 0.47907555103302, 'max_degree_sim': 0.8697539567947388, 'root_sim': 0.7752321362495422, 'quant_sim': 0.7388635873794556, 'score_wlk': 0.8240059614181519, 'score_wwlk': 0.7652643322944641, 'residual': 0.8510493040084839, 'sent_a': 'My answer to your question is \"Probably Not\".', 'sent_b': 'I think that the short  answer to your question is: No.'}\n",
      "{'global': 0.9282305836677551, 'Concepts ': 0.9686203598976135, 'Frames ': 0.9280418157577515, 'Named Ent. ': 0.9424317479133606, 'Negations ': 0.9296612739562988, 'Reentrancies ': 0.9655525088310242, 'SRL ': 0.9340196251869202, 'Smatch ': 0.9677190184593201, 'Unlabeled ': 0.9457920789718628, 'max_indegree_sim': 0.9489319920539856, 'max_outdegree_sim': 0.905079185962677, 'max_degree_sim': 0.8750264644622803, 'root_sim': 0.9299977421760559, 'quant_sim': 0.9746613502502441, 'score_wlk': 0.9055458903312683, 'score_wwlk': 0.8859146237373352, 'residual': 0.9291424751281738, 'sent_a': \"I don't think it makes any tremendous difference.\", 'sent_b': \"I don't think it makes much difference.\"}\n",
      "{'global': 0.26879239082336426, 'Concepts ': -0.01190432719886303, 'Frames ': 0.3512119650840759, 'Named Ent. ': -0.027094444260001183, 'Negations ': 0.37579551339149475, 'Reentrancies ': 0.5450112819671631, 'SRL ': 0.5042334198951721, 'Smatch ': 0.4682740867137909, 'Unlabeled ': -0.05030474439263344, 'max_indegree_sim': 0.14602254331111908, 'max_outdegree_sim': 0.2421332597732544, 'max_degree_sim': -0.27672967314720154, 'root_sim': 0.4335942566394806, 'quant_sim': -0.0451473630964756, 'score_wlk': 0.032255519181489944, 'score_wwlk': 0.21918675303459167, 'residual': 0.2832891345024109, 'sent_a': 'It depends on what they are.', 'sent_b': 'It depends on what they are evaluating, and how.'}\n",
      "{'global': 0.29015886783599854, 'Concepts ': 0.0015524643240496516, 'Frames ': 0.3803270161151886, 'Named Ent. ': 0.2189234495162964, 'Negations ': 0.3526962995529175, 'Reentrancies ': 0.7920676469802856, 'SRL ': 0.613716185092926, 'Smatch ': 0.5393362641334534, 'Unlabeled ': 0.696647047996521, 'max_indegree_sim': 0.25960829854011536, 'max_outdegree_sim': 0.10354436933994293, 'max_degree_sim': 0.5728493928909302, 'root_sim': -0.355375200510025, 'quant_sim': 0.2611388862133026, 'score_wlk': 0.24693500995635986, 'score_wwlk': 0.4368914067745209, 'residual': 0.28320157527923584, 'sent_a': \"There's not a lot you can do about that.\", 'sent_b': \"There's not that much that you can do with a sourdough starter.\"}\n",
      "{'global': 0.9478879570960999, 'Concepts ': 0.9494934678077698, 'Frames ': 0.9665656089782715, 'Named Ent. ': 0.9071076512336731, 'Negations ': 0.9228202700614929, 'Reentrancies ': 0.9248095154762268, 'SRL ': 0.9628636837005615, 'Smatch ': 0.8296516537666321, 'Unlabeled ': 0.8446354866027832, 'max_indegree_sim': 0.9062488675117493, 'max_outdegree_sim': 0.8694167733192444, 'max_degree_sim': 0.9443732500076294, 'root_sim': 0.8989241123199463, 'quant_sim': 0.961642861366272, 'score_wlk': 0.9407011270523071, 'score_wwlk': 0.9398927092552185, 'residual': 0.9509857892990112, 'sent_a': 'You answered your own question.', 'sent_b': 'You have answered your own question.'}\n",
      "{'global': 0.42277416586875916, 'Concepts ': 0.4673740863800049, 'Frames ': 0.3162965774536133, 'Named Ent. ': 0.05945926532149315, 'Negations ': 0.020576348528265953, 'Reentrancies ': 0.6156163811683655, 'SRL ': 0.5668817162513733, 'Smatch ': 0.7785536050796509, 'Unlabeled ': 0.6786201000213623, 'max_indegree_sim': 0.4908733665943146, 'max_outdegree_sim': 0.8405975103378296, 'max_degree_sim': 0.589773952960968, 'root_sim': 0.2286728322505951, 'quant_sim': 0.5070173740386963, 'score_wlk': 0.6394475698471069, 'score_wwlk': 0.7415739893913269, 'residual': 0.41531699895858765, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': \"Th answer to you problem is that you dont actually know what you're getting in .\"}\n",
      "{'global': 0.6870932579040527, 'Concepts ': 0.672254741191864, 'Frames ': 0.383847177028656, 'Named Ent. ': 0.5777689814567566, 'Negations ': 0.6634146571159363, 'Reentrancies ': 0.8328524231910706, 'SRL ': 0.5421032309532166, 'Smatch ': 0.45943278074264526, 'Unlabeled ': 0.07283167541027069, 'max_indegree_sim': 0.764565646648407, 'max_outdegree_sim': 0.17209072411060333, 'max_degree_sim': 0.46041011810302734, 'root_sim': 0.5910419225692749, 'quant_sim': 0.5773247480392456, 'score_wlk': 0.7376956343650818, 'score_wwlk': 0.14769145846366882, 'residual': 0.7226842641830444, 'sent_a': 'This is not a good idea.', 'sent_b': 'This is probably not a good idea but I will suggest it anyhow.'}\n",
      "{'global': 0.3156423270702362, 'Concepts ': 0.2788061499595642, 'Frames ': 0.551058828830719, 'Named Ent. ': 0.5025461316108704, 'Negations ': 0.6026657223701477, 'Reentrancies ': 0.6076253056526184, 'SRL ': 0.4748118817806244, 'Smatch ': -0.5542129874229431, 'Unlabeled ': 0.03235817700624466, 'max_indegree_sim': 0.6522693037986755, 'max_outdegree_sim': 0.468748539686203, 'max_degree_sim': 0.6638866066932678, 'root_sim': 0.7626119256019592, 'quant_sim': 0.6026636958122253, 'score_wlk': 0.7333351969718933, 'score_wwlk': 0.5037895441055298, 'residual': 0.2958071231842041, 'sent_a': 'The best thing you can do is to know your stuff.', 'sent_b': 'My recommendation is not to say anything, and do the best you can.'}\n",
      "{'global': 0.6608457565307617, 'Concepts ': 0.5570930242538452, 'Frames ': 0.8059707880020142, 'Named Ent. ': 0.695579469203949, 'Negations ': 0.5133352279663086, 'Reentrancies ': 0.834159255027771, 'SRL ': 0.480158269405365, 'Smatch ': 0.6275520324707031, 'Unlabeled ': -0.20375606417655945, 'max_indegree_sim': 0.7220489382743835, 'max_outdegree_sim': 0.4116111099720001, 'max_degree_sim': 0.5723625421524048, 'root_sim': 0.8185890913009644, 'quant_sim': 0.7188123464584351, 'score_wlk': 0.7565481662750244, 'score_wwlk': 0.5849851965904236, 'residual': 0.6640474796295166, 'sent_a': \"I'd say it primarily depends on two things:\", 'sent_b': \"I'd say it depends what conditions you have.\"}\n",
      "{'global': 0.331510066986084, 'Concepts ': 0.6749911904335022, 'Frames ': 0.4082258343696594, 'Named Ent. ': -0.07769891619682312, 'Negations ': 0.3897484242916107, 'Reentrancies ': 0.5761758685112, 'SRL ': 0.6338240504264832, 'Smatch ': 0.6324006915092468, 'Unlabeled ': 0.7673824429512024, 'max_indegree_sim': 0.31213298439979553, 'max_outdegree_sim': 0.35695880651474, 'max_degree_sim': 0.14062753319740295, 'root_sim': 0.14696790277957916, 'quant_sim': 0.5742326974868774, 'score_wlk': 0.35149678587913513, 'score_wwlk': 0.48977556824684143, 'residual': 0.30060774087905884, 'sent_a': \"You don't have to know.\", 'sent_b': \"You don't have to do anything to season it.\"}\n",
      "{'global': 0.23863109946250916, 'Concepts ': 0.4511605203151703, 'Frames ': 0.4491129517555237, 'Named Ent. ': -0.23283472657203674, 'Negations ': 0.464814692735672, 'Reentrancies ': 0.7722439169883728, 'SRL ': 0.542622983455658, 'Smatch ': -0.1554248332977295, 'Unlabeled ': 0.46700698137283325, 'max_indegree_sim': 0.08516398817300797, 'max_outdegree_sim': 0.5300064086914062, 'max_degree_sim': 0.015856558457016945, 'root_sim': 0.3427681028842926, 'quant_sim': 0.6361515522003174, 'score_wlk': 0.360246866941452, 'score_wwlk': 0.3071833550930023, 'residual': 0.2261846661567688, 'sent_a': 'I have the same thing.', 'sent_b': 'I have the same situation and have traveled extensively.'}\n",
      "{'global': 0.9204698204994202, 'Concepts ': 0.9317737817764282, 'Frames ': 0.8637427687644958, 'Named Ent. ': 0.9353774189949036, 'Negations ': 0.7997115254402161, 'Reentrancies ': 0.9184458255767822, 'SRL ': 0.8365343809127808, 'Smatch ': 0.9098861217498779, 'Unlabeled ': 0.9698516130447388, 'max_indegree_sim': 0.8808405995368958, 'max_outdegree_sim': 0.9609172940254211, 'max_degree_sim': 0.8438553214073181, 'root_sim': 0.7897125482559204, 'quant_sim': 0.9230889678001404, 'score_wlk': 0.9320362210273743, 'score_wwlk': 0.8941177129745483, 'residual': 0.9233081340789795, 'sent_a': \"It's not a good idea.\", 'sent_b': \"I do not think it's a good idea.\"}\n",
      "{'global': 0.3241797685623169, 'Concepts ': 0.6688112616539001, 'Frames ': 0.7115209698677063, 'Named Ent. ': -0.14681918919086456, 'Negations ': 0.30390340089797974, 'Reentrancies ': 0.6204308867454529, 'SRL ': 0.13605031371116638, 'Smatch ': 0.20624135434627533, 'Unlabeled ': 0.3204854130744934, 'max_indegree_sim': 0.36055946350097656, 'max_outdegree_sim': 0.05849674344062805, 'max_degree_sim': 0.261258602142334, 'root_sim': 0.4741189181804657, 'quant_sim': 0.6073952317237854, 'score_wlk': 0.5047297477722168, 'score_wwlk': 0.5396730303764343, 'residual': 0.3169243037700653, 'sent_a': 'You are on the right path.', 'sent_b': 'You are right on the mark.'}\n",
      "{'global': 0.5527589917182922, 'Concepts ': 0.6484782099723816, 'Frames ': 0.5779733657836914, 'Named Ent. ': 0.7456254959106445, 'Negations ': 0.6890935897827148, 'Reentrancies ': 0.6722133755683899, 'SRL ': 0.3567187786102295, 'Smatch ': 0.5480956435203552, 'Unlabeled ': 0.40163370966911316, 'max_indegree_sim': 0.571595311164856, 'max_outdegree_sim': 0.1028042584657669, 'max_degree_sim': 0.06561201065778732, 'root_sim': 0.5825346112251282, 'quant_sim': 0.40416669845581055, 'score_wlk': 0.6387887001037598, 'score_wwlk': 0.5230183601379395, 'residual': 0.5629496574401855, 'sent_a': \"This doesn't answer your question, but:\", 'sent_b': 'This is a part answer to your question'}\n",
      "{'global': 0.6316236853599548, 'Concepts ': 0.49765390157699585, 'Frames ': 0.27915021777153015, 'Named Ent. ': 0.2303352952003479, 'Negations ': 0.6926819086074829, 'Reentrancies ': 0.7813466191291809, 'SRL ': 0.4981936812400818, 'Smatch ': 0.48153308033943176, 'Unlabeled ': 0.5252839922904968, 'max_indegree_sim': 0.2849016487598419, 'max_outdegree_sim': 0.886987030506134, 'max_degree_sim': 0.707390308380127, 'root_sim': 0.6911649703979492, 'quant_sim': 0.7104828953742981, 'score_wlk': 0.6775632500648499, 'score_wwlk': 0.6482781767845154, 'residual': 0.6360675096511841, 'sent_a': 'How should I proceed about this?', 'sent_b': 'So how should I do this?'}\n",
      "{'global': 0.6247557401657104, 'Concepts ': 0.47120165824890137, 'Frames ': 0.19658061861991882, 'Named Ent. ': 0.38930851221084595, 'Negations ': 0.614072322845459, 'Reentrancies ': 0.5914286375045776, 'SRL ': 0.5092965960502625, 'Smatch ': 0.939858615398407, 'Unlabeled ': 0.8492595553398132, 'max_indegree_sim': 0.578172504901886, 'max_outdegree_sim': 0.014207975938916206, 'max_degree_sim': 0.7889081239700317, 'root_sim': 0.46513980627059937, 'quant_sim': 0.2752402126789093, 'score_wlk': 0.7282583117485046, 'score_wwlk': 0.7988754510879517, 'residual': 0.6270390152931213, 'sent_a': 'Does this page answer your question?', 'sent_b': 'Does this answer your questions?'}\n",
      "{'global': 0.44000861048698425, 'Concepts ': 0.24114829301834106, 'Frames ': 0.3402848541736603, 'Named Ent. ': 0.5595268607139587, 'Negations ': 0.802631676197052, 'Reentrancies ': 0.6886301040649414, 'SRL ': 0.7158008217811584, 'Smatch ': 0.8959342837333679, 'Unlabeled ': 0.5470204949378967, 'max_indegree_sim': 0.22497424483299255, 'max_outdegree_sim': 0.44770485162734985, 'max_degree_sim': 0.615714430809021, 'root_sim': 0.48356783390045166, 'quant_sim': 0.5853332281112671, 'score_wlk': 0.7347686290740967, 'score_wwlk': 0.8094666004180908, 'residual': 0.42651572823524475, 'sent_a': 'You can use it, too.', 'sent_b': 'You can still use it for practice.'}\n",
      "{'global': 0.2748226821422577, 'Concepts ': 0.12065351009368896, 'Frames ': -0.04174983873963356, 'Named Ent. ': 0.7537891864776611, 'Negations ': 0.20925064384937286, 'Reentrancies ': 0.7987521290779114, 'SRL ': -0.14350074529647827, 'Smatch ': 0.5470766425132751, 'Unlabeled ': 0.32647067308425903, 'max_indegree_sim': -0.14631305634975433, 'max_outdegree_sim': 0.46632325649261475, 'max_degree_sim': 0.2695239186286926, 'root_sim': 0.41629648208618164, 'quant_sim': 0.471603661775589, 'score_wlk': 0.6854955554008484, 'score_wwlk': 0.49750834703445435, 'residual': 0.268684059381485, 'sent_a': 'It really depends on how the employer documents it.', 'sent_b': \"It depends how you're stating it.\"}\n",
      "{'global': 0.37370991706848145, 'Concepts ': 0.5480642318725586, 'Frames ': 0.7856148481369019, 'Named Ent. ': 0.405490905046463, 'Negations ': 0.6783859729766846, 'Reentrancies ': 0.38928884267807007, 'SRL ': 0.5244131088256836, 'Smatch ': 0.3885962665081024, 'Unlabeled ': 0.2314911037683487, 'max_indegree_sim': -0.13514107465744019, 'max_outdegree_sim': 0.39120686054229736, 'max_degree_sim': 0.4762257933616638, 'root_sim': 0.25836876034736633, 'quant_sim': 0.6489152908325195, 'score_wlk': 0.6166247129440308, 'score_wwlk': 0.5570616126060486, 'residual': 0.3586316704750061, 'sent_a': \"I've had this same problem.\", 'sent_b': \"I've had this problem while working in a pubs.\"}\n",
      "{'global': 0.3003959059715271, 'Concepts ': 0.30368053913116455, 'Frames ': 0.5821259617805481, 'Named Ent. ': 0.2051345407962799, 'Negations ': 0.10371078550815582, 'Reentrancies ': 0.7864173054695129, 'SRL ': 0.6692250370979309, 'Smatch ': 0.5812143087387085, 'Unlabeled ': 0.6342995762825012, 'max_indegree_sim': 0.5309043526649475, 'max_outdegree_sim': 0.48214805126190186, 'max_degree_sim': 0.48484137654304504, 'root_sim': 0.45256295800209045, 'quant_sim': 0.2284402996301651, 'score_wlk': 0.6052863001823425, 'score_wwlk': 0.17221875488758087, 'residual': 0.2826575040817261, 'sent_a': \"You need to read a lot to know what you like and what you don't.\", 'sent_b': 'You should tell a good story and sometimes you have to tweak reality/history to do so.'}\n",
      "{'global': 0.6802862882614136, 'Concepts ': 0.7582170963287354, 'Frames ': 0.6933178901672363, 'Named Ent. ': 0.7058137655258179, 'Negations ': 0.5460646152496338, 'Reentrancies ': 0.8764523267745972, 'SRL ': 0.7042253017425537, 'Smatch ': 0.8592002391815186, 'Unlabeled ': 0.7115208506584167, 'max_indegree_sim': 0.890125572681427, 'max_outdegree_sim': 0.8330915570259094, 'max_degree_sim': 0.6068275570869446, 'root_sim': 0.7998713850975037, 'quant_sim': 0.8332433104515076, 'score_wlk': 0.8606314659118652, 'score_wwlk': 0.7314725518226624, 'residual': 0.6700199842453003, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'It depends on what you want to be able to do.'}\n",
      "{'global': 0.7816953659057617, 'Concepts ': 0.8071939945220947, 'Frames ': 0.8149569034576416, 'Named Ent. ': 0.6904631853103638, 'Negations ': 0.8144480586051941, 'Reentrancies ': 0.732236385345459, 'SRL ': 0.964676022529602, 'Smatch ': 0.22950483858585358, 'Unlabeled ': 0.6235554218292236, 'max_indegree_sim': 0.6037763357162476, 'max_outdegree_sim': 0.8575003743171692, 'max_degree_sim': 0.8491350412368774, 'root_sim': 0.8173697590827942, 'quant_sim': 0.8059067726135254, 'score_wlk': 0.6984961032867432, 'score_wwlk': 0.579114556312561, 'residual': 0.8003683686256409, 'sent_a': 'Yes, you have to file a tax return in Canada.', 'sent_b': 'You are not required to file a tax return in Canada if you have no taxable income.'}\n",
      "{'global': 0.8536571264266968, 'Concepts ': 0.8770816922187805, 'Frames ': 0.8434262871742249, 'Named Ent. ': 0.8772241473197937, 'Negations ': 0.8270639777183533, 'Reentrancies ': 0.9310683012008667, 'SRL ': 0.808272659778595, 'Smatch ': 0.9058054685592651, 'Unlabeled ': 0.9153890609741211, 'max_indegree_sim': 0.8963881731033325, 'max_outdegree_sim': 0.813676655292511, 'max_degree_sim': 0.6329699754714966, 'root_sim': 0.8512788414955139, 'quant_sim': 0.8432135581970215, 'score_wlk': 0.8922651410102844, 'score_wwlk': 0.413411021232605, 'residual': 0.8562631011009216, 'sent_a': \"I don't see why there should be any problem with this whatsoever.\", 'sent_b': \"I don't see why that should be a problem.\"}\n",
      "{'global': 0.10162019729614258, 'Concepts ': 0.3676255941390991, 'Frames ': -0.21921144425868988, 'Named Ent. ': 0.14620517194271088, 'Negations ': 0.11057312786579132, 'Reentrancies ': 0.1876988410949707, 'SRL ': 0.2727530002593994, 'Smatch ': -0.2609057128429413, 'Unlabeled ': -0.6487846374511719, 'max_indegree_sim': 0.3260762095451355, 'max_outdegree_sim': 0.12254343926906586, 'max_degree_sim': -0.05809706076979637, 'root_sim': 0.1128561794757843, 'quant_sim': 0.47660189867019653, 'score_wlk': -0.2247968316078186, 'score_wwlk': 0.0006409414345398545, 'residual': 0.10507935285568237, 'sent_a': 'Hope this is what you are looking for.', 'sent_b': 'If what you are looking for is much higher, they get the picture.'}\n",
      "{'global': 0.38984936475753784, 'Concepts ': 0.3043529689311981, 'Frames ': 0.6768036484718323, 'Named Ent. ': 0.32181885838508606, 'Negations ': 0.5207451581954956, 'Reentrancies ': 0.5715656876564026, 'SRL ': 0.679046630859375, 'Smatch ': 0.7055038213729858, 'Unlabeled ': 0.8286246061325073, 'max_indegree_sim': 0.563717246055603, 'max_outdegree_sim': 0.5127993822097778, 'max_degree_sim': 0.44854801893234253, 'root_sim': 0.4355490803718567, 'quant_sim': 0.6027441024780273, 'score_wlk': 0.5249562859535217, 'score_wwlk': 0.6320004463195801, 'residual': 0.3756476640701294, 'sent_a': 'The best thing you can do is to know your stuff.', 'sent_b': 'The best thing to do is to overcome the fussiness.'}\n",
      "{'global': 0.425772100687027, 'Concepts ': 0.15831002593040466, 'Frames ': 0.3767363429069519, 'Named Ent. ': 0.6212620735168457, 'Negations ': 0.4582275450229645, 'Reentrancies ': 0.7577035427093506, 'SRL ': 0.6193323135375977, 'Smatch ': 0.7658329010009766, 'Unlabeled ': 0.4029242694377899, 'max_indegree_sim': 0.6819806694984436, 'max_outdegree_sim': 0.38263052701950073, 'max_degree_sim': 0.5265864133834839, 'root_sim': 0.41164350509643555, 'quant_sim': 0.20327362418174744, 'score_wlk': 0.3653682470321655, 'score_wwlk': 0.4861364960670471, 'residual': 0.42228251695632935, 'sent_a': 'It depends on the dish and how amenable it is at the stage you make the mistake.', 'sent_b': 'It depends on the sauce and the result you want.'}\n",
      "{'global': 0.21983493864536285, 'Concepts ': 0.10791544616222382, 'Frames ': 0.4248754382133484, 'Named Ent. ': 0.058603864163160324, 'Negations ': 0.29495418071746826, 'Reentrancies ': 0.5558485984802246, 'SRL ': 0.5856565833091736, 'Smatch ': 0.04792099446058273, 'Unlabeled ': 0.7177717089653015, 'max_indegree_sim': 0.013704146258533001, 'max_outdegree_sim': 0.3543989658355713, 'max_degree_sim': 0.5090259909629822, 'root_sim': 0.34749865531921387, 'quant_sim': 0.43938952684402466, 'score_wlk': 0.48544687032699585, 'score_wwlk': 0.2727426588535309, 'residual': 0.20655791461467743, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'You have to do what is right for you.'}\n",
      "{'global': 0.05463770404458046, 'Concepts ': 0.013302088715136051, 'Frames ': 0.37669840455055237, 'Named Ent. ': -0.5326324105262756, 'Negations ': 0.04279209300875664, 'Reentrancies ': 0.41307878494262695, 'SRL ': 0.16847845911979675, 'Smatch ': -0.16744180023670197, 'Unlabeled ': 0.023568658158183098, 'max_indegree_sim': 0.10192067176103592, 'max_outdegree_sim': -0.043982986360788345, 'max_degree_sim': -0.19915352761745453, 'root_sim': -0.505011260509491, 'quant_sim': 0.0751795545220375, 'score_wlk': 0.07342273741960526, 'score_wwlk': -0.2523886263370514, 'residual': 0.07551520317792892, 'sent_a': \"You PROBABLY don't have any chance at the moment.\", 'sent_b': 'Saying \"thanks, I don\\'t have any questions at the moment.\"'}\n",
      "{'global': 0.5110142827033997, 'Concepts ': 0.650054931640625, 'Frames ': 0.7978888750076294, 'Named Ent. ': 0.3692905306816101, 'Negations ': 0.3177235722541809, 'Reentrancies ': 0.7536801099777222, 'SRL ': 0.18223616480827332, 'Smatch ': 0.26256948709487915, 'Unlabeled ': 0.35817962884902954, 'max_indegree_sim': 0.7131392359733582, 'max_outdegree_sim': 0.48859894275665283, 'max_degree_sim': 0.3451119065284729, 'root_sim': 0.6942824721336365, 'quant_sim': 0.589076817035675, 'score_wlk': 0.7261261343955994, 'score_wwlk': 0.4141332805156708, 'residual': 0.5167413949966431, 'sent_a': \"It really doesn't matter.\", 'sent_b': \"It doesn't matter unless it is really far off.\"}\n",
      "{'global': 0.796298623085022, 'Concepts ': 0.8518171906471252, 'Frames ': 0.748123824596405, 'Named Ent. ': 0.658328115940094, 'Negations ': 0.7782730460166931, 'Reentrancies ': 0.9569775462150574, 'SRL ': 0.8246637582778931, 'Smatch ': 0.9092101454734802, 'Unlabeled ': 0.9389533400535583, 'max_indegree_sim': 0.9126029014587402, 'max_outdegree_sim': 0.8887621760368347, 'max_degree_sim': 0.7565421462059021, 'root_sim': 0.5841201543807983, 'quant_sim': 0.7079460024833679, 'score_wlk': 0.8617433309555054, 'score_wwlk': 0.8622028231620789, 'residual': 0.7832064032554626, 'sent_a': \"You don't need to know everything.\", 'sent_b': \"You don't have to know.\"}\n",
      "{'global': 0.5757205486297607, 'Concepts ': 0.35073280334472656, 'Frames ': 0.8120012283325195, 'Named Ent. ': 0.6543422937393188, 'Negations ': 0.5519019365310669, 'Reentrancies ': -0.3526918590068817, 'SRL ': -0.09749160706996918, 'Smatch ': 0.5034805536270142, 'Unlabeled ': 0.47420284152030945, 'max_indegree_sim': 0.4809154272079468, 'max_outdegree_sim': 0.5198817253112793, 'max_degree_sim': 0.6953835487365723, 'root_sim': 0.8572970628738403, 'quant_sim': 0.7066683173179626, 'score_wlk': 0.5538122057914734, 'score_wwlk': 0.2808558940887451, 'residual': 0.5841894149780273, 'sent_a': \"I think you're looking for Mikey (1992).\", 'sent_b': \"I think you're looking for the movie\"}\n",
      "{'global': 0.9018113613128662, 'Concepts ': 0.945384681224823, 'Frames ': 0.9683395624160767, 'Named Ent. ': 0.9165955185890198, 'Negations ': 0.9224579930305481, 'Reentrancies ': 0.9092828035354614, 'SRL ': 0.7730253338813782, 'Smatch ': 0.8511958122253418, 'Unlabeled ': 0.9167912602424622, 'max_indegree_sim': 0.7903656363487244, 'max_outdegree_sim': 0.5904443860054016, 'max_degree_sim': 0.9055928587913513, 'root_sim': 0.8683232665061951, 'quant_sim': 0.813632607460022, 'score_wlk': 0.9467492699623108, 'score_wwlk': 0.9081547856330872, 'residual': 0.9067665338516235, 'sent_a': 'It makes absolutely NO difference.', 'sent_b': 'No, it makes no difference.'}\n",
      "{'global': 0.8808292150497437, 'Concepts ': 0.9160191416740417, 'Frames ': 0.9485525488853455, 'Named Ent. ': 0.9330894947052002, 'Negations ': 0.9259501695632935, 'Reentrancies ': 0.9531542658805847, 'SRL ': 0.7358537316322327, 'Smatch ': 0.9812964797019958, 'Unlabeled ': 0.9306986331939697, 'max_indegree_sim': 0.8011294603347778, 'max_outdegree_sim': 0.8334963321685791, 'max_degree_sim': 0.912733256816864, 'root_sim': 0.8267684578895569, 'quant_sim': 0.8691725730895996, 'score_wlk': 0.8809031844139099, 'score_wwlk': 0.8515967130661011, 'residual': 0.8789183497428894, 'sent_a': \"I think it's fine to ask this question.\", 'sent_b': 'I think it is okay to ask the question.'}\n",
      "{'global': 0.42504459619522095, 'Concepts ': 0.6557897329330444, 'Frames ': 0.2671131491661072, 'Named Ent. ': 0.41981345415115356, 'Negations ': 0.5493566989898682, 'Reentrancies ': 0.8032687306404114, 'SRL ': 0.64752596616745, 'Smatch ': 0.13772831857204437, 'Unlabeled ': 0.5608090758323669, 'max_indegree_sim': 0.24328671395778656, 'max_outdegree_sim': 0.09373940527439117, 'max_degree_sim': 0.5549325942993164, 'root_sim': 0.5020265579223633, 'quant_sim': 0.30496302247047424, 'score_wlk': 0.5161998867988586, 'score_wwlk': 0.7105534076690674, 'residual': 0.42347487807273865, 'sent_a': \"I'm going to be very direct here.\", 'sent_b': \"I'm going to be blunt, here: You don't.\"}\n",
      "{'global': 0.16237027943134308, 'Concepts ': 0.03873618319630623, 'Frames ': 0.8228267431259155, 'Named Ent. ': 0.15241260826587677, 'Negations ': 0.2959434986114502, 'Reentrancies ': 0.7252105474472046, 'SRL ': 0.5465583801269531, 'Smatch ': -0.22453752160072327, 'Unlabeled ': -0.4201798141002655, 'max_indegree_sim': -0.4961812198162079, 'max_outdegree_sim': 0.5058642029762268, 'max_degree_sim': 0.24326159060001373, 'root_sim': 0.5436729192733765, 'quant_sim': 0.3969481587409973, 'score_wlk': 0.3044065237045288, 'score_wwlk': 0.6196587085723877, 'residual': 0.15692807734012604, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'Yes, you can do exactly what you want to do.'}\n",
      "{'global': 0.3154929280281067, 'Concepts ': 0.5041285157203674, 'Frames ': 0.1968277394771576, 'Named Ent. ': 0.3097975552082062, 'Negations ': -0.07613897323608398, 'Reentrancies ': 0.7529179453849792, 'SRL ': 0.2733805179595947, 'Smatch ': 0.38462701439857483, 'Unlabeled ': 0.35519570112228394, 'max_indegree_sim': 0.2816600501537323, 'max_outdegree_sim': 0.35324668884277344, 'max_degree_sim': -0.004508455749601126, 'root_sim': -0.04757646098732948, 'quant_sim': 0.5395583510398865, 'score_wlk': 0.4340370297431946, 'score_wwlk': 0.44863563776016235, 'residual': 0.32329195737838745, 'sent_a': 'You should do it.', 'sent_b': 'You should prime it first.'}\n",
      "{'global': 0.8103445768356323, 'Concepts ': 0.8520234227180481, 'Frames ': 0.5796566605567932, 'Named Ent. ': 0.6083358526229858, 'Negations ': 0.7963231205940247, 'Reentrancies ': 0.7922938466072083, 'SRL ': 0.9131637811660767, 'Smatch ': 0.9235082268714905, 'Unlabeled ': 0.7960264682769775, 'max_indegree_sim': 0.9056726098060608, 'max_outdegree_sim': 0.7925276756286621, 'max_degree_sim': 0.728581428527832, 'root_sim': 0.7641280889511108, 'quant_sim': 0.8090070486068726, 'score_wlk': 0.7093145847320557, 'score_wwlk': 0.8955076336860657, 'residual': 0.8158438205718994, 'sent_a': \"There's not a lot you can do about that.\", 'sent_b': \"I'm afraid there's not really a lot you can do.\"}\n",
      "{'global': 0.3449251651763916, 'Concepts ': 0.6714346408843994, 'Frames ': 0.15341205894947052, 'Named Ent. ': 0.1354372352361679, 'Negations ': 0.4053872525691986, 'Reentrancies ': 0.550209105014801, 'SRL ': 0.23761729896068573, 'Smatch ': 0.8200589418411255, 'Unlabeled ': 0.15274134278297424, 'max_indegree_sim': 0.24310928583145142, 'max_outdegree_sim': 0.5090526342391968, 'max_degree_sim': 0.47568652033805847, 'root_sim': 0.3734113872051239, 'quant_sim': -0.11423369497060776, 'score_wlk': 0.532781183719635, 'score_wwlk': 0.46203041076660156, 'residual': 0.3432585895061493, 'sent_a': 'What kind of insulation is it?', 'sent_b': 'What kind of floors are above?'}\n",
      "{'global': 0.7951777577400208, 'Concepts ': 0.8354194164276123, 'Frames ': 0.7494791746139526, 'Named Ent. ': 0.9431776404380798, 'Negations ': 0.8063684701919556, 'Reentrancies ': 0.8658971786499023, 'SRL ': 0.7885832786560059, 'Smatch ': 0.7735130190849304, 'Unlabeled ': 0.488026887178421, 'max_indegree_sim': 0.8966132402420044, 'max_outdegree_sim': 0.7419118881225586, 'max_degree_sim': 0.8044688105583191, 'root_sim': 0.7360543608665466, 'quant_sim': 0.8711239695549011, 'score_wlk': 0.9215323328971863, 'score_wwlk': 0.765571653842926, 'residual': 0.7911118865013123, 'sent_a': 'It depends entirely on your company and your contract.', 'sent_b': 'I guess it depends on the nature of your contract.'}\n",
      "{'global': 0.9021628499031067, 'Concepts ': 0.927759051322937, 'Frames ': 0.8782192468643188, 'Named Ent. ': 0.8778935670852661, 'Negations ': 0.8717401027679443, 'Reentrancies ': 0.8750714659690857, 'SRL ': 0.9043481945991516, 'Smatch ': 0.7512259483337402, 'Unlabeled ': 0.7440839409828186, 'max_indegree_sim': 0.7452000379562378, 'max_outdegree_sim': 0.556523859500885, 'max_degree_sim': 0.9400397539138794, 'root_sim': 0.87826007604599, 'quant_sim': 0.9352946281433105, 'score_wlk': 0.8977797627449036, 'score_wwlk': 0.8779964447021484, 'residual': 0.9085914492607117, 'sent_a': 'You answered your own question.', 'sent_b': \"You've answered your own question already.\"}\n",
      "{'global': 0.22145983576774597, 'Concepts ': 0.37591123580932617, 'Frames ': -0.08659904450178146, 'Named Ent. ': 0.40959662199020386, 'Negations ': -0.2779391407966614, 'Reentrancies ': 0.34450677037239075, 'SRL ': -0.2046957015991211, 'Smatch ': -0.3356422185897827, 'Unlabeled ': -0.04356939718127251, 'max_indegree_sim': 0.17515908181667328, 'max_outdegree_sim': 0.27461329102516174, 'max_degree_sim': 0.6801419258117676, 'root_sim': 0.0073129902593791485, 'quant_sim': 0.596194326877594, 'score_wlk': 0.4463813304901123, 'score_wwlk': 0.05975385010242462, 'residual': 0.239660382270813, 'sent_a': \"I don't think that there's any.\", 'sent_b': \"I don't think there is any universal term.\"}\n",
      "{'global': 0.486551970243454, 'Concepts ': 0.42116183042526245, 'Frames ': 0.7392433285713196, 'Named Ent. ': 0.2966849207878113, 'Negations ': 0.3348565399646759, 'Reentrancies ': 0.708362340927124, 'SRL ': 0.714078426361084, 'Smatch ': 0.34646475315093994, 'Unlabeled ': -0.47745180130004883, 'max_indegree_sim': 0.42896488308906555, 'max_outdegree_sim': 0.2654639184474945, 'max_degree_sim': 0.3543558120727539, 'root_sim': 0.3395672142505646, 'quant_sim': 0.7185171842575073, 'score_wlk': 0.15302543342113495, 'score_wwlk': 0.5943424105644226, 'residual': 0.4941384792327881, 'sent_a': 'There are a few things you can do: ', 'sent_b': 'There are a few minimally-effective things you can do at the personal level.'}\n",
      "{'global': 0.7284501791000366, 'Concepts ': 0.8069812655448914, 'Frames ': 0.5571569800376892, 'Named Ent. ': 0.6616382002830505, 'Negations ': 0.771386981010437, 'Reentrancies ': 0.8323514461517334, 'SRL ': 0.7152822017669678, 'Smatch ': 0.7356422543525696, 'Unlabeled ': 0.5452448725700378, 'max_indegree_sim': 0.5040856599807739, 'max_outdegree_sim': 0.40151262283325195, 'max_degree_sim': 0.6831092238426208, 'root_sim': 0.5791972279548645, 'quant_sim': 0.7338426113128662, 'score_wlk': 0.8491243124008179, 'score_wwlk': 0.8700376152992249, 'residual': 0.7355627417564392, 'sent_a': 'This is the tip I find most useful:', 'sent_b': 'Something like this is useful:  '}\n",
      "{'global': 0.2522788345813751, 'Concepts ': 0.38870349526405334, 'Frames ': 0.4657336473464966, 'Named Ent. ': 0.15087835490703583, 'Negations ': 0.2433108687400818, 'Reentrancies ': 0.7704908847808838, 'SRL ': 0.6973245739936829, 'Smatch ': -0.10290726274251938, 'Unlabeled ': -0.673340916633606, 'max_indegree_sim': -0.004546442534774542, 'max_outdegree_sim': 0.3071124255657196, 'max_degree_sim': 0.22306296229362488, 'root_sim': 0.34843242168426514, 'quant_sim': 0.45562708377838135, 'score_wlk': 0.3621136248111725, 'score_wwlk': 0.22027097642421722, 'residual': 0.2524707615375519, 'sent_a': \"That's what I believe.\", 'sent_b': \"Yes, I believe it's a good idea.\"}\n",
      "{'global': 0.4017491936683655, 'Concepts ': 0.7891990542411804, 'Frames ': 0.562091052532196, 'Named Ent. ': 0.26426854729652405, 'Negations ': 0.14742803573608398, 'Reentrancies ': 0.7417100667953491, 'SRL ': 0.7200321555137634, 'Smatch ': 0.9194099307060242, 'Unlabeled ': 0.9435258507728577, 'max_indegree_sim': 0.4503169655799866, 'max_outdegree_sim': 0.43515315651893616, 'max_degree_sim': 0.6018895506858826, 'root_sim': -0.04029981046915054, 'quant_sim': 0.7728323936462402, 'score_wlk': 0.664456844329834, 'score_wwlk': 0.5539913773536682, 'residual': 0.37360554933547974, 'sent_a': \"If you are not sure how to do it, don't do it at all.\", 'sent_b': \"If not, don't do that and spend that time with something you like to do.\"}\n",
      "{'global': 0.20041680335998535, 'Concepts ': 0.13774964213371277, 'Frames ': 0.47699928283691406, 'Named Ent. ': 0.07587238401174545, 'Negations ': 0.45530349016189575, 'Reentrancies ': 0.35665327310562134, 'SRL ': 0.6025558114051819, 'Smatch ': 0.09142176061868668, 'Unlabeled ': 0.72824627161026, 'max_indegree_sim': 0.5332867503166199, 'max_outdegree_sim': 0.11988340318202972, 'max_degree_sim': 0.1031876802444458, 'root_sim': 0.5129594206809998, 'quant_sim': 0.7289556860923767, 'score_wlk': 0.6565601229667664, 'score_wwlk': 0.1855388879776001, 'residual': 0.18255755305290222, 'sent_a': \"It's all in her head.\", 'sent_b': \"It's all about adhesion.\"}\n",
      "{'global': 0.6024397611618042, 'Concepts ': 0.3730771243572235, 'Frames ': 0.7499068975448608, 'Named Ent. ': 0.4081636667251587, 'Negations ': 0.6575301289558411, 'Reentrancies ': 0.8957167267799377, 'SRL ': 0.6069954633712769, 'Smatch ': 0.9280967712402344, 'Unlabeled ': 0.8452556133270264, 'max_indegree_sim': 0.6854762434959412, 'max_outdegree_sim': 0.8554875254631042, 'max_degree_sim': 0.6462919116020203, 'root_sim': 0.7240592837333679, 'quant_sim': 0.6933746933937073, 'score_wlk': 0.7027357816696167, 'score_wwlk': 0.46110159158706665, 'residual': 0.583018958568573, 'sent_a': \"If you are not sure how to do it, don't do it at all.\", 'sent_b': \"If you do not have a very strong scientific reason to do it, don't.\"}\n",
      "{'global': 0.5589377284049988, 'Concepts ': 0.7540135383605957, 'Frames ': 0.8059785962104797, 'Named Ent. ': 0.28025922179222107, 'Negations ': 0.6008654832839966, 'Reentrancies ': 0.5924762487411499, 'SRL ': 0.6330249309539795, 'Smatch ': 0.3450907766819, 'Unlabeled ': 0.6883416771888733, 'max_indegree_sim': 0.6569511294364929, 'max_outdegree_sim': 0.8174002170562744, 'max_degree_sim': 0.5351681709289551, 'root_sim': 0.6865274906158447, 'quant_sim': 0.505729615688324, 'score_wlk': 0.7047961950302124, 'score_wwlk': 0.5731220245361328, 'residual': 0.5502776503562927, 'sent_a': \"You need to read a lot to know what you like and what you don't.\", 'sent_b': 'You have to know what you want to do.'}\n",
      "{'global': 0.47572824358940125, 'Concepts ': 0.32196012139320374, 'Frames ': 0.46161916851997375, 'Named Ent. ': 0.15579204261302948, 'Negations ': 0.049222182482481, 'Reentrancies ': 0.7932701110839844, 'SRL ': 0.34936925768852234, 'Smatch ': 0.2953357696533203, 'Unlabeled ': 0.3111872375011444, 'max_indegree_sim': 0.242894247174263, 'max_outdegree_sim': 0.15378302335739136, 'max_degree_sim': 0.6330633759498596, 'root_sim': 0.5397663116455078, 'quant_sim': 0.5774511694908142, 'score_wlk': 0.6835569739341736, 'score_wwlk': 0.4451190233230591, 'residual': 0.4997835159301758, 'sent_a': \"I would say you can do it, but it wouldn't be advised.\", 'sent_b': 'Personally, I would say not unless it suits you.'}\n",
      "{'global': 0.8257508873939514, 'Concepts ': 0.6899659633636475, 'Frames ': 0.8307908177375793, 'Named Ent. ': 0.43514737486839294, 'Negations ': 0.7870561480522156, 'Reentrancies ': 0.7291269302368164, 'SRL ': 0.8296630382537842, 'Smatch ': 0.9911550283432007, 'Unlabeled ': 0.9638987183570862, 'max_indegree_sim': 0.9077447652816772, 'max_outdegree_sim': 0.8006772398948669, 'max_degree_sim': 0.7366321682929993, 'root_sim': 0.775466799736023, 'quant_sim': 0.8088470697402954, 'score_wlk': 0.5926272869110107, 'score_wwlk': 0.7910550236701965, 'residual': 0.8226866722106934, 'sent_a': 'Can you do this?', 'sent_b': 'Can you do it?'}\n",
      "{'global': 0.4012773633003235, 'Concepts ': 0.7088143825531006, 'Frames ': 0.6844469308853149, 'Named Ent. ': 0.38809746503829956, 'Negations ': 0.5563789010047913, 'Reentrancies ': 0.8007670044898987, 'SRL ': 0.7756829261779785, 'Smatch ': 0.4524359107017517, 'Unlabeled ': 0.3888242244720459, 'max_indegree_sim': 0.6448527574539185, 'max_outdegree_sim': -0.04300319403409958, 'max_degree_sim': 0.5073385238647461, 'root_sim': 0.4557405412197113, 'quant_sim': 0.28039732575416565, 'score_wlk': 0.4365728497505188, 'score_wwlk': 0.32711443305015564, 'residual': 0.39159366488456726, 'sent_a': \"Sure, I've  done this very trip.\", 'sent_b': \"I've done this many dozens of times.\"}\n",
      "{'global': 0.6598821878433228, 'Concepts ': 0.8139553666114807, 'Frames ': 0.4036644995212555, 'Named Ent. ': 0.22350770235061646, 'Negations ': -0.051617514342069626, 'Reentrancies ': 0.6228801012039185, 'SRL ': 0.8148784041404724, 'Smatch ': 0.46120771765708923, 'Unlabeled ': 0.8667576909065247, 'max_indegree_sim': 0.6848079562187195, 'max_outdegree_sim': 0.3126085698604584, 'max_degree_sim': 0.5713380575180054, 'root_sim': 0.4159930348396301, 'quant_sim': 0.7108314633369446, 'score_wlk': 0.5758857131004333, 'score_wwlk': 0.7899207472801208, 'residual': 0.6821907758712769, 'sent_a': 'You can use it, too.', 'sent_b': 'You can do it, too.'}\n",
      "{'global': 0.8424965143203735, 'Concepts ': 0.8066328763961792, 'Frames ': 0.7014658451080322, 'Named Ent. ': 0.7502886652946472, 'Negations ': 0.8779252767562866, 'Reentrancies ': 0.9396634101867676, 'SRL ': 0.7089913487434387, 'Smatch ': 0.6923858523368835, 'Unlabeled ': 0.3094210624694824, 'max_indegree_sim': 0.8616823554039001, 'max_outdegree_sim': 0.863521158695221, 'max_degree_sim': 0.6970677971839905, 'root_sim': 0.7412897944450378, 'quant_sim': 0.894767165184021, 'score_wlk': 0.8872456550598145, 'score_wwlk': 0.818541407585144, 'residual': 0.8479957580566406, 'sent_a': 'How do you do that?', 'sent_b': 'How to do that?'}\n",
      "{'global': 0.7737005352973938, 'Concepts ': 0.7023101449012756, 'Frames ': 0.7091308236122131, 'Named Ent. ': 0.6666918396949768, 'Negations ': 0.8623005151748657, 'Reentrancies ': 0.8279611468315125, 'SRL ': 0.614061176776886, 'Smatch ': 0.8005590438842773, 'Unlabeled ': 0.7623515129089355, 'max_indegree_sim': 0.5785233378410339, 'max_outdegree_sim': 0.7211942672729492, 'max_degree_sim': 0.7177547812461853, 'root_sim': 0.5230882167816162, 'quant_sim': 0.6967343688011169, 'score_wlk': 0.7406967878341675, 'score_wwlk': 0.8699185252189636, 'residual': 0.7820455431938171, 'sent_a': \"I've located an article that might be of some help\", 'sent_b': 'I found some link that might be of help to you:'}\n",
      "{'global': 0.47703635692596436, 'Concepts ': 0.4160071313381195, 'Frames ': 0.43117576837539673, 'Named Ent. ': 0.6639877557754517, 'Negations ': 0.17581722140312195, 'Reentrancies ': 0.7534530162811279, 'SRL ': 0.5485289692878723, 'Smatch ': 0.5431954264640808, 'Unlabeled ': 0.12236553430557251, 'max_indegree_sim': -0.32294657826423645, 'max_outdegree_sim': 0.2819506824016571, 'max_degree_sim': 0.5896172523498535, 'root_sim': 0.5289171934127808, 'quant_sim': 0.5586270689964294, 'score_wlk': 0.6321209073066711, 'score_wwlk': 0.6237847805023193, 'residual': 0.4911802113056183, 'sent_a': \"I would say you can do it, but it wouldn't be advised.\", 'sent_b': \"I would say it is impossible to know, so don't risk it.\"}\n",
      "{'global': 0.4323141276836395, 'Concepts ': 0.5565347671508789, 'Frames ': 0.4135662615299225, 'Named Ent. ': 0.5505132675170898, 'Negations ': 0.22079399228096008, 'Reentrancies ': 0.7709099054336548, 'SRL ': 0.37505412101745605, 'Smatch ': 0.634207546710968, 'Unlabeled ': 0.4313540458679199, 'max_indegree_sim': -0.008440102450549603, 'max_outdegree_sim': 0.2259007841348648, 'max_degree_sim': 0.16432680189609528, 'root_sim': 0.36006587743759155, 'quant_sim': 0.6129787564277649, 'score_wlk': 0.6010284423828125, 'score_wwlk': 0.46796634793281555, 'residual': 0.4335538446903229, 'sent_a': 'Unfortunately the answer to your question is we simply do not know.', 'sent_b': 'The answer to your question is not really.'}\n",
      "{'global': 0.4130658209323883, 'Concepts ': 0.5353674292564392, 'Frames ': 0.37766945362091064, 'Named Ent. ': 0.33717313408851624, 'Negations ': 0.42681023478507996, 'Reentrancies ': 0.6711456775665283, 'SRL ': 0.49843913316726685, 'Smatch ': -0.1506345272064209, 'Unlabeled ': 0.3977699279785156, 'max_indegree_sim': -0.3897629678249359, 'max_outdegree_sim': 0.2447962611913681, 'max_degree_sim': 0.4603634178638458, 'root_sim': 0.3642284572124481, 'quant_sim': 0.5201256275177002, 'score_wlk': 0.7598894834518433, 'score_wwlk': 0.340777188539505, 'residual': 0.43017837405204773, 'sent_a': \"If you are not sure how to do it, don't do it at all.\", 'sent_b': \"You don't, it will not work.\"}\n",
      "{'global': 0.8283101916313171, 'Concepts ': 0.8901215195655823, 'Frames ': 0.896240234375, 'Named Ent. ': 0.8774954080581665, 'Negations ': 0.8545365333557129, 'Reentrancies ': 0.9575212597846985, 'SRL ': 0.8960031867027283, 'Smatch ': 0.7604193091392517, 'Unlabeled ': 0.7489170432090759, 'max_indegree_sim': 0.7346512079238892, 'max_outdegree_sim': 0.8560718297958374, 'max_degree_sim': 0.789654016494751, 'root_sim': 0.8063715100288391, 'quant_sim': 0.7342705130577087, 'score_wlk': 0.8291453123092651, 'score_wwlk': 0.8179526925086975, 'residual': 0.8282434344291687, 'sent_a': \"It's also a matter of taste.\", 'sent_b': \"It's definitely just a matter of preference.\"}\n",
      "{'global': 0.3949897885322571, 'Concepts ': 0.5882317423820496, 'Frames ': 0.39083024859428406, 'Named Ent. ': 0.5681606531143188, 'Negations ': 0.16476497054100037, 'Reentrancies ': 0.8220470547676086, 'SRL ': 0.37890467047691345, 'Smatch ': -0.2116580456495285, 'Unlabeled ': 0.05672066658735275, 'max_indegree_sim': 0.20348693430423737, 'max_outdegree_sim': 0.12260984629392624, 'max_degree_sim': 0.7755803465843201, 'root_sim': 0.31366613507270813, 'quant_sim': 0.45762258768081665, 'score_wlk': 0.7240740060806274, 'score_wwlk': 0.27323487401008606, 'residual': 0.4080750048160553, 'sent_a': \"It's not a good idea.\", 'sent_b': \"It's a good idea to do both.\"}\n",
      "{'global': 0.08037835359573364, 'Concepts ': 0.1897619068622589, 'Frames ': -0.21264763176441193, 'Named Ent. ': -0.020437190309166908, 'Negations ': 0.49224206805229187, 'Reentrancies ': 0.17662853002548218, 'SRL ': -0.4839612543582916, 'Smatch ': 0.13198953866958618, 'Unlabeled ': 0.3832036852836609, 'max_indegree_sim': -0.010944130830466747, 'max_outdegree_sim': 0.3843749463558197, 'max_degree_sim': 0.2355693280696869, 'root_sim': 0.37351152300834656, 'quant_sim': 0.64155513048172, 'score_wlk': 0.1098475381731987, 'score_wwlk': -0.11732541024684906, 'residual': 0.07273522764444351, 'sent_a': 'This answer is based on my experience teaching undergraduate math in the US.', 'sent_b': 'This answer is based on experience I did not researched for references.'}\n",
      "{'global': 0.6102132201194763, 'Concepts ': 0.6806957125663757, 'Frames ': 0.45579272508621216, 'Named Ent. ': 0.36646997928619385, 'Negations ': 0.35178276896476746, 'Reentrancies ': 0.7941468358039856, 'SRL ': 0.5636800527572632, 'Smatch ': 0.21074137091636658, 'Unlabeled ': 0.5691142082214355, 'max_indegree_sim': 0.42291831970214844, 'max_outdegree_sim': 0.7577264308929443, 'max_degree_sim': 0.8295379877090454, 'root_sim': 0.3856962025165558, 'quant_sim': 0.7615393400192261, 'score_wlk': 0.700384795665741, 'score_wwlk': 0.7619040608406067, 'residual': 0.6163480877876282, 'sent_a': 'There are a few things I think you should do.', 'sent_b': 'There are quite a few things you would need to do.'}\n",
      "{'global': 0.6047793030738831, 'Concepts ': 0.5098865628242493, 'Frames ': 0.48457419872283936, 'Named Ent. ': 0.6046263575553894, 'Negations ': 0.8402739763259888, 'Reentrancies ': 0.711441159248352, 'SRL ': 0.6799294352531433, 'Smatch ': 0.32132893800735474, 'Unlabeled ': 0.4031040370464325, 'max_indegree_sim': 0.860531210899353, 'max_outdegree_sim': 0.6536489129066467, 'max_degree_sim': 0.6164294481277466, 'root_sim': 0.7331216931343079, 'quant_sim': 0.5495675206184387, 'score_wlk': 0.7963852286338806, 'score_wwlk': 0.483920156955719, 'residual': 0.5987721681594849, 'sent_a': 'You will want to clean the area first.', 'sent_b': 'You will also want to remove the seeds.'}\n",
      "{'global': 0.4199554920196533, 'Concepts ': 0.6457858681678772, 'Frames ': 0.6533277034759521, 'Named Ent. ': 0.582737386226654, 'Negations ': 0.6161337494850159, 'Reentrancies ': 0.7451373934745789, 'SRL ': 0.6175907850265503, 'Smatch ': 0.7771823406219482, 'Unlabeled ': 0.860807478427887, 'max_indegree_sim': 0.7101850509643555, 'max_outdegree_sim': 0.5897337794303894, 'max_degree_sim': 0.401200532913208, 'root_sim': 0.5382068157196045, 'quant_sim': 0.4237678647041321, 'score_wlk': 0.7669939994812012, 'score_wwlk': 0.16840995848178864, 'residual': 0.39831581711769104, 'sent_a': 'It depends on what you want to have in your tank.', 'sent_b': 'It depends on what you want to achieve.'}\n",
      "{'global': 0.18533074855804443, 'Concepts ': 0.19260968267917633, 'Frames ': 0.19753406941890717, 'Named Ent. ': 0.04009213298559189, 'Negations ': 0.28714388608932495, 'Reentrancies ': 0.21988801658153534, 'SRL ': 0.31054413318634033, 'Smatch ': 0.2857678234577179, 'Unlabeled ': -0.14145122468471527, 'max_indegree_sim': 0.5623827576637268, 'max_outdegree_sim': 0.4728786051273346, 'max_degree_sim': 0.09459524601697922, 'root_sim': -0.41306713223457336, 'quant_sim': 0.5354526042938232, 'score_wlk': 0.5928650498390198, 'score_wwlk': 0.3920208215713501, 'residual': 0.17755042016506195, 'sent_a': 'There are a couple of options that you could consider:', 'sent_b': 'There are a lot of ways that could go poorly for you.'}\n",
      "{'global': 0.5438929796218872, 'Concepts ': 0.4314393997192383, 'Frames ': 0.8037042021751404, 'Named Ent. ': 0.29389700293540955, 'Negations ': 0.7451316118240356, 'Reentrancies ': 0.7930116653442383, 'SRL ': 0.6402500867843628, 'Smatch ': 0.6285739541053772, 'Unlabeled ': 0.8020245432853699, 'max_indegree_sim': 0.554193377494812, 'max_outdegree_sim': 0.4264240860939026, 'max_degree_sim': 0.5334200263023376, 'root_sim': 0.5791531205177307, 'quant_sim': 0.5488190650939941, 'score_wlk': 0.44890812039375305, 'score_wwlk': 0.4657590687274933, 'residual': 0.5321552753448486, 'sent_a': \"You don't have to worry.\", 'sent_b': \"Since you have one ticket, you don't have to worry.\"}\n",
      "{'global': 0.5125744342803955, 'Concepts ': 0.7281929850578308, 'Frames ': 0.6317315697669983, 'Named Ent. ': 0.5529118180274963, 'Negations ': 0.8784179091453552, 'Reentrancies ': 0.7647378444671631, 'SRL ': 0.7847747802734375, 'Smatch ': 0.2857140898704529, 'Unlabeled ': 0.9129766225814819, 'max_indegree_sim': 0.7984865307807922, 'max_outdegree_sim': 0.8011139035224915, 'max_degree_sim': 0.7322604060173035, 'root_sim': 0.739669144153595, 'quant_sim': 0.5332489013671875, 'score_wlk': 0.7836370468139648, 'score_wwlk': 0.5405688285827637, 'residual': 0.48671478033065796, 'sent_a': 'Vietnamese citizens need a visa to visit the USA.', 'sent_b': 'Nepalese citizens require a visa to visit the UK.'}\n",
      "{'global': 0.4828213155269623, 'Concepts ': 0.48028671741485596, 'Frames ': 0.12589925527572632, 'Named Ent. ': 0.7369256019592285, 'Negations ': 0.12256518751382828, 'Reentrancies ': 0.45042142271995544, 'SRL ': 0.3253919184207916, 'Smatch ': 0.3715062737464905, 'Unlabeled ': 0.3966559171676636, 'max_indegree_sim': 0.524694561958313, 'max_outdegree_sim': -0.019625164568424225, 'max_degree_sim': 0.4881975054740906, 'root_sim': 0.7010928392410278, 'quant_sim': 0.5880602598190308, 'score_wlk': 0.22708651423454285, 'score_wwlk': 0.4533958435058594, 'residual': 0.48966625332832336, 'sent_a': \"It depends on how it's used.\", 'sent_b': \"It depends on how long it's been out.\"}\n",
      "{'global': 0.7662869691848755, 'Concepts ': 0.8816755414009094, 'Frames ': 0.7287257313728333, 'Named Ent. ': 0.8600640892982483, 'Negations ': 0.6099429726600647, 'Reentrancies ': 0.9382485747337341, 'SRL ': 0.6826633214950562, 'Smatch ': 0.7528529763221741, 'Unlabeled ': 0.7670913934707642, 'max_indegree_sim': 0.7190672159194946, 'max_outdegree_sim': 0.8035403490066528, 'max_degree_sim': 0.6100547909736633, 'root_sim': 0.6653493046760559, 'quant_sim': 0.8108202815055847, 'score_wlk': 0.8809259533882141, 'score_wwlk': 0.5394651889801025, 'residual': 0.7700639367103577, 'sent_a': \"I don't see why there should be any problem with this whatsoever.\", 'sent_b': \"I don't see why this could be a problem.\"}\n",
      "{'global': 0.9272467494010925, 'Concepts ': 0.9311527013778687, 'Frames ': 0.8601436018943787, 'Named Ent. ': 0.9432543516159058, 'Negations ': 0.8674125671386719, 'Reentrancies ': 0.936586856842041, 'SRL ': 0.8657040596008301, 'Smatch ': 0.9763438105583191, 'Unlabeled ': 0.8232454061508179, 'max_indegree_sim': 0.9389722943305969, 'max_outdegree_sim': 0.8506495356559753, 'max_degree_sim': 0.8391111493110657, 'root_sim': 0.8305344581604004, 'quant_sim': 0.9338696002960205, 'score_wlk': 0.8831616640090942, 'score_wwlk': 0.9167129993438721, 'residual': 0.9325734376907349, 'sent_a': \"There's not a lot you can do about that.\", 'sent_b': 'I really do not think there is much you can do about that.'}\n",
      "{'global': 0.776824414730072, 'Concepts ': 0.7059780955314636, 'Frames ': 0.8515138030052185, 'Named Ent. ': 0.6961793303489685, 'Negations ': 0.5581510066986084, 'Reentrancies ': 0.7816473841667175, 'SRL ': 0.49140962958335876, 'Smatch ': 0.3320442736148834, 'Unlabeled ': 0.2836002707481384, 'max_indegree_sim': 0.5398873686790466, 'max_outdegree_sim': 0.5905936360359192, 'max_degree_sim': 0.8068322539329529, 'root_sim': 0.6998406648635864, 'quant_sim': 0.7052146792411804, 'score_wlk': 0.7424330711364746, 'score_wwlk': 0.7882172465324402, 'residual': 0.7899343371391296, 'sent_a': 'You got it right.', 'sent_b': 'Yes you got it.'}\n",
      "{'global': 0.30622580647468567, 'Concepts ': 0.11768901348114014, 'Frames ': 0.5032129883766174, 'Named Ent. ': 0.38063377141952515, 'Negations ': 0.08992540836334229, 'Reentrancies ': 0.5745077133178711, 'SRL ': 0.5764194130897522, 'Smatch ': 0.26388025283813477, 'Unlabeled ': 0.9552615284919739, 'max_indegree_sim': 0.07493504136800766, 'max_outdegree_sim': 0.6639670729637146, 'max_degree_sim': 0.3116157054901123, 'root_sim': 0.39925235509872437, 'quant_sim': 0.5403652787208557, 'score_wlk': 0.6473183035850525, 'score_wwlk': 0.4639008045196533, 'residual': 0.2872242033481598, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': 'Remember this is your degree we are talking about, its up to you what you want to do.'}\n",
      "{'global': 0.6955870985984802, 'Concepts ': 0.6594865322113037, 'Frames ': 0.8109869360923767, 'Named Ent. ': 0.5318436622619629, 'Negations ': 0.39414268732070923, 'Reentrancies ': 0.8500106334686279, 'SRL ': 0.7788441181182861, 'Smatch ': 0.06076715886592865, 'Unlabeled ': 0.1205621212720871, 'max_indegree_sim': 0.7610964179039001, 'max_outdegree_sim': 0.7878658771514893, 'max_degree_sim': 0.7430572509765625, 'root_sim': 0.577997088432312, 'quant_sim': 0.8570975065231323, 'score_wlk': 0.7691852450370789, 'score_wwlk': 0.5616084933280945, 'residual': 0.7008962035179138, 'sent_a': 'How do you do that?', 'sent_b': 'How should you do that?'}\n",
      "{'global': 0.8369462490081787, 'Concepts ': 0.9130668044090271, 'Frames ': 0.8932173252105713, 'Named Ent. ': 0.8417765498161316, 'Negations ': 0.8930757641792297, 'Reentrancies ': 0.9183590412139893, 'SRL ': 0.7432790994644165, 'Smatch ': 0.8880211710929871, 'Unlabeled ': 0.6630564332008362, 'max_indegree_sim': 0.7597423195838928, 'max_outdegree_sim': 0.7585391998291016, 'max_degree_sim': 0.8897270560264587, 'root_sim': 0.762336015701294, 'quant_sim': 0.8958166837692261, 'score_wlk': 0.8988334536552429, 'score_wwlk': 0.9535841941833496, 'residual': 0.8364012837409973, 'sent_a': 'I was in a similar situation.', 'sent_b': 'I am in a similar situation.'}\n",
      "{'global': 0.23897112905979156, 'Concepts ': -0.2909988462924957, 'Frames ': 0.053628936409950256, 'Named Ent. ': -0.15495768189430237, 'Negations ': 0.2543027102947235, 'Reentrancies ': 0.6289577484130859, 'SRL ': 0.21667243540287018, 'Smatch ': -0.5962413549423218, 'Unlabeled ': 0.08118905872106552, 'max_indegree_sim': 0.39558789134025574, 'max_outdegree_sim': 0.6350460052490234, 'max_degree_sim': 0.08748400956392288, 'root_sim': 0.0028963738586753607, 'quant_sim': 0.48775020241737366, 'score_wlk': 0.06701045483350754, 'score_wwlk': 0.4629313051700592, 'residual': 0.2501745820045471, 'sent_a': 'What is your lid made of?', 'sent_b': 'What is your paper for?'}\n",
      "{'global': 0.3206488788127899, 'Concepts ': 0.46311742067337036, 'Frames ': 0.36098355054855347, 'Named Ent. ': 0.28766387701034546, 'Negations ': 0.44909828901290894, 'Reentrancies ': 0.01859743520617485, 'SRL ': -0.1452198326587677, 'Smatch ': 0.656258225440979, 'Unlabeled ': 0.8285543918609619, 'max_indegree_sim': -0.06457041949033737, 'max_outdegree_sim': -0.07894837111234665, 'max_degree_sim': 0.7410138845443726, 'root_sim': 0.08823034167289734, 'quant_sim': 0.5965208411216736, 'score_wlk': -0.3671153485774994, 'score_wwlk': -0.026491528376936913, 'residual': 0.3088550269603729, 'sent_a': \"You don't need any visa.\", 'sent_b': \"You don't need sauce at all.\"}\n",
      "{'global': 0.6642659902572632, 'Concepts ': 0.6829632520675659, 'Frames ': 0.765091061592102, 'Named Ent. ': 0.774368405342102, 'Negations ': 0.22772809863090515, 'Reentrancies ': 0.7687353491783142, 'SRL ': 0.6796647310256958, 'Smatch ': 0.6438637971878052, 'Unlabeled ': 0.4441620707511902, 'max_indegree_sim': 0.8358173966407776, 'max_outdegree_sim': 0.5613046288490295, 'max_degree_sim': 0.42886656522750854, 'root_sim': 0.7344543933868408, 'quant_sim': 0.7627263069152832, 'score_wlk': 0.6349998712539673, 'score_wwlk': 0.3713412880897522, 'residual': 0.6674174070358276, 'sent_a': \"That is your problem, not your supervisor's.\", 'sent_b': 'Work with your supervisor and your team to solve the problem.'}\n",
      "{'global': 0.8037984371185303, 'Concepts ': 0.8431990146636963, 'Frames ': 0.6540106534957886, 'Named Ent. ': 0.5662171244621277, 'Negations ': 0.423956036567688, 'Reentrancies ': 0.7914342284202576, 'SRL ': 0.6844138503074646, 'Smatch ': 0.8234975934028625, 'Unlabeled ': 0.8488482236862183, 'max_indegree_sim': 0.8231219053268433, 'max_outdegree_sim': 0.8592197895050049, 'max_degree_sim': 0.7899255752563477, 'root_sim': 0.8027180433273315, 'quant_sim': 0.7720006704330444, 'score_wlk': 0.871523380279541, 'score_wwlk': 0.7742923498153687, 'residual': 0.8103304505348206, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': \"It's up to you what you want to do next.\"}\n",
      "{'global': 0.8459784388542175, 'Concepts ': 0.8321412801742554, 'Frames ': 0.8540688753128052, 'Named Ent. ': 0.8738909959793091, 'Negations ': 0.7768270969390869, 'Reentrancies ': 0.9422574639320374, 'SRL ': 0.8657425045967102, 'Smatch ': 0.8380020260810852, 'Unlabeled ': 0.87865149974823, 'max_indegree_sim': 0.8663740754127502, 'max_outdegree_sim': 0.8767721056938171, 'max_degree_sim': 0.8188993334770203, 'root_sim': 0.8917317986488342, 'quant_sim': 0.7247346043586731, 'score_wlk': 0.8837810158729553, 'score_wwlk': 0.715914249420166, 'residual': 0.8473491668701172, 'sent_a': 'There are two things to consider:', 'sent_b': 'A couple things to consider:'}\n",
      "{'global': 0.8197407126426697, 'Concepts ': 0.8565314412117004, 'Frames ': 0.7204592823982239, 'Named Ent. ': 0.8910035490989685, 'Negations ': 0.867296040058136, 'Reentrancies ': 0.7150726914405823, 'SRL ': 0.9260696768760681, 'Smatch ': 0.6011204719543457, 'Unlabeled ': 0.8179357051849365, 'max_indegree_sim': 0.8215427994728088, 'max_outdegree_sim': 0.3995354175567627, 'max_degree_sim': 0.7963864207267761, 'root_sim': 0.9351497888565063, 'quant_sim': 0.8391587734222412, 'score_wlk': 0.9108518362045288, 'score_wwlk': 0.7448879480361938, 'residual': 0.8194631338119507, 'sent_a': 'It depends entirely on your company and your contract.', 'sent_b': 'It depends on your company.'}\n",
      "{'global': 0.4363759756088257, 'Concepts ': 0.4597416818141937, 'Frames ': 0.266561359167099, 'Named Ent. ': 0.5521020889282227, 'Negations ': 0.06280633062124252, 'Reentrancies ': 0.5564786195755005, 'SRL ': 0.2984609007835388, 'Smatch ': 0.3416460454463959, 'Unlabeled ': 0.7469728589057922, 'max_indegree_sim': 0.7428295612335205, 'max_outdegree_sim': 0.44783538579940796, 'max_degree_sim': 0.425468385219574, 'root_sim': 0.3182581067085266, 'quant_sim': 0.727961540222168, 'score_wlk': 0.312691330909729, 'score_wwlk': 0.32503876090049744, 'residual': 0.43793749809265137, 'sent_a': \"The wood probably isn't rotten, it's just worn out.\", 'sent_b': \"It's just worn out and not safe.\"}\n",
      "{'global': 0.805841863155365, 'Concepts ': 0.7782601714134216, 'Frames ': 0.9219208359718323, 'Named Ent. ': 0.7670919895172119, 'Negations ': 0.8473449945449829, 'Reentrancies ': 0.6052558422088623, 'SRL ': 0.8300564289093018, 'Smatch ': 0.8186667561531067, 'Unlabeled ': 0.9699974060058594, 'max_indegree_sim': 0.864406943321228, 'max_outdegree_sim': 0.24503614008426666, 'max_degree_sim': 0.8024420142173767, 'root_sim': 0.7519169449806213, 'quant_sim': 0.8117963671684265, 'score_wlk': 0.8369438648223877, 'score_wwlk': 0.67807936668396, 'residual': 0.8036394715309143, 'sent_a': \"Well, I wouldn't put it on my cv.\", 'sent_b': \"I wouldn't put this job on my resume.\"}\n",
      "{'global': 0.4817277789115906, 'Concepts ': 0.8286085724830627, 'Frames ': 0.2590845227241516, 'Named Ent. ': 0.13365790247917175, 'Negations ': 0.6060436964035034, 'Reentrancies ': 0.9413873553276062, 'SRL ': 0.0303704421967268, 'Smatch ': 0.22495999932289124, 'Unlabeled ': -0.2058112919330597, 'max_indegree_sim': 0.4886860251426697, 'max_outdegree_sim': -0.3411523699760437, 'max_degree_sim': 0.26992082595825195, 'root_sim': 0.19865934550762177, 'quant_sim': 0.48194456100463867, 'score_wlk': 0.15485982596874237, 'score_wwlk': 0.7080492973327637, 'residual': 0.49972963333129883, 'sent_a': 'This is a great one...', 'sent_b': 'This is a difficult one.'}\n",
      "{'global': 0.39732852578163147, 'Concepts ': 0.27366289496421814, 'Frames ': 0.31750860810279846, 'Named Ent. ': 0.17617866396903992, 'Negations ': 0.08020513504743576, 'Reentrancies ': 0.7795602679252625, 'SRL ': 0.4720122218132019, 'Smatch ': 0.4182036519050598, 'Unlabeled ': 0.3597972095012665, 'max_indegree_sim': 0.1280347853899002, 'max_outdegree_sim': -1.3104129720886704e-05, 'max_degree_sim': 0.09760328382253647, 'root_sim': 0.041042398661375046, 'quant_sim': 0.37130382657051086, 'score_wlk': 0.5426437854766846, 'score_wwlk': 0.4361645579338074, 'residual': 0.4145999550819397, 'sent_a': 'The answer to both questions is: Yes.', 'sent_b': 'To directly answer the two questions in your title.'}\n",
      "{'global': 0.391244500875473, 'Concepts ': 0.6632804870605469, 'Frames ': 0.4240477383136749, 'Named Ent. ': 0.22500023245811462, 'Negations ': 0.09822040796279907, 'Reentrancies ': 0.8603365421295166, 'SRL ': 0.14180094003677368, 'Smatch ': 0.5341514348983765, 'Unlabeled ': 0.6755697727203369, 'max_indegree_sim': 0.2950741946697235, 'max_outdegree_sim': 0.16304440796375275, 'max_degree_sim': 0.4942927956581116, 'root_sim': 0.7895232439041138, 'quant_sim': 0.5199834704399109, 'score_wlk': 0.3563276529312134, 'score_wwlk': 0.391511470079422, 'residual': 0.3797294795513153, 'sent_a': 'Yes, there is a reason to not do what you propose.', 'sent_b': 'Valid reasons to feel the way you do, but not a reason to resign.'}\n",
      "{'global': 0.4339454770088196, 'Concepts ': 0.27026593685150146, 'Frames ': 0.2517847716808319, 'Named Ent. ': 0.5129726529121399, 'Negations ': 0.07451426982879639, 'Reentrancies ': 0.7459840774536133, 'SRL ': 0.20912297070026398, 'Smatch ': 0.543279767036438, 'Unlabeled ': 0.5242739319801331, 'max_indegree_sim': 0.07891923189163208, 'max_outdegree_sim': -0.21961823105812073, 'max_degree_sim': 0.2702888548374176, 'root_sim': 0.3831920921802521, 'quant_sim': 0.5041305422782898, 'score_wlk': 0.46120864152908325, 'score_wwlk': 0.5130203366279602, 'residual': 0.446307510137558, 'sent_a': 'I think it is depends on the program.', 'sent_b': 'It depends on how the term is used I think.'}\n",
      "{'global': 0.8888424634933472, 'Concepts ': 0.7934809923171997, 'Frames ': 0.8550044894218445, 'Named Ent. ': 0.921161413192749, 'Negations ': 0.7011659145355225, 'Reentrancies ': 0.8960797786712646, 'SRL ': 0.8543974161148071, 'Smatch ': 0.9944911003112793, 'Unlabeled ': 0.9912776947021484, 'max_indegree_sim': 0.8207816481590271, 'max_outdegree_sim': 0.7964503765106201, 'max_degree_sim': 0.853326678276062, 'root_sim': 0.8640047311782837, 'quant_sim': 0.7952473759651184, 'score_wlk': 0.734802782535553, 'score_wwlk': 0.6775494813919067, 'residual': 0.8889743089675903, 'sent_a': 'Can you do this?', 'sent_b': 'So, CAN you do this?'}\n",
      "{'global': 0.42735475301742554, 'Concepts ': 0.338483065366745, 'Frames ': 0.301537424325943, 'Named Ent. ': -0.07306860387325287, 'Negations ': 0.5652539134025574, 'Reentrancies ': 0.7969216108322144, 'SRL ': 0.6014242768287659, 'Smatch ': -0.11483968049287796, 'Unlabeled ': 0.5545528531074524, 'max_indegree_sim': 0.7517659068107605, 'max_outdegree_sim': 0.7240399122238159, 'max_degree_sim': 0.5178152918815613, 'root_sim': 0.27459046244621277, 'quant_sim': 0.7637225389480591, 'score_wlk': 0.5245887041091919, 'score_wwlk': 0.1822570264339447, 'residual': 0.43218928575515747, 'sent_a': \"I think there isn't a general answer.\", 'sent_b': \"I don't think there is an answer for this.\"}\n",
      "{'global': 0.8087577223777771, 'Concepts ': 0.6864932775497437, 'Frames ': 0.6432352066040039, 'Named Ent. ': 0.8401796221733093, 'Negations ': 0.7696182131767273, 'Reentrancies ': 0.7114065885543823, 'SRL ': 0.7762361168861389, 'Smatch ': 0.6767223477363586, 'Unlabeled ': 0.8278121948242188, 'max_indegree_sim': 0.6469107866287231, 'max_outdegree_sim': 0.6555301547050476, 'max_degree_sim': 0.9182472825050354, 'root_sim': 0.8774610757827759, 'quant_sim': 0.8714463114738464, 'score_wlk': 0.7994658946990967, 'score_wwlk': 0.8668790459632874, 'residual': 0.812907874584198, 'sent_a': 'There are a few things you can do: ', 'sent_b': 'There are a couple things you can try.'}\n",
      "{'global': 0.39258307218551636, 'Concepts ': 0.38119831681251526, 'Frames ': 0.733698844909668, 'Named Ent. ': 0.21954385936260223, 'Negations ': 0.16584517061710358, 'Reentrancies ': 0.644233226776123, 'SRL ': 0.7574710249900818, 'Smatch ': 0.6273610591888428, 'Unlabeled ': -0.06437422335147858, 'max_indegree_sim': 0.6796021461486816, 'max_outdegree_sim': 0.794409990310669, 'max_degree_sim': 0.20321795344352722, 'root_sim': 0.5540953874588013, 'quant_sim': 0.3545633554458618, 'score_wlk': 0.5757110714912415, 'score_wwlk': 0.6960583329200745, 'residual': 0.38019296526908875, 'sent_a': 'The key is you have to find something that works for you, and your goals.', 'sent_b': 'You may want it, but the process given to you is what you have to work within.'}\n",
      "{'global': 0.15975992381572723, 'Concepts ': 0.3086600601673126, 'Frames ': -0.08746600896120071, 'Named Ent. ': -0.5200227499008179, 'Negations ': -0.03882549703121185, 'Reentrancies ': 0.3426395058631897, 'SRL ': 0.5656121969223022, 'Smatch ': 0.32903024554252625, 'Unlabeled ': 0.43937113881111145, 'max_indegree_sim': 0.29207175970077515, 'max_outdegree_sim': 0.2515459954738617, 'max_degree_sim': 0.12935933470726013, 'root_sim': -0.05065073072910309, 'quant_sim': 0.1048474833369255, 'score_wlk': 0.07186322659254074, 'score_wwlk': 0.1310582309961319, 'residual': 0.14714819192886353, 'sent_a': \"You don't have to know.\", 'sent_b': \"You don't have equipments/facilities to do research in B.\"}\n",
      "{'global': 0.31788524985313416, 'Concepts ': 0.2741120457649231, 'Frames ': 0.5464605689048767, 'Named Ent. ': -0.010379116982221603, 'Negations ': 0.1656758189201355, 'Reentrancies ': 0.7898604273796082, 'SRL ': 0.793096661567688, 'Smatch ': 0.15079401433467865, 'Unlabeled ': 0.6286942958831787, 'max_indegree_sim': 0.41810786724090576, 'max_outdegree_sim': 0.20917518436908722, 'max_degree_sim': 0.4240463376045227, 'root_sim': -0.17319892346858978, 'quant_sim': 0.190129354596138, 'score_wlk': 0.2689383327960968, 'score_wwlk': 0.6591572761535645, 'residual': 0.3216383159160614, 'sent_a': \"I think you've pretty much done your review.\", 'sent_b': \"You've pretty much answered your own question.\"}\n",
      "{'global': 0.19909286499023438, 'Concepts ': 0.365904837846756, 'Frames ': 0.1340610235929489, 'Named Ent. ': -0.25470757484436035, 'Negations ': -0.005161574110388756, 'Reentrancies ': 0.49663105607032776, 'SRL ': 0.6290931701660156, 'Smatch ': -0.47711095213890076, 'Unlabeled ': -0.3747141659259796, 'max_indegree_sim': -0.02494802325963974, 'max_outdegree_sim': 0.285249263048172, 'max_degree_sim': 0.6254820823669434, 'root_sim': -0.02088223397731781, 'quant_sim': 0.2447846233844757, 'score_wlk': 0.29344117641448975, 'score_wwlk': 0.5161607265472412, 'residual': 0.21206073462963104, 'sent_a': 'There are a few things I think you should do.', 'sent_b': \"There are a few things I don't understand:\"}\n",
      "{'global': 0.5975317358970642, 'Concepts ': 0.7509775161743164, 'Frames ': 0.6639976501464844, 'Named Ent. ': 0.5075979828834534, 'Negations ': 0.8241302371025085, 'Reentrancies ': 0.7592714428901672, 'SRL ': 0.7380599975585938, 'Smatch ': 0.5270859003067017, 'Unlabeled ': 0.633385181427002, 'max_indegree_sim': 0.7494286894798279, 'max_outdegree_sim': 0.6195094585418701, 'max_degree_sim': 0.7440253496170044, 'root_sim': 0.19854842126369476, 'quant_sim': 0.598814070224762, 'score_wlk': 0.7788619995117188, 'score_wwlk': 0.5617476105690002, 'residual': 0.5892484784126282, 'sent_a': 'You burn more Calories digesting protein than you do carbohydrates and fat.', 'sent_b': 'The more you sweat, the faster you will burn calories and fat.'}\n",
      "{'global': 0.36211103200912476, 'Concepts ': 0.3071157932281494, 'Frames ': 0.37595558166503906, 'Named Ent. ': 0.3320342004299164, 'Negations ': 0.20457708835601807, 'Reentrancies ': 0.800945520401001, 'SRL ': 0.24083198606967926, 'Smatch ': 0.8248608112335205, 'Unlabeled ': 0.758103609085083, 'max_indegree_sim': 0.6473685503005981, 'max_outdegree_sim': 0.02311435528099537, 'max_degree_sim': 0.42077168822288513, 'root_sim': 0.6575871706008911, 'quant_sim': 0.48958829045295715, 'score_wlk': 0.6138931512832642, 'score_wwlk': 0.5611007809638977, 'residual': 0.35515376925468445, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'It depends on what you want to say/imply.'}\n",
      "{'global': 0.9192807078361511, 'Concepts ': 0.971553385257721, 'Frames ': 0.9140288829803467, 'Named Ent. ': 0.8731980919837952, 'Negations ': 0.8869388103485107, 'Reentrancies ': 0.8639857769012451, 'SRL ': 0.7733344435691833, 'Smatch ': 0.8661537170410156, 'Unlabeled ': 0.8490610122680664, 'max_indegree_sim': 0.9195830821990967, 'max_outdegree_sim': 0.9399362802505493, 'max_degree_sim': 0.9549092650413513, 'root_sim': 0.8219149112701416, 'quant_sim': 0.8861392140388489, 'score_wlk': 0.9296196699142456, 'score_wwlk': 0.905643105506897, 'residual': 0.9217764139175415, 'sent_a': 'You got it right.', 'sent_b': \"You've got it right.\"}\n",
      "{'global': 0.8757912516593933, 'Concepts ': 0.7885157465934753, 'Frames ': 0.9204451441764832, 'Named Ent. ': 0.7694880366325378, 'Negations ': 0.8251044154167175, 'Reentrancies ': 0.9472742676734924, 'SRL ': 0.9055637717247009, 'Smatch ': 0.690922737121582, 'Unlabeled ': 0.9068721532821655, 'max_indegree_sim': 0.6017223000526428, 'max_outdegree_sim': 0.8338440656661987, 'max_degree_sim': 0.8950260281562805, 'root_sim': 0.8756400942802429, 'quant_sim': 0.8064998388290405, 'score_wlk': 0.8639786839485168, 'score_wwlk': 0.8663887977600098, 'residual': 0.8817694783210754, 'sent_a': 'You answered your own question.', 'sent_b': 'You pretty much answered your own question.'}\n",
      "{'global': 0.6459137797355652, 'Concepts ': 0.8170238137245178, 'Frames ': 0.4723227322101593, 'Named Ent. ': 0.3210970461368561, 'Negations ': 0.5824699401855469, 'Reentrancies ': 0.6424567699432373, 'SRL ': 0.8691811561584473, 'Smatch ': 0.3947053849697113, 'Unlabeled ': 0.9043551087379456, 'max_indegree_sim': 0.7229412794113159, 'max_outdegree_sim': 0.7297064065933228, 'max_degree_sim': 0.5424197316169739, 'root_sim': 0.763285219669342, 'quant_sim': 0.7040134072303772, 'score_wlk': 0.6315716505050659, 'score_wwlk': 0.4477636218070984, 'residual': 0.6482816338539124, 'sent_a': 'I have the same thing.', 'sent_b': 'I had the same problem as you.'}\n",
      "{'global': 0.37236249446868896, 'Concepts ': 0.29632237553596497, 'Frames ': 0.4171733558177948, 'Named Ent. ': 0.3022555112838745, 'Negations ': 0.5018250346183777, 'Reentrancies ': 0.8003180027008057, 'SRL ': 0.24394752085208893, 'Smatch ': 0.8569621443748474, 'Unlabeled ': 0.7722129225730896, 'max_indegree_sim': 0.30588483810424805, 'max_outdegree_sim': -0.3239678740501404, 'max_degree_sim': 0.24372951686382294, 'root_sim': 0.38256514072418213, 'quant_sim': -0.1529289186000824, 'score_wlk': 0.826001763343811, 'score_wwlk': 0.33460715413093567, 'residual': 0.366909384727478, 'sent_a': 'Yes, there is a chemical difference.', 'sent_b': 'Yes, there is a reason for it.'}\n",
      "{'global': 0.2236354947090149, 'Concepts ': -0.0841718539595604, 'Frames ': 0.4561217725276947, 'Named Ent. ': -0.16534645855426788, 'Negations ': -0.06095369532704353, 'Reentrancies ': 0.7358434200286865, 'SRL ': 0.6598368883132935, 'Smatch ': 0.7582696080207825, 'Unlabeled ': 0.4228900969028473, 'max_indegree_sim': 0.35272446274757385, 'max_outdegree_sim': -0.03760441392660141, 'max_degree_sim': 0.06017505005002022, 'root_sim': 0.21339616179466248, 'quant_sim': 0.04235541447997093, 'score_wlk': 0.5960755348205566, 'score_wwlk': 0.1620994359254837, 'residual': 0.22623880207538605, 'sent_a': 'They can be out of level by a bit, but flat is important.', 'sent_b': 'Yes, although they can be a bit sour.'}\n",
      "{'global': 0.29574647545814514, 'Concepts ': 0.1626468300819397, 'Frames ': -0.05550660192966461, 'Named Ent. ': 0.5194225907325745, 'Negations ': 0.1668338179588318, 'Reentrancies ': -0.1771882325410843, 'SRL ': -0.24813148379325867, 'Smatch ': 0.04375559464097023, 'Unlabeled ': 0.09010317176580429, 'max_indegree_sim': 0.5957341194152832, 'max_outdegree_sim': 0.33853650093078613, 'max_degree_sim': -0.05121823772788048, 'root_sim': 0.038525741547346115, 'quant_sim': -0.37187299132347107, 'score_wlk': 0.42467254400253296, 'score_wwlk': 0.508301854133606, 'residual': 0.31598636507987976, 'sent_a': 'I would look at this: U.S.', 'sent_b': 'I would look at this way: '}\n",
      "{'global': 0.98139488697052, 'Concepts ': 0.9873666763305664, 'Frames ': 0.9868030548095703, 'Named Ent. ': 0.985888659954071, 'Negations ': 0.9699422717094421, 'Reentrancies ': 0.989554226398468, 'SRL ': 0.9841948747634888, 'Smatch ': 0.9660088419914246, 'Unlabeled ': 0.9535885453224182, 'max_indegree_sim': 0.9540140628814697, 'max_outdegree_sim': 0.9769216179847717, 'max_degree_sim': 0.9849641919136047, 'root_sim': 0.9872192740440369, 'quant_sim': 0.9519698619842529, 'score_wlk': 0.9923685193061829, 'score_wwlk': 0.985262930393219, 'residual': 0.9816575646400452, 'sent_a': 'You are on the right path.', 'sent_b': \"You're on the right path.\"}\n",
      "{'global': 0.5173876881599426, 'Concepts ': 0.7527796626091003, 'Frames ': 0.7220788598060608, 'Named Ent. ': 0.6153994202613831, 'Negations ': 0.4877466857433319, 'Reentrancies ': 0.8987424373626709, 'SRL ': 0.7650457620620728, 'Smatch ': 0.7067148685455322, 'Unlabeled ': 0.7026687264442444, 'max_indegree_sim': 0.8026843070983887, 'max_outdegree_sim': 0.645813524723053, 'max_degree_sim': 0.4498617947101593, 'root_sim': 0.6007170677185059, 'quant_sim': 0.8576914668083191, 'score_wlk': 0.8020230531692505, 'score_wwlk': 0.47117581963539124, 'residual': 0.4915388822555542, 'sent_a': \"If you haven't, you should.\", 'sent_b': \"You should, but you don't have to.\"}\n",
      "{'global': 0.49068519473075867, 'Concepts ': 0.5339593291282654, 'Frames ': 0.38528192043304443, 'Named Ent. ': 0.31038057804107666, 'Negations ': 0.23553119599819183, 'Reentrancies ': 0.7421207427978516, 'SRL ': 0.5535919070243835, 'Smatch ': 0.8164299726486206, 'Unlabeled ': 0.47974124550819397, 'max_indegree_sim': 0.3953840732574463, 'max_outdegree_sim': 0.12723922729492188, 'max_degree_sim': 0.36329805850982666, 'root_sim': -0.022459210827946663, 'quant_sim': 0.4325384497642517, 'score_wlk': 0.39698290824890137, 'score_wwlk': 0.3532310128211975, 'residual': 0.5051388144493103, 'sent_a': 'Yes it is common!', 'sent_b': 'The simple answer is yes!'}\n",
      "{'global': 0.48748958110809326, 'Concepts ': 0.32683709263801575, 'Frames ': 0.6397765278816223, 'Named Ent. ': -0.1414458155632019, 'Negations ': 0.2329917848110199, 'Reentrancies ': 0.4506343901157379, 'SRL ': 0.42989885807037354, 'Smatch ': 0.5586576461791992, 'Unlabeled ': 0.9364143013954163, 'max_indegree_sim': 0.24394698441028595, 'max_outdegree_sim': 0.5596042275428772, 'max_degree_sim': 0.032471489161252975, 'root_sim': 0.5182041525840759, 'quant_sim': 0.7061297297477722, 'score_wlk': 0.5577440857887268, 'score_wwlk': 0.576306939125061, 'residual': 0.47501686215400696, 'sent_a': \"I don't think that there's any.\", 'sent_b': \"I don't think there are any benefits.\"}\n",
      "{'global': 0.050202351063489914, 'Concepts ': 0.031885240226984024, 'Frames ': 0.2642615735530853, 'Named Ent. ': -0.15646451711654663, 'Negations ': 0.38602182269096375, 'Reentrancies ': 0.4000393748283386, 'SRL ': 0.22757785022258759, 'Smatch ': -0.7038092017173767, 'Unlabeled ': -0.4654761850833893, 'max_indegree_sim': 0.035384513437747955, 'max_outdegree_sim': -0.07366141676902771, 'max_degree_sim': 0.06661783903837204, 'root_sim': 0.04874471202492714, 'quant_sim': 0.29679611325263977, 'score_wlk': 0.2804776430130005, 'score_wwlk': 0.021439967676997185, 'residual': 0.05647771805524826, 'sent_a': 'It looks to me that this is set up for what you want:', 'sent_b': 'It is up to you whether you want to do that.'}\n",
      "{'global': 0.4488694369792938, 'Concepts ': 0.5508013963699341, 'Frames ': 0.35559237003326416, 'Named Ent. ': 0.11719433963298798, 'Negations ': 0.06963668018579483, 'Reentrancies ': -0.08749371021986008, 'SRL ': 0.3453674614429474, 'Smatch ': 0.8609756827354431, 'Unlabeled ': 0.8711694478988647, 'max_indegree_sim': 0.43691256642341614, 'max_outdegree_sim': 0.1628848761320114, 'max_degree_sim': 0.16671523451805115, 'root_sim': 0.1647496372461319, 'quant_sim': 0.36188453435897827, 'score_wlk': 0.41898417472839355, 'score_wwlk': -0.23100146651268005, 'residual': 0.43317192792892456, 'sent_a': \"Problem is, the system doesn't account for people who could care less about money, or title.\", 'sent_b': \"The Joker doesn't care about money.\"}\n",
      "{'global': 0.45540651679039, 'Concepts ': 0.4880445897579193, 'Frames ': 0.4432111084461212, 'Named Ent. ': 0.2845137119293213, 'Negations ': 0.6080774664878845, 'Reentrancies ': 0.5264350771903992, 'SRL ': 0.6136359572410583, 'Smatch ': 0.6428032517433167, 'Unlabeled ': 0.7587707042694092, 'max_indegree_sim': 0.2220088690519333, 'max_outdegree_sim': -0.23163390159606934, 'max_degree_sim': 0.49361616373062134, 'root_sim': 0.23241709172725677, 'quant_sim': 0.5853263139724731, 'score_wlk': 0.40076079964637756, 'score_wwlk': -0.2542916536331177, 'residual': 0.4517664909362793, 'sent_a': \"In the first case, I think you don't need it.\", 'sent_b': \"So I don't think you need to put it on the cover.\"}\n",
      "{'global': 0.40155813097953796, 'Concepts ': -0.12814268469810486, 'Frames ': 0.5430448651313782, 'Named Ent. ': 0.3726503849029541, 'Negations ': 0.32233309745788574, 'Reentrancies ': 0.7452874779701233, 'SRL ': 0.23060882091522217, 'Smatch ': 0.6375858187675476, 'Unlabeled ': 0.2739068567752838, 'max_indegree_sim': 0.4047700762748718, 'max_outdegree_sim': 0.2663877308368683, 'max_degree_sim': 0.5701860189437866, 'root_sim': 0.17169982194900513, 'quant_sim': 0.3606294095516205, 'score_wlk': 0.5462191104888916, 'score_wwlk': 0.21643537282943726, 'residual': 0.4095391035079956, 'sent_a': 'It is impossible to answer this question without a form check.', 'sent_b': 'It is difficult, if not impossible, to give a complete answer to this question.'}\n",
      "{'global': 0.2930527329444885, 'Concepts ': 0.08287302404642105, 'Frames ': 0.5382034778594971, 'Named Ent. ': 0.3801232874393463, 'Negations ': 0.08473631739616394, 'Reentrancies ': 0.6135900020599365, 'SRL ': 0.012857536785304546, 'Smatch ': 0.02923094667494297, 'Unlabeled ': 0.12814961373806, 'max_indegree_sim': -0.20406155288219452, 'max_outdegree_sim': 0.5001828670501709, 'max_degree_sim': 0.5950170159339905, 'root_sim': 0.20306776463985443, 'quant_sim': 0.5043002963066101, 'score_wlk': 0.4063763916492462, 'score_wwlk': 0.1505592167377472, 'residual': 0.2927650511264801, 'sent_a': 'You need to add this to your resume because it is the only position you have had.', 'sent_b': 'This gives you the opportunity to make your case that you really do have expertise in XXX.'}\n",
      "{'global': 0.8288869261741638, 'Concepts ': 0.824357271194458, 'Frames ': 0.7760556936264038, 'Named Ent. ': 0.8140320181846619, 'Negations ': 0.6569160223007202, 'Reentrancies ': 0.8089441657066345, 'SRL ': 0.8967337608337402, 'Smatch ': 0.6391946077346802, 'Unlabeled ': 0.7783544659614563, 'max_indegree_sim': 0.7653567790985107, 'max_outdegree_sim': 0.3767578899860382, 'max_degree_sim': 0.7619428038597107, 'root_sim': 0.8788381218910217, 'quant_sim': 0.7865964770317078, 'score_wlk': 0.8532313108444214, 'score_wwlk': 0.7613314986228943, 'residual': 0.835915207862854, 'sent_a': 'Work into it slowly.', 'sent_b': 'You work on it slowly.'}\n",
      "{'global': 0.36371690034866333, 'Concepts ': 0.34579160809516907, 'Frames ': 0.5141763091087341, 'Named Ent. ': 0.5610446333885193, 'Negations ': -0.017359545454382896, 'Reentrancies ': 0.5502661466598511, 'SRL ': 0.020480507984757423, 'Smatch ': 0.6765307188034058, 'Unlabeled ': 0.8804918527603149, 'max_indegree_sim': 0.5045682191848755, 'max_outdegree_sim': 0.5861884951591492, 'max_degree_sim': 0.4108341336250305, 'root_sim': 0.6184543967247009, 'quant_sim': 0.002805513795465231, 'score_wlk': 0.6160840392112732, 'score_wwlk': -0.007977944798767567, 'residual': 0.3222742974758148, 'sent_a': 'No, you do not NEED a bread machine.', 'sent_b': 'You do not need to worry.'}\n",
      "{'global': 0.4739772379398346, 'Concepts ': 0.4508117437362671, 'Frames ': 0.34890860319137573, 'Named Ent. ': 0.5757873058319092, 'Negations ': 0.2824758291244507, 'Reentrancies ': 0.7470126152038574, 'SRL ': 0.6932342648506165, 'Smatch ': 0.7549642324447632, 'Unlabeled ': 0.8896223902702332, 'max_indegree_sim': 0.3044039309024811, 'max_outdegree_sim': 0.35421180725097656, 'max_degree_sim': 0.29484567046165466, 'root_sim': 0.29844552278518677, 'quant_sim': 0.750282883644104, 'score_wlk': 0.6552844643592834, 'score_wwlk': 0.659288763999939, 'residual': 0.4702451825141907, 'sent_a': 'Yes, it is possible to publish a paper on model analysis.', 'sent_b': 'Yes, it is hard to publish in a peer reviewed journal.'}\n",
      "{'global': 0.43848899006843567, 'Concepts ': 0.5111193656921387, 'Frames ': 0.6971536874771118, 'Named Ent. ': -0.046536028385162354, 'Negations ': 0.3466860353946686, 'Reentrancies ': 0.693033754825592, 'SRL ': 0.4519723951816559, 'Smatch ': 0.788011908531189, 'Unlabeled ': 0.701323390007019, 'max_indegree_sim': 0.3814390301704407, 'max_outdegree_sim': 0.8079145550727844, 'max_degree_sim': 0.4426378905773163, 'root_sim': 0.5664427876472473, 'quant_sim': 0.7301359176635742, 'score_wlk': 0.626817524433136, 'score_wwlk': 0.7404947876930237, 'residual': 0.4247109591960907, 'sent_a': 'You just have to base your answer on what you do know, which is what you want.', 'sent_b': \"So if that's what you need, you really do have to be explicit about it.\"}\n",
      "{'global': 0.4891643226146698, 'Concepts ': 0.18012474477291107, 'Frames ': 0.6858206987380981, 'Named Ent. ': 0.2579210102558136, 'Negations ': 0.5458155274391174, 'Reentrancies ': 0.7792910933494568, 'SRL ': 0.6330533027648926, 'Smatch ': 0.906990110874176, 'Unlabeled ': 0.7546349763870239, 'max_indegree_sim': 0.605804443359375, 'max_outdegree_sim': 0.4139516353607178, 'max_degree_sim': 0.640680730342865, 'root_sim': 0.4844364523887634, 'quant_sim': 0.7047844529151917, 'score_wlk': 0.7210357189178467, 'score_wwlk': 0.3122747242450714, 'residual': 0.47333666682243347, 'sent_a': 'Yes, you should mention your experience.', 'sent_b': 'Yes, you should make a rsum.'}\n",
      "{'global': 0.2518875002861023, 'Concepts ': 0.5202236771583557, 'Frames ': 0.5937310457229614, 'Named Ent. ': 0.18693745136260986, 'Negations ': 0.20138294994831085, 'Reentrancies ': 0.5109078884124756, 'SRL ': 0.2436351627111435, 'Smatch ': 0.6068639755249023, 'Unlabeled ': 0.4887877106666565, 'max_indegree_sim': 0.0952562689781189, 'max_outdegree_sim': 0.620592474937439, 'max_degree_sim': 0.47233402729034424, 'root_sim': 0.6047451496124268, 'quant_sim': 0.4714542329311371, 'score_wlk': 0.47150588035583496, 'score_wwlk': 0.410417765378952, 'residual': 0.23274444043636322, 'sent_a': 'I think it is great that you had a test conducted.', 'sent_b': \"I think that's a great plan.\"}\n",
      "{'global': 0.191751167178154, 'Concepts ': 0.5635127425193787, 'Frames ': 0.12021242827177048, 'Named Ent. ': -0.2629033923149109, 'Negations ': 0.29675137996673584, 'Reentrancies ': 0.7525496482849121, 'SRL ': 0.600338876247406, 'Smatch ': 0.3475096821784973, 'Unlabeled ': 0.4235423803329468, 'max_indegree_sim': -0.07825502753257751, 'max_outdegree_sim': 0.29953277111053467, 'max_degree_sim': 0.4114415943622589, 'root_sim': 0.4458718001842499, 'quant_sim': 0.21350757777690887, 'score_wlk': 0.16952447593212128, 'score_wwlk': 0.2533065974712372, 'residual': 0.17631128430366516, 'sent_a': \"You don't have to know.\", 'sent_b': \"Other than that you don't have a tax issue.\"}\n",
      "{'global': 0.890200138092041, 'Concepts ': 0.7220973968505859, 'Frames ': 0.9436911344528198, 'Named Ent. ': 0.6949977874755859, 'Negations ': 0.9561713337898254, 'Reentrancies ': 0.9352751970291138, 'SRL ': 0.8420308828353882, 'Smatch ': 0.8366904258728027, 'Unlabeled ': 0.9521481990814209, 'max_indegree_sim': 0.8637091517448425, 'max_outdegree_sim': 0.6524865627288818, 'max_degree_sim': 0.9125761985778809, 'root_sim': 0.9469079375267029, 'quant_sim': 0.9028522968292236, 'score_wlk': 0.8517017364501953, 'score_wwlk': 0.9207547903060913, 'residual': 0.8921912312507629, 'sent_a': 'Yes, there is a rule against this.', 'sent_b': \"There's no rule against it.\"}\n",
      "{'global': 0.9892770051956177, 'Concepts ': 0.983325719833374, 'Frames ': 0.9938863515853882, 'Named Ent. ': 0.9822064638137817, 'Negations ': 0.9915692806243896, 'Reentrancies ': 0.997613787651062, 'SRL ': 0.9920206069946289, 'Smatch ': 0.9790825247764587, 'Unlabeled ': 0.9524638056755066, 'max_indegree_sim': 0.9795011878013611, 'max_outdegree_sim': 0.9905213713645935, 'max_degree_sim': 0.9951780438423157, 'root_sim': 0.9918238520622253, 'quant_sim': 0.9965343475341797, 'score_wlk': 0.9869475364685059, 'score_wwlk': 0.9917750358581543, 'residual': 0.98919278383255, 'sent_a': 'What are your goals?', 'sent_b': 'What are you goals?'}\n",
      "{'global': 0.2759183645248413, 'Concepts ': 0.2532646954059601, 'Frames ': 0.5684674382209778, 'Named Ent. ': 0.6025248169898987, 'Negations ': -0.2760867774486542, 'Reentrancies ': 0.704889178276062, 'SRL ': 0.6822836399078369, 'Smatch ': 0.6415453553199768, 'Unlabeled ': 0.5001623034477234, 'max_indegree_sim': 0.07023029029369354, 'max_outdegree_sim': 0.5894587635993958, 'max_degree_sim': -0.14060622453689575, 'root_sim': 0.37588247656822205, 'quant_sim': 0.43158602714538574, 'score_wlk': 0.5019506216049194, 'score_wwlk': 0.16509662568569183, 'residual': 0.2756901681423187, 'sent_a': \"I'm going to go out on a limb here...\", 'sent_b': \"I'm going to be very direct here.\"}\n",
      "{'global': 0.8504798412322998, 'Concepts ': 0.7802057862281799, 'Frames ': 0.8729827404022217, 'Named Ent. ': 0.8754419684410095, 'Negations ': 0.8225923180580139, 'Reentrancies ': 0.8266900181770325, 'SRL ': 0.8710340857505798, 'Smatch ': 0.9841824173927307, 'Unlabeled ': 0.8942698836326599, 'max_indegree_sim': 0.630686342716217, 'max_outdegree_sim': 0.8660181164741516, 'max_degree_sim': 0.8099691867828369, 'root_sim': 0.888864278793335, 'quant_sim': 0.8033947348594666, 'score_wlk': 0.7793652415275574, 'score_wwlk': 0.6867208480834961, 'residual': 0.8500387668609619, 'sent_a': 'If your long stay visa for Switzerland is valid, then you will not need a transit visa.', 'sent_b': 'You do not need a transit visa, but to enter Zurich you will need a visa.'}\n",
      "{'global': 0.7392945289611816, 'Concepts ': 0.7684874534606934, 'Frames ': 0.702012300491333, 'Named Ent. ': 0.7124814391136169, 'Negations ': 0.6798280477523804, 'Reentrancies ': 0.8934895396232605, 'SRL ': 0.5365956425666809, 'Smatch ': 0.7070873379707336, 'Unlabeled ': 0.2117384821176529, 'max_indegree_sim': 0.6671990156173706, 'max_outdegree_sim': 0.8184424638748169, 'max_degree_sim': 0.8524376153945923, 'root_sim': 0.7263155579566956, 'quant_sim': 0.7674281597137451, 'score_wlk': 0.712970495223999, 'score_wwlk': 0.8559141159057617, 'residual': 0.7402709126472473, 'sent_a': 'A few more point to think about:', 'sent_b': 'A few things to think about:'}\n",
      "{'global': 0.2857758104801178, 'Concepts ': 0.459696888923645, 'Frames ': 0.3342735171318054, 'Named Ent. ': 0.012522930279374123, 'Negations ': 0.29576778411865234, 'Reentrancies ': 0.7492810487747192, 'SRL ': 0.28346630930900574, 'Smatch ': 0.16685788333415985, 'Unlabeled ': 0.22416576743125916, 'max_indegree_sim': 0.20824432373046875, 'max_outdegree_sim': 0.0010210077743977308, 'max_degree_sim': 0.2567327320575714, 'root_sim': 0.07175511866807938, 'quant_sim': -0.07150738686323166, 'score_wlk': 0.5210650563240051, 'score_wwlk': 0.3000720143318176, 'residual': 0.29983317852020264, 'sent_a': \"Now you don't have to be perfect on every answer.\", 'sent_b': \"You don't have to know.\"}\n",
      "{'global': 0.7758013606071472, 'Concepts ': 0.823172926902771, 'Frames ': 0.2836326062679291, 'Named Ent. ': 0.8555101156234741, 'Negations ': 0.7945908904075623, 'Reentrancies ': 0.844934344291687, 'SRL ': 0.8584851026535034, 'Smatch ': 0.6990765929222107, 'Unlabeled ': 0.4373205304145813, 'max_indegree_sim': 0.7705095410346985, 'max_outdegree_sim': 0.8583990335464478, 'max_degree_sim': 0.659026026725769, 'root_sim': 0.5588182806968689, 'quant_sim': 0.8091257214546204, 'score_wlk': 0.7957044243812561, 'score_wwlk': 0.6886411905288696, 'residual': 0.7832902669906616, 'sent_a': \"I'm afraid we are out of luck in this one.\", 'sent_b': 'I think this is one of those things where you are just out of luck.'}\n",
      "{'global': 0.5062106251716614, 'Concepts ': 0.7289572954177856, 'Frames ': 0.551821231842041, 'Named Ent. ': 0.03899727016687393, 'Negations ': 0.3695584237575531, 'Reentrancies ': 0.8597099184989929, 'SRL ': 0.5478965640068054, 'Smatch ': 0.32964345812797546, 'Unlabeled ': 0.388564795255661, 'max_indegree_sim': 0.5893612504005432, 'max_outdegree_sim': 0.6150635480880737, 'max_degree_sim': 0.6723906993865967, 'root_sim': 0.4547814428806305, 'quant_sim': 0.7720853090286255, 'score_wlk': 0.8197351098060608, 'score_wwlk': 0.7056758999824524, 'residual': 0.49911001324653625, 'sent_a': 'It is more deciding what you should do and what you should get others to do.', 'sent_b': \"Point is: know what you want and don't be afraid to ask for it.\"}\n",
      "{'global': 0.5594483613967896, 'Concepts ': 0.3955546021461487, 'Frames ': 0.6243956685066223, 'Named Ent. ': 0.5584954023361206, 'Negations ': 0.4777815639972687, 'Reentrancies ': 0.6006837487220764, 'SRL ': 0.3879337012767792, 'Smatch ': 0.6239109635353088, 'Unlabeled ': -0.3617389500141144, 'max_indegree_sim': 0.7065699100494385, 'max_outdegree_sim': 0.3998259902000427, 'max_degree_sim': 0.6967526078224182, 'root_sim': 0.6153287887573242, 'quant_sim': 0.6406107544898987, 'score_wlk': 0.7880054712295532, 'score_wwlk': 0.6579300165176392, 'residual': 0.5593433976173401, 'sent_a': 'It varies by the situation.', 'sent_b': 'This varies by institution.'}\n",
      "{'global': 0.7633565068244934, 'Concepts ': 0.9016799926757812, 'Frames ': 0.7141240835189819, 'Named Ent. ': 0.6089134812355042, 'Negations ': 0.828339695930481, 'Reentrancies ': 0.7264308333396912, 'SRL ': 0.8394753932952881, 'Smatch ': 0.6412509083747864, 'Unlabeled ': 0.8613095879554749, 'max_indegree_sim': 0.8129115700721741, 'max_outdegree_sim': 0.8517974019050598, 'max_degree_sim': 0.5803630352020264, 'root_sim': 0.8176882863044739, 'quant_sim': 0.855139970779419, 'score_wlk': 0.7786036133766174, 'score_wwlk': 0.7878430485725403, 'residual': 0.7594515681266785, 'sent_a': 'I have the same thing.', 'sent_b': 'I have the same problem.'}\n",
      "{'global': 0.7537488341331482, 'Concepts ': 0.8408440351486206, 'Frames ': 0.7222281098365784, 'Named Ent. ': 0.5854305624961853, 'Negations ': 0.8449318408966064, 'Reentrancies ': 0.763460099697113, 'SRL ': 0.8778201341629028, 'Smatch ': 0.6310690641403198, 'Unlabeled ': 0.890471875667572, 'max_indegree_sim': 0.8799021244049072, 'max_outdegree_sim': 0.8267253041267395, 'max_degree_sim': 0.6453967690467834, 'root_sim': 0.8150308132171631, 'quant_sim': 0.8353111743927002, 'score_wlk': 0.7918815612792969, 'score_wwlk': 0.7939147353172302, 'residual': 0.7487053275108337, 'sent_a': 'I have the same thing.', 'sent_b': 'I have the exact same problem.'}\n",
      "{'global': 0.07510321587324142, 'Concepts ': -0.02848183363676071, 'Frames ': -0.07698310911655426, 'Named Ent. ': -0.28760242462158203, 'Negations ': 0.3668510317802429, 'Reentrancies ': 0.4254229962825775, 'SRL ': 0.6120932102203369, 'Smatch ': 0.19603870809078217, 'Unlabeled ': 0.1432662010192871, 'max_indegree_sim': 0.06956835091114044, 'max_outdegree_sim': 0.3371320962905884, 'max_degree_sim': 0.23875607550144196, 'root_sim': 0.1765206903219223, 'quant_sim': 0.2506893277168274, 'score_wlk': 0.7303195595741272, 'score_wwlk': 0.5167257785797119, 'residual': 0.05842570587992668, 'sent_a': 'You are on the right path.', 'sent_b': \"You're right on about the sample definition.\"}\n",
      "{'global': 0.26173996925354004, 'Concepts ': 0.4710644483566284, 'Frames ': 0.0398808978497982, 'Named Ent. ': -0.05344425141811371, 'Negations ': 0.2751084268093109, 'Reentrancies ': 0.5123265981674194, 'SRL ': 0.6489728689193726, 'Smatch ': 0.7296050786972046, 'Unlabeled ': 0.7117946147918701, 'max_indegree_sim': 0.3829185366630554, 'max_outdegree_sim': -0.07982181757688522, 'max_degree_sim': 0.07293631136417389, 'root_sim': 0.48706504702568054, 'quant_sim': 0.5292572379112244, 'score_wlk': 0.397549569606781, 'score_wwlk': -0.11010205745697021, 'residual': 0.25351348519325256, 'sent_a': 'Primer/paint will not work.', 'sent_b': 'Nope that will not work.'}\n",
      "{'global': 0.2723138928413391, 'Concepts ': 0.2041158676147461, 'Frames ': 0.6512089967727661, 'Named Ent. ': -0.08419130742549896, 'Negations ': 0.41284799575805664, 'Reentrancies ': 0.5878912210464478, 'SRL ': 0.6606289744377136, 'Smatch ': -0.3755675256252289, 'Unlabeled ': 0.34159407019615173, 'max_indegree_sim': 0.3577366769313812, 'max_outdegree_sim': 0.3824809193611145, 'max_degree_sim': 0.5815950632095337, 'root_sim': 0.4524634778499603, 'quant_sim': 0.576583981513977, 'score_wlk': 0.2438524067401886, 'score_wwlk': 0.4838449954986572, 'residual': 0.2592322528362274, 'sent_a': 'The best thing you can do is to know your stuff.', 'sent_b': 'The best thing you can do is find something else interesting to do while you wait on hold.'}\n",
      "{'global': 0.9505036473274231, 'Concepts ': 0.9670062065124512, 'Frames ': 0.9912275671958923, 'Named Ent. ': 0.9602521657943726, 'Negations ': 0.9585379958152771, 'Reentrancies ': 0.9695767760276794, 'SRL ': 0.9205796122550964, 'Smatch ': 0.8322086930274963, 'Unlabeled ': 0.9498656392097473, 'max_indegree_sim': 0.9260861873626709, 'max_outdegree_sim': 0.9255464673042297, 'max_degree_sim': 0.947148859500885, 'root_sim': 0.9579153656959534, 'quant_sim': 0.9382163286209106, 'score_wlk': 0.9614434838294983, 'score_wwlk': 0.9675382375717163, 'residual': 0.9501948952674866, 'sent_a': \"It's also a matter of taste.\", 'sent_b': \"It's mostly a matter of taste.\"}\n",
      "{'global': 0.20719344913959503, 'Concepts ': -0.2172566056251526, 'Frames ': 0.4463258385658264, 'Named Ent. ': 0.30268368124961853, 'Negations ': -0.20271676778793335, 'Reentrancies ': 0.5903387069702148, 'SRL ': 0.37236857414245605, 'Smatch ': 0.07306943088769913, 'Unlabeled ': 0.40358030796051025, 'max_indegree_sim': 0.0814761221408844, 'max_outdegree_sim': 0.39986878633499146, 'max_degree_sim': 0.1936640590429306, 'root_sim': 0.01683250442147255, 'quant_sim': 0.33506616950035095, 'score_wlk': -0.026734989136457443, 'score_wwlk': 0.2702197730541229, 'residual': 0.21865981817245483, 'sent_a': 'I was in a similar situation.', 'sent_b': 'Personally when I was in a similar situation I sent the couple a card and a gift.'}\n",
      "{'global': 0.37551793456077576, 'Concepts ': 0.5599234700202942, 'Frames ': 0.26305317878723145, 'Named Ent. ': 0.12678301334381104, 'Negations ': -0.0018487508641555905, 'Reentrancies ': 0.6474385857582092, 'SRL ': 0.7377396821975708, 'Smatch ': -0.14962676167488098, 'Unlabeled ': 0.34597283601760864, 'max_indegree_sim': 0.4311750829219818, 'max_outdegree_sim': -0.12047890573740005, 'max_degree_sim': 0.4465886056423187, 'root_sim': 0.37817826867103577, 'quant_sim': 0.585381031036377, 'score_wlk': 0.3449398875236511, 'score_wwlk': 0.7832614183425903, 'residual': 0.38653260469436646, 'sent_a': 'You can use it, too.', 'sent_b': 'Yes, you can do it.'}\n",
      "{'global': 0.563149094581604, 'Concepts ': 0.5660048127174377, 'Frames ': 0.7565319538116455, 'Named Ent. ': 0.5499524474143982, 'Negations ': 0.4015381932258606, 'Reentrancies ': 0.7000464200973511, 'SRL ': 0.5993683934211731, 'Smatch ': 0.8567082285881042, 'Unlabeled ': 0.7966619729995728, 'max_indegree_sim': 0.6661942601203918, 'max_outdegree_sim': 0.9017128944396973, 'max_degree_sim': 0.6836783289909363, 'root_sim': 0.6904576420783997, 'quant_sim': 0.7104893326759338, 'score_wlk': 0.719407856464386, 'score_wwlk': 0.5666383504867554, 'residual': 0.5464968085289001, 'sent_a': 'You have to decide what you want to get out of this.', 'sent_b': 'You have to find out what works for you.'}\n",
      "{'global': 0.41987985372543335, 'Concepts ': 0.11957801133394241, 'Frames ': 0.25855347514152527, 'Named Ent. ': -0.06740686297416687, 'Negations ': 0.12011662125587463, 'Reentrancies ': 0.7642757892608643, 'SRL ': 0.5795565843582153, 'Smatch ': 0.6654008626937866, 'Unlabeled ': 0.08873791247606277, 'max_indegree_sim': 0.7389789819717407, 'max_outdegree_sim': -0.08646116405725479, 'max_degree_sim': 0.240474134683609, 'root_sim': 0.18468229472637177, 'quant_sim': 0.23435434699058533, 'score_wlk': 0.09833435714244843, 'score_wwlk': 0.6268464922904968, 'residual': 0.4399906098842621, 'sent_a': 'It depends on what they are.', 'sent_b': \"It depends on how it's used.\"}\n",
      "{'global': 0.7828282117843628, 'Concepts ': 0.9095192551612854, 'Frames ': 0.7834057807922363, 'Named Ent. ': 0.5194094181060791, 'Negations ': 0.8098347783088684, 'Reentrancies ': 0.8162355422973633, 'SRL ': 0.6388954520225525, 'Smatch ': 0.9253854751586914, 'Unlabeled ': 0.8435100317001343, 'max_indegree_sim': 0.77887362241745, 'max_outdegree_sim': 0.8069823980331421, 'max_degree_sim': 0.6921746134757996, 'root_sim': 0.7298984527587891, 'quant_sim': 0.7197046875953674, 'score_wlk': 0.7754702568054199, 'score_wwlk': 0.7259072065353394, 'residual': 0.7872974276542664, 'sent_a': 'I had the same problem as you.', 'sent_b': 'I have just had the same problem.'}\n",
      "{'global': 0.46184614300727844, 'Concepts ': 0.5890460014343262, 'Frames ': 0.06675370037555695, 'Named Ent. ': 0.4178139865398407, 'Negations ': 0.23835806548595428, 'Reentrancies ': 0.6813291311264038, 'SRL ': 0.5605185627937317, 'Smatch ': -0.08836603164672852, 'Unlabeled ': 0.29121652245521545, 'max_indegree_sim': 0.1382029950618744, 'max_outdegree_sim': 0.3945370316505432, 'max_degree_sim': 0.5351033210754395, 'root_sim': 0.6242386102676392, 'quant_sim': 0.42456838488578796, 'score_wlk': 0.4813818335533142, 'score_wwlk': 0.3526536822319031, 'residual': 0.4711756408214569, 'sent_a': 'Sometime if you really want it you might need to pay an agency to get the place for you.', 'sent_b': 'You could probably get a tour agency to do it for you but it would cost you.'}\n",
      "{'global': 0.3072050213813782, 'Concepts ': 0.12047913670539856, 'Frames ': 0.5871345400810242, 'Named Ent. ': 0.5141198039054871, 'Negations ': 0.4428277313709259, 'Reentrancies ': 0.9220901131629944, 'SRL ': 0.577738344669342, 'Smatch ': 0.49276965856552124, 'Unlabeled ': 0.5670042037963867, 'max_indegree_sim': 0.21167072653770447, 'max_outdegree_sim': 0.4427175521850586, 'max_degree_sim': 0.5463621616363525, 'root_sim': -0.10017912089824677, 'quant_sim': -0.013316964730620384, 'score_wlk': 0.6731361746788025, 'score_wwlk': 0.39714494347572327, 'residual': 0.27976927161216736, 'sent_a': \"You don't need to know everything.\", 'sent_b': \"They just don't want you to know it.\"}\n",
      "{'global': 0.6536696553230286, 'Concepts ': 0.865529477596283, 'Frames ': 0.6698639988899231, 'Named Ent. ': 0.6804510951042175, 'Negations ': 0.02140515297651291, 'Reentrancies ': 0.9074750542640686, 'SRL ': 0.6917981505393982, 'Smatch ': 0.6708244681358337, 'Unlabeled ': 0.5158873796463013, 'max_indegree_sim': 0.5857328772544861, 'max_outdegree_sim': 0.3406718671321869, 'max_degree_sim': 0.12463279068470001, 'root_sim': 0.5702738761901855, 'quant_sim': 0.7342048287391663, 'score_wlk': 0.6479724049568176, 'score_wwlk': 0.811302125453949, 'residual': 0.6591439247131348, 'sent_a': 'Also it is useful to keep the consistency in your story.', 'sent_b': 'It is important to keep the story in the scope of your limit.'}\n",
      "{'global': 0.31579846143722534, 'Concepts ': 0.5612884759902954, 'Frames ': 0.23056308925151825, 'Named Ent. ': -0.04597204551100731, 'Negations ': 0.3131932020187378, 'Reentrancies ': 0.8824341297149658, 'SRL ': 0.4115632176399231, 'Smatch ': -0.22261539101600647, 'Unlabeled ': -0.13052712380886078, 'max_indegree_sim': 0.38872578740119934, 'max_outdegree_sim': 0.40455225110054016, 'max_degree_sim': 0.3202679753303528, 'root_sim': -0.2723577320575714, 'quant_sim': 0.7229363918304443, 'score_wlk': 0.8098061084747314, 'score_wwlk': 0.5750002861022949, 'residual': 0.3121297359466553, 'sent_a': \"I think it's just a habit.\", 'sent_b': \"I think it's a bad idea.\"}\n",
      "{'global': 0.5669751167297363, 'Concepts ': 0.7532090544700623, 'Frames ': 0.6208915114402771, 'Named Ent. ': 0.7036605477333069, 'Negations ': 0.45359233021736145, 'Reentrancies ': 0.7780488729476929, 'SRL ': 0.6248401403427124, 'Smatch ': 0.46905457973480225, 'Unlabeled ': 0.6320176124572754, 'max_indegree_sim': 0.666507363319397, 'max_outdegree_sim': 0.6452284455299377, 'max_degree_sim': 0.7606496810913086, 'root_sim': 0.4295763075351715, 'quant_sim': 0.8287954926490784, 'score_wlk': 0.7037118673324585, 'score_wwlk': 0.8490495085716248, 'residual': 0.5565139651298523, 'sent_a': \"It's not a good idea.\", 'sent_b': \"No, it's not a good thing.\"}\n",
      "{'global': 0.9202661514282227, 'Concepts ': 0.9603450894355774, 'Frames ': 0.8353785872459412, 'Named Ent. ': 0.9539350271224976, 'Negations ': 0.939295768737793, 'Reentrancies ': 0.9197002053260803, 'SRL ': 0.8658531308174133, 'Smatch ': 0.8658958077430725, 'Unlabeled ': 0.8645719885826111, 'max_indegree_sim': 0.7201165556907654, 'max_outdegree_sim': 0.8880060315132141, 'max_degree_sim': 0.928985059261322, 'root_sim': 0.8321033120155334, 'quant_sim': 0.9632180333137512, 'score_wlk': 0.9613274931907654, 'score_wwlk': 0.9662445783615112, 'residual': 0.9201586842536926, 'sent_a': \"I'm thinking it's a bad idea.\", 'sent_b': \"I think it's a bad idea.\"}\n",
      "{'global': 0.1637779325246811, 'Concepts ': 0.53236323595047, 'Frames ': 0.002568728057667613, 'Named Ent. ': 0.26406601071357727, 'Negations ': 0.009541742503643036, 'Reentrancies ': 0.7398508787155151, 'SRL ': 0.2880820631980896, 'Smatch ': 0.009601807221770287, 'Unlabeled ': -0.042229097336530685, 'max_indegree_sim': 0.6959764957427979, 'max_outdegree_sim': 0.46195265650749207, 'max_degree_sim': 0.12959006428718567, 'root_sim': 0.29853469133377075, 'quant_sim': 0.15210765600204468, 'score_wlk': 0.31072208285331726, 'score_wwlk': 0.05753426253795624, 'residual': 0.14507894217967987, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'You need to pick how and where you want to place your foot and do it.'}\n",
      "{'global': 0.6115216016769409, 'Concepts ': 0.7216219902038574, 'Frames ': 0.4508303999900818, 'Named Ent. ': 0.42626476287841797, 'Negations ': 0.38198336958885193, 'Reentrancies ': 0.873887836933136, 'SRL ': 0.5988851189613342, 'Smatch ': 0.6090494394302368, 'Unlabeled ': 0.39250877499580383, 'max_indegree_sim': 0.7722237706184387, 'max_outdegree_sim': 0.8587543368339539, 'max_degree_sim': 0.44032275676727295, 'root_sim': 0.3809041380882263, 'quant_sim': 0.7464767694473267, 'score_wlk': 0.6101037263870239, 'score_wwlk': 0.7153347730636597, 'residual': 0.6146252751350403, 'sent_a': 'You have a problem.', 'sent_b': 'This is a big problem.'}\n",
      "{'global': 0.16626085340976715, 'Concepts ': 0.5218384265899658, 'Frames ': 0.19545553624629974, 'Named Ent. ': 0.618343710899353, 'Negations ': 0.38214775919914246, 'Reentrancies ': 0.5207850337028503, 'SRL ': 0.3778553903102875, 'Smatch ': -0.0554027184844017, 'Unlabeled ': 0.1870458424091339, 'max_indegree_sim': 0.5359453558921814, 'max_outdegree_sim': -0.11624198406934738, 'max_degree_sim': -0.03526221588253975, 'root_sim': 0.3097597062587738, 'quant_sim': 0.33037951588630676, 'score_wlk': 0.5902183651924133, 'score_wwlk': -0.02504589594900608, 'residual': 0.14914937317371368, 'sent_a': 'Unfortunately the answer to your question is we simply do not know.', 'sent_b': 'If the conversation is not about work, you know what the answer is to your own question.'}\n",
      "{'global': 0.7360978722572327, 'Concepts ': 0.6267247200012207, 'Frames ': 0.7032812237739563, 'Named Ent. ': 0.7040113210678101, 'Negations ': 0.4855077564716339, 'Reentrancies ': 0.6550995707511902, 'SRL ': 0.7336262464523315, 'Smatch ': 0.8493561744689941, 'Unlabeled ': 0.8347967267036438, 'max_indegree_sim': 0.740216851234436, 'max_outdegree_sim': 0.658787727355957, 'max_degree_sim': 0.6812551021575928, 'root_sim': 0.5367889404296875, 'quant_sim': 0.7191092371940613, 'score_wlk': 0.7247311472892761, 'score_wwlk': 0.7297508120536804, 'residual': 0.7433434128761292, 'sent_a': 'Unfortunately, this question cannot be answered in its full generality.', 'sent_b': 'This cannot be answered in general.'}\n",
      "{'global': 0.7999423742294312, 'Concepts ': 0.7796404361724854, 'Frames ': 0.6672242283821106, 'Named Ent. ': 0.8431433439254761, 'Negations ': 0.5775216221809387, 'Reentrancies ': 0.3405308127403259, 'SRL ': 0.8728011250495911, 'Smatch ': 0.33510345220565796, 'Unlabeled ': 0.7069900035858154, 'max_indegree_sim': 0.3941894471645355, 'max_outdegree_sim': 0.8957741260528564, 'max_degree_sim': 0.6878110766410828, 'root_sim': 0.39407286047935486, 'quant_sim': 0.7741358876228333, 'score_wlk': 0.6425363421440125, 'score_wwlk': 0.8021502494812012, 'residual': 0.8131601214408875, 'sent_a': 'I would personally beware of the Motley Fool.', 'sent_b': 'I would recommend looking at The Motley Fool.'}\n",
      "{'global': 0.5087750554084778, 'Concepts ': 0.32248303294181824, 'Frames ': 0.44411778450012207, 'Named Ent. ': 0.40037837624549866, 'Negations ': 0.3066839575767517, 'Reentrancies ': 0.8156054019927979, 'SRL ': 0.5246280431747437, 'Smatch ': 0.36708468198776245, 'Unlabeled ': 0.6515586972236633, 'max_indegree_sim': -0.07039618492126465, 'max_outdegree_sim': 0.49899744987487793, 'max_degree_sim': 0.4169505834579468, 'root_sim': 0.11728870868682861, 'quant_sim': 0.3649739623069763, 'score_wlk': 0.5249919891357422, 'score_wwlk': 0.6279805302619934, 'residual': 0.5258879661560059, 'sent_a': \"I think there isn't a general answer.\", 'sent_b': \"I don't think there is a single definition.\"}\n",
      "{'global': 0.899354875087738, 'Concepts ': 0.9483217597007751, 'Frames ': 0.8946443200111389, 'Named Ent. ': 0.8125002384185791, 'Negations ': 0.9111985564231873, 'Reentrancies ': 0.9294337034225464, 'SRL ': 0.8627709150314331, 'Smatch ': 0.9645659327507019, 'Unlabeled ': 0.9673600196838379, 'max_indegree_sim': 0.9409704208374023, 'max_outdegree_sim': 0.9023070335388184, 'max_degree_sim': 0.9493163824081421, 'root_sim': 0.8924654722213745, 'quant_sim': 0.9564846158027649, 'score_wlk': 0.9079113602638245, 'score_wwlk': 0.8109658360481262, 'residual': 0.9002417325973511, 'sent_a': \"It's not a good idea.\", 'sent_b': 'Not a good idea.'}\n",
      "{'global': 0.22787053883075714, 'Concepts ': 0.27686405181884766, 'Frames ': 0.3804568350315094, 'Named Ent. ': 0.47391653060913086, 'Negations ': 0.3578786551952362, 'Reentrancies ': 0.3599439859390259, 'SRL ': 0.2225402444601059, 'Smatch ': 0.29041048884391785, 'Unlabeled ': 0.7528241872787476, 'max_indegree_sim': 0.2330302745103836, 'max_outdegree_sim': 0.6492959260940552, 'max_degree_sim': 0.1904134452342987, 'root_sim': 0.29699188470840454, 'quant_sim': 0.24431002140045166, 'score_wlk': 0.3379582166671753, 'score_wwlk': 0.27587226033210754, 'residual': 0.2037154585123062, 'sent_a': \"You don't need any visa.\", 'sent_b': \"You don't have to worry.\"}\n",
      "{'global': 0.19355136156082153, 'Concepts ': 0.41621333360671997, 'Frames ': 0.27826887369155884, 'Named Ent. ': 0.23829449713230133, 'Negations ': -0.3676406145095825, 'Reentrancies ': 0.3884217441082001, 'SRL ': 0.09909781068563461, 'Smatch ': -0.06511107832193375, 'Unlabeled ': 0.5663899183273315, 'max_indegree_sim': 0.45747044682502747, 'max_outdegree_sim': 0.4368438124656677, 'max_degree_sim': 0.29795971512794495, 'root_sim': 0.003526671789586544, 'quant_sim': 0.25278156995773315, 'score_wlk': 0.2746735215187073, 'score_wwlk': 0.5181158781051636, 'residual': 0.18434974551200867, 'sent_a': 'You have to decide what you want to get out of this.', 'sent_b': \"You're probably going to have to read various blogs to get all the information you want.\"}\n",
      "{'global': 0.7850472927093506, 'Concepts ': 0.8545023798942566, 'Frames ': 0.7502336502075195, 'Named Ent. ': 0.7615087032318115, 'Negations ': 0.7507425546646118, 'Reentrancies ': 0.8468419909477234, 'SRL ': 0.8914132118225098, 'Smatch ': 0.9174027442932129, 'Unlabeled ': 0.9529687762260437, 'max_indegree_sim': 0.8342970609664917, 'max_outdegree_sim': 0.7226940989494324, 'max_degree_sim': 0.7228205800056458, 'root_sim': 0.7698385119438171, 'quant_sim': 0.8560681343078613, 'score_wlk': 0.7302491068840027, 'score_wwlk': 0.654346227645874, 'residual': 0.7877554297447205, 'sent_a': 'I had the same problem as you.', 'sent_b': 'I have exactly the same problem.'}\n",
      "{'global': 0.5474033355712891, 'Concepts ': 0.42638131976127625, 'Frames ': 0.5993617177009583, 'Named Ent. ': 0.7220422029495239, 'Negations ': 0.05683493986725807, 'Reentrancies ': 0.6183385252952576, 'SRL ': 0.5483835339546204, 'Smatch ': 0.8447597622871399, 'Unlabeled ': 0.8353433609008789, 'max_indegree_sim': 0.21132799983024597, 'max_outdegree_sim': 0.4657982289791107, 'max_degree_sim': 0.4457189738750458, 'root_sim': 0.20445245504379272, 'quant_sim': 0.39799368381500244, 'score_wlk': 0.8141772150993347, 'score_wwlk': 0.6518160104751587, 'residual': 0.5492043495178223, 'sent_a': 'My answer to your question is \"Probably Not\".', 'sent_b': 'The answer to your question is not really.'}\n",
      "{'global': 0.3921748399734497, 'Concepts ': 0.44459912180900574, 'Frames ': 0.5187727212905884, 'Named Ent. ': 0.6225717663764954, 'Negations ': 0.380001962184906, 'Reentrancies ': 0.7804477214813232, 'SRL ': 0.386014461517334, 'Smatch ': -0.07686933875083923, 'Unlabeled ': 0.13161006569862366, 'max_indegree_sim': 0.7484956383705139, 'max_outdegree_sim': 0.17898839712142944, 'max_degree_sim': 0.589377224445343, 'root_sim': 0.3143276870250702, 'quant_sim': 0.5657476782798767, 'score_wlk': 0.5995177626609802, 'score_wwlk': 0.37864163517951965, 'residual': 0.3948567509651184, 'sent_a': 'You should do it.', 'sent_b': 'You should never do it.'}\n",
      "{'global': 0.9111079573631287, 'Concepts ': 0.9720442891120911, 'Frames ': 0.932769238948822, 'Named Ent. ': 0.8150433897972107, 'Negations ': 0.9492425918579102, 'Reentrancies ': 0.8272563815116882, 'SRL ': 0.7929350733757019, 'Smatch ': 0.9159546494483948, 'Unlabeled ': 0.8934335112571716, 'max_indegree_sim': 0.889558732509613, 'max_outdegree_sim': 0.9147757887840271, 'max_degree_sim': 0.8118247389793396, 'root_sim': 0.9031304717063904, 'quant_sim': 0.9332180023193359, 'score_wlk': 0.8634070754051208, 'score_wwlk': 0.8905405402183533, 'residual': 0.913388729095459, 'sent_a': 'I had the same problem as you.', 'sent_b': 'I had the same issue.'}\n",
      "{'global': 0.5197927951812744, 'Concepts ': 0.340774804353714, 'Frames ': 0.29124459624290466, 'Named Ent. ': 0.729671835899353, 'Negations ': 0.34714940190315247, 'Reentrancies ': 0.7265651822090149, 'SRL ': 0.6033820509910583, 'Smatch ': -0.40023279190063477, 'Unlabeled ': 0.17126548290252686, 'max_indegree_sim': 0.6910263299942017, 'max_outdegree_sim': 0.424620121717453, 'max_degree_sim': 0.5665557980537415, 'root_sim': 0.5430543422698975, 'quant_sim': 0.8126638531684875, 'score_wlk': 0.5905336141586304, 'score_wwlk': 0.6371675729751587, 'residual': 0.5426414608955383, 'sent_a': 'Hope this is what you are looking for.', 'sent_b': \"Is this the kind of thing you're looking for ?\"}\n",
      "{'global': 0.8939204812049866, 'Concepts ': 0.9254375100135803, 'Frames ': 0.9066534638404846, 'Named Ent. ': 0.7709974050521851, 'Negations ': 0.8349990844726562, 'Reentrancies ': 0.9069545865058899, 'SRL ': 0.8578693270683289, 'Smatch ': 0.9025360941886902, 'Unlabeled ': 0.8956996202468872, 'max_indegree_sim': 0.8690732717514038, 'max_outdegree_sim': 0.9489403367042542, 'max_degree_sim': 0.8639528751373291, 'root_sim': 0.8807908296585083, 'quant_sim': 0.9063345193862915, 'score_wlk': 0.884929358959198, 'score_wwlk': 0.8995342254638672, 'residual': 0.8950952887535095, 'sent_a': 'There are two things to consider:', 'sent_b': 'I think there are two important things to consider:'}\n",
      "{'global': 0.8952698111534119, 'Concepts ': 0.9625610113143921, 'Frames ': 0.9227587580680847, 'Named Ent. ': 0.9419775009155273, 'Negations ': 0.6979547739028931, 'Reentrancies ': 0.9710100293159485, 'SRL ': 0.8404216170310974, 'Smatch ': 0.9544753432273865, 'Unlabeled ': 0.8489900231361389, 'max_indegree_sim': 0.880273163318634, 'max_outdegree_sim': 0.8830342888832092, 'max_degree_sim': 0.9111320972442627, 'root_sim': 0.9311608672142029, 'quant_sim': 0.9008828997612, 'score_wlk': 0.933232307434082, 'score_wwlk': 0.8761261105537415, 'residual': 0.8948747515678406, 'sent_a': \"It's pretty much up to you.\", 'sent_b': 'It is up to you.'}\n",
      "{'global': 0.4938867688179016, 'Concepts ': 0.5029281973838806, 'Frames ': 0.514708936214447, 'Named Ent. ': 0.3060005307197571, 'Negations ': 0.7200238704681396, 'Reentrancies ': 0.5863933563232422, 'SRL ': 0.7958821654319763, 'Smatch ': 0.16925720870494843, 'Unlabeled ': 0.7235082387924194, 'max_indegree_sim': 0.5328356623649597, 'max_outdegree_sim': 0.4876018464565277, 'max_degree_sim': 0.6578559279441833, 'root_sim': 0.4207777976989746, 'quant_sim': 0.509689450263977, 'score_wlk': 0.7880295515060425, 'score_wwlk': 0.5437580347061157, 'residual': 0.4877413809299469, 'sent_a': 'There are a few things you can do: ', 'sent_b': 'There are a few things you need to consider:'}\n",
      "{'global': 0.5549926161766052, 'Concepts ': 0.3919954001903534, 'Frames ': 0.33990204334259033, 'Named Ent. ': 0.6314620971679688, 'Negations ': 0.1511053591966629, 'Reentrancies ': 0.7536022067070007, 'SRL ': 0.4814442992210388, 'Smatch ': -0.035467930138111115, 'Unlabeled ': 0.04365161433815956, 'max_indegree_sim': 0.09814269840717316, 'max_outdegree_sim': 0.5777726173400879, 'max_degree_sim': 0.5843956470489502, 'root_sim': 0.024894800037145615, 'quant_sim': 0.5219824910163879, 'score_wlk': 0.4236409664154053, 'score_wwlk': 0.055666644126176834, 'residual': 0.5811059474945068, 'sent_a': 'The answer to this and all such similar questions is in two stages:', 'sent_b': 'The answer to both questions is: Yes.'}\n",
      "{'global': 0.31588393449783325, 'Concepts ': 0.5632861256599426, 'Frames ': 0.36473914980888367, 'Named Ent. ': 0.08992711454629898, 'Negations ': 0.45833680033683777, 'Reentrancies ': 0.30869922041893005, 'SRL ': 0.024753713980317116, 'Smatch ': 0.48969581723213196, 'Unlabeled ': 0.48951882123947144, 'max_indegree_sim': 0.2511701285839081, 'max_outdegree_sim': -0.04055584967136383, 'max_degree_sim': -0.007174510043114424, 'root_sim': -0.04126342013478279, 'quant_sim': 0.21492478251457214, 'score_wlk': 0.1372058242559433, 'score_wwlk': 0.49466922879219055, 'residual': 0.3318483829498291, 'sent_a': 'It is impossible to answer this question without a form check.', 'sent_b': 'This is a part answer to your question'}\n",
      "{'global': 0.6864137649536133, 'Concepts ': 0.5459399223327637, 'Frames ': 0.4378758370876312, 'Named Ent. ': 0.6300773620605469, 'Negations ': 0.6671003699302673, 'Reentrancies ': 0.7860230803489685, 'SRL ': 0.713742196559906, 'Smatch ': 0.4148273169994354, 'Unlabeled ': 0.7009842395782471, 'max_indegree_sim': 0.4045650362968445, 'max_outdegree_sim': 0.6560968160629272, 'max_degree_sim': 0.8681151270866394, 'root_sim': 0.6046419739723206, 'quant_sim': 0.7973453998565674, 'score_wlk': 0.7753681540489197, 'score_wwlk': 0.8231808543205261, 'residual': 0.6907804608345032, 'sent_a': 'There are a few things you can do: ', 'sent_b': 'There are a few things I think you should do.'}\n",
      "{'global': 0.5720809698104858, 'Concepts ': 0.3085996210575104, 'Frames ': 0.7739145159721375, 'Named Ent. ': 0.5920425057411194, 'Negations ': 0.648625910282135, 'Reentrancies ': 0.8853083848953247, 'SRL ': 0.5737919807434082, 'Smatch ': 0.6703780293464661, 'Unlabeled ': 0.48246103525161743, 'max_indegree_sim': 0.7460854649543762, 'max_outdegree_sim': 0.5553286075592041, 'max_degree_sim': 0.47295641899108887, 'root_sim': 0.6404703259468079, 'quant_sim': 0.3619500696659088, 'score_wlk': 0.5710037350654602, 'score_wwlk': 0.2763044834136963, 'residual': 0.5725001096725464, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'It depends on what and where you are going to do.'}\n",
      "{'global': 0.3423306941986084, 'Concepts ': 0.5379886627197266, 'Frames ': 0.36347082257270813, 'Named Ent. ': 0.11467304825782776, 'Negations ': 0.15152524411678314, 'Reentrancies ': 0.6161452531814575, 'SRL ': 0.7699511051177979, 'Smatch ': -0.42285510897636414, 'Unlabeled ': 0.5362042188644409, 'max_indegree_sim': -0.024401990696787834, 'max_outdegree_sim': 0.03498335927724838, 'max_degree_sim': 0.10266582667827606, 'root_sim': 0.38497644662857056, 'quant_sim': 0.6351374387741089, 'score_wlk': 0.5655435919761658, 'score_wwlk': 0.3977532982826233, 'residual': 0.34841570258140564, 'sent_a': 'There are a few possible reasons.', 'sent_b': 'There are a few reasons that paint wrinkles.'}\n",
      "{'global': 0.9425278902053833, 'Concepts ': 0.964715838432312, 'Frames ': 0.9140815734863281, 'Named Ent. ': 0.9120808839797974, 'Negations ': 0.9044055342674255, 'Reentrancies ': 0.9908447265625, 'SRL ': 0.9427585005760193, 'Smatch ': 0.9728451371192932, 'Unlabeled ': 0.9771293997764587, 'max_indegree_sim': 0.9385984539985657, 'max_outdegree_sim': 0.9494637250900269, 'max_degree_sim': 0.9605013728141785, 'root_sim': 0.929179310798645, 'quant_sim': 0.9706664085388184, 'score_wlk': 0.9147988557815552, 'score_wwlk': 0.9507800936698914, 'residual': 0.9431747794151306, 'sent_a': 'There are three options:', 'sent_b': 'There are only three options:'}\n",
      "{'global': 0.8131406903266907, 'Concepts ': 0.9046036601066589, 'Frames ': 0.7851871848106384, 'Named Ent. ': 0.8000887036323547, 'Negations ': 0.7606350779533386, 'Reentrancies ': 0.9208976030349731, 'SRL ': 0.6566193699836731, 'Smatch ': 0.8107849955558777, 'Unlabeled ': 0.47918400168418884, 'max_indegree_sim': 0.7956496477127075, 'max_outdegree_sim': 0.5981540679931641, 'max_degree_sim': 0.8746093511581421, 'root_sim': 0.6557395458221436, 'quant_sim': 0.9093649387359619, 'score_wlk': 0.5008211135864258, 'score_wwlk': 0.7286122441291809, 'residual': 0.8163979649543762, 'sent_a': 'So think it in this way.', 'sent_b': 'Think of it this way.'}\n",
      "{'global': 0.99216628074646, 'Concepts ': 0.9901174902915955, 'Frames ': 0.9941754341125488, 'Named Ent. ': 0.994471549987793, 'Negations ': 0.9901692271232605, 'Reentrancies ': 0.9926964640617371, 'SRL ': 0.9852547645568848, 'Smatch ': 0.9834952354431152, 'Unlabeled ': 0.9917017817497253, 'max_indegree_sim': 0.9955113530158997, 'max_outdegree_sim': 0.9913824200630188, 'max_degree_sim': 0.9908438324928284, 'root_sim': 0.9872038960456848, 'quant_sim': 0.9909125566482544, 'score_wlk': 0.9934453964233398, 'score_wwlk': 0.9931788444519043, 'residual': 0.9923250079154968, 'sent_a': 'Maybe this could work for you.', 'sent_b': 'Maybe this can work for you.'}\n",
      "{'global': -0.03649453818798065, 'Concepts ': 0.16891063749790192, 'Frames ': 0.05984461307525635, 'Named Ent. ': -0.28500354290008545, 'Negations ': 0.25789815187454224, 'Reentrancies ': 0.6449844241142273, 'SRL ': 0.680191159248352, 'Smatch ': -0.3316713273525238, 'Unlabeled ': -0.4509808421134949, 'max_indegree_sim': 0.0841413289308548, 'max_outdegree_sim': -0.15690748393535614, 'max_degree_sim': 0.3467627763748169, 'root_sim': 0.12007661908864975, 'quant_sim': 0.3018682897090912, 'score_wlk': -0.08523499220609665, 'score_wwlk': 0.5829910039901733, 'residual': -0.04541495442390442, 'sent_a': 'It depends on what you want to do next, and where you want to do it.', 'sent_b': 'They can, but the way to do it depends on what you have available.'}\n",
      "{'global': 0.18300598859786987, 'Concepts ': 0.06939791887998581, 'Frames ': -0.05136001855134964, 'Named Ent. ': -0.031301505863666534, 'Negations ': 0.13953065872192383, 'Reentrancies ': 0.48019012808799744, 'SRL ': 0.13454607129096985, 'Smatch ': 0.09402114897966385, 'Unlabeled ': -0.20472262799739838, 'max_indegree_sim': -0.004810628481209278, 'max_outdegree_sim': -0.3534315824508667, 'max_degree_sim': -0.5500956773757935, 'root_sim': -0.08926459401845932, 'quant_sim': 0.31903085112571716, 'score_wlk': 0.4436700642108917, 'score_wwlk': 0.1708967238664627, 'residual': 0.20014123618602753, 'sent_a': 'Microwave would be your best bet.', 'sent_b': 'Your best bet is research.'}\n",
      "{'global': 0.7645172476768494, 'Concepts ': 0.7170167565345764, 'Frames ': 0.6882842183113098, 'Named Ent. ': 0.35557258129119873, 'Negations ': 0.8178241848945618, 'Reentrancies ': 0.24959228932857513, 'SRL ': 0.2755807042121887, 'Smatch ': 0.9036729335784912, 'Unlabeled ': 0.4409649074077606, 'max_indegree_sim': 0.5393029451370239, 'max_outdegree_sim': 0.7977901697158813, 'max_degree_sim': 0.7781485915184021, 'root_sim': 0.6868933439254761, 'quant_sim': 0.8161377906799316, 'score_wlk': 0.7794387936592102, 'score_wwlk': 0.44377201795578003, 'residual': 0.7798740863800049, 'sent_a': 'In the US, it will depend on the school.', 'sent_b': 'This will depend on the university.'}\n",
      "{'global': 0.5530236959457397, 'Concepts ': 0.7347639203071594, 'Frames ': 0.30651965737342834, 'Named Ent. ': 0.793518602848053, 'Negations ': 0.3941563069820404, 'Reentrancies ': 0.6996675729751587, 'SRL ': 0.4014374315738678, 'Smatch ': 0.11198649555444717, 'Unlabeled ': 0.12647536396980286, 'max_indegree_sim': 0.5877058506011963, 'max_outdegree_sim': 0.5807193517684937, 'max_degree_sim': 0.7076944708824158, 'root_sim': 0.3116326630115509, 'quant_sim': 0.7412125468254089, 'score_wlk': 0.6181687116622925, 'score_wwlk': 0.5462120175361633, 'residual': 0.5699833631515503, 'sent_a': \"It's not a good idea.\", 'sent_b': \"It's not just a good idea, it's an excellent idea.\"}\n",
      "{'global': 0.37837737798690796, 'Concepts ': -0.1222369521856308, 'Frames ': 0.4650925397872925, 'Named Ent. ': -0.09397876262664795, 'Negations ': 0.12495087832212448, 'Reentrancies ': 0.8629981279373169, 'SRL ': 0.6516798138618469, 'Smatch ': 0.3216962516307831, 'Unlabeled ': -0.2722829580307007, 'max_indegree_sim': 0.053098294883966446, 'max_outdegree_sim': 0.025269804522395134, 'max_degree_sim': 0.27265802025794983, 'root_sim': 0.2793588936328888, 'quant_sim': 0.7007036805152893, 'score_wlk': -0.017782052978873253, 'score_wwlk': 0.4998730421066284, 'residual': 0.3970290422439575, 'sent_a': 'There are a few things I think you should do.', 'sent_b': 'There are a few minimally-effective things you can do at the personal level.'}\n",
      "{'global': 0.6501200795173645, 'Concepts ': 0.8214541077613831, 'Frames ': 0.9122442007064819, 'Named Ent. ': 0.5058153867721558, 'Negations ': 0.6673277616500854, 'Reentrancies ': 0.7320658564567566, 'SRL ': 0.8616095185279846, 'Smatch ': 0.576416015625, 'Unlabeled ': 0.5526418685913086, 'max_indegree_sim': 0.6991360783576965, 'max_outdegree_sim': 0.7648787498474121, 'max_degree_sim': 0.5734126567840576, 'root_sim': 0.6883353590965271, 'quant_sim': 0.8136436343193054, 'score_wlk': 0.5924356579780579, 'score_wwlk': 0.5076354146003723, 'residual': 0.6466580033302307, 'sent_a': \"I've had this same problem.\", 'sent_b': 'I had this same situation.'}\n",
      "{'global': 0.8825774788856506, 'Concepts ': 0.8997637033462524, 'Frames ': 0.8755353093147278, 'Named Ent. ': 0.9217949509620667, 'Negations ': 0.8358340859413147, 'Reentrancies ': 0.935602605342865, 'SRL ': 0.6781595349311829, 'Smatch ': 0.9399789571762085, 'Unlabeled ': 0.9395707249641418, 'max_indegree_sim': 0.9127283096313477, 'max_outdegree_sim': 0.8746336102485657, 'max_degree_sim': 0.8949275612831116, 'root_sim': 0.8188421130180359, 'quant_sim': 0.9494390487670898, 'score_wlk': 0.8772057294845581, 'score_wwlk': 0.8326435089111328, 'residual': 0.8837074041366577, 'sent_a': 'This is not a good idea.', 'sent_b': 'Not a good idea.'}\n",
      "{'global': 0.6882152557373047, 'Concepts ': 0.7479327917098999, 'Frames ': 0.8235000967979431, 'Named Ent. ': 0.4961361885070801, 'Negations ': 0.32296279072761536, 'Reentrancies ': 0.7596419453620911, 'SRL ': 0.8400862812995911, 'Smatch ': 0.8253503441810608, 'Unlabeled ': 0.6771915555000305, 'max_indegree_sim': 0.8118795156478882, 'max_outdegree_sim': 0.24571077525615692, 'max_degree_sim': 0.34231987595558167, 'root_sim': 0.5621860027313232, 'quant_sim': 0.6892133951187134, 'score_wlk': 0.6797321438789368, 'score_wwlk': 0.3736984133720398, 'residual': 0.7013989686965942, 'sent_a': 'I realized there is already an accepted answer but I figure I would add my 2 cents.', 'sent_b': 'I know this is an old question but I feel I should add my 2 cents.'}\n",
      "{'global': 0.20205751061439514, 'Concepts ': 0.16080257296562195, 'Frames ': 0.21328875422477722, 'Named Ent. ': 0.05798020586371422, 'Negations ': 0.37037742137908936, 'Reentrancies ': 0.6678102016448975, 'SRL ': 0.5533959865570068, 'Smatch ': 0.6191537380218506, 'Unlabeled ': 0.480007141828537, 'max_indegree_sim': -0.02771110087633133, 'max_outdegree_sim': -0.26574569940567017, 'max_degree_sim': -0.32422831654548645, 'root_sim': 0.14750397205352783, 'quant_sim': 0.008757984265685081, 'score_wlk': 0.3458767831325531, 'score_wwlk': 0.2884354293346405, 'residual': 0.1981944590806961, 'sent_a': 'Yes, there is a reason to not do what you propose.', 'sent_b': 'Yes, there is a chemical difference.'}\n",
      "{'global': 0.21535886824131012, 'Concepts ': -0.08690635114908218, 'Frames ': 0.10386133939027786, 'Named Ent. ': 0.4640192687511444, 'Negations ': 0.3351248502731323, 'Reentrancies ': 0.851597785949707, 'SRL ': 0.23395821452140808, 'Smatch ': -0.21941491961479187, 'Unlabeled ': 0.40085750818252563, 'max_indegree_sim': -0.4035521447658539, 'max_outdegree_sim': -0.13655677437782288, 'max_degree_sim': 0.04346730560064316, 'root_sim': 0.47183752059936523, 'quant_sim': 0.13089431822299957, 'score_wlk': 0.06869668513536453, 'score_wwlk': -0.29819613695144653, 'residual': 0.22825276851654053, 'sent_a': 'Yes, there is at least one to my knowledge.', 'sent_b': \"To my knowledge, there's no time requirement.\"}\n",
      "{'global': 0.3903675675392151, 'Concepts ': 0.41907086968421936, 'Frames ': 0.336795836687088, 'Named Ent. ': 0.20288902521133423, 'Negations ': 0.628741979598999, 'Reentrancies ': 0.6692166924476624, 'SRL ': 0.4903002083301544, 'Smatch ': 0.7507779002189636, 'Unlabeled ': 0.8998032212257385, 'max_indegree_sim': -0.12003687024116516, 'max_outdegree_sim': 0.11122254282236099, 'max_degree_sim': -0.10170894116163254, 'root_sim': 0.02252272516489029, 'quant_sim': 0.6830235123634338, 'score_wlk': 0.5778413414955139, 'score_wwlk': 0.5692406892776489, 'residual': 0.36347654461860657, 'sent_a': \"You don't have to worry.\", 'sent_b': \"You don't have to do anything to season it.\"}\n",
      "{'global': 0.22100776433944702, 'Concepts ': 0.5613476037979126, 'Frames ': 0.5688413977622986, 'Named Ent. ': 0.5557283759117126, 'Negations ': 0.05560089647769928, 'Reentrancies ': 0.3780723810195923, 'SRL ': 0.3749772906303406, 'Smatch ': 0.7267472147941589, 'Unlabeled ': 0.5759825110435486, 'max_indegree_sim': -0.5252507328987122, 'max_outdegree_sim': 0.10338760912418365, 'max_degree_sim': -0.33934223651885986, 'root_sim': -0.04386580362915993, 'quant_sim': -0.18128883838653564, 'score_wlk': 0.4699251353740692, 'score_wwlk': 0.49524369835853577, 'residual': 0.1999529004096985, 'sent_a': 'There is no test that can tell you if it is sealed or not.', 'sent_b': 'There is no code telling you that you cannot do this.'}\n",
      "{'global': 0.4055621922016144, 'Concepts ': 0.38303691148757935, 'Frames ': 0.04081711918115616, 'Named Ent. ': 0.24076539278030396, 'Negations ': 0.46433743834495544, 'Reentrancies ': 0.9075725078582764, 'SRL ': 0.6308028101921082, 'Smatch ': 0.8706596493721008, 'Unlabeled ': 0.7367975115776062, 'max_indegree_sim': 0.5871606469154358, 'max_outdegree_sim': 0.7222910523414612, 'max_degree_sim': 0.39040011167526245, 'root_sim': 0.664461612701416, 'quant_sim': 0.5535802841186523, 'score_wlk': 0.7434015870094299, 'score_wwlk': 0.32937145233154297, 'residual': 0.37975308299064636, 'sent_a': 'Yes, you should make a rsum.', 'sent_b': 'Yes, you should talk to your professor.'}\n",
      "{'global': 0.4231815040111542, 'Concepts ': 0.35495418310165405, 'Frames ': 0.21411637961864471, 'Named Ent. ': 0.35833224654197693, 'Negations ': 0.27991268038749695, 'Reentrancies ': 0.6213586926460266, 'SRL ': -0.14315475523471832, 'Smatch ': 0.8112810254096985, 'Unlabeled ': 0.9736621379852295, 'max_indegree_sim': 0.0189236830919981, 'max_outdegree_sim': 0.10560797154903412, 'max_degree_sim': 0.29960814118385315, 'root_sim': -0.029542850330471992, 'quant_sim': 0.44447073340415955, 'score_wlk': 0.1650867462158203, 'score_wwlk': 0.309508740901947, 'residual': 0.4003964364528656, 'sent_a': \"You shouldn't feel any obligation at all.\", 'sent_b': \"You don't need sauce at all.\"}\n",
      "{'global': 0.8516717553138733, 'Concepts ': 0.8752612471580505, 'Frames ': 0.8126351833343506, 'Named Ent. ': 0.8004646897315979, 'Negations ': 0.7175056338310242, 'Reentrancies ': 0.6526407599449158, 'SRL ': 0.8365439176559448, 'Smatch ': 0.8418542146682739, 'Unlabeled ': 0.9337309002876282, 'max_indegree_sim': 0.568774402141571, 'max_outdegree_sim': 0.8937137722969055, 'max_degree_sim': 0.8379316329956055, 'root_sim': 0.7633234858512878, 'quant_sim': 0.8731722235679626, 'score_wlk': 0.740608811378479, 'score_wwlk': 0.8548123836517334, 'residual': 0.85892653465271, 'sent_a': \"I've had this same problem.\", 'sent_b': 'I had this problem before.'}\n",
      "{'global': 0.5602824091911316, 'Concepts ': 0.43083399534225464, 'Frames ': 0.6502780914306641, 'Named Ent. ': 0.5487979650497437, 'Negations ': 0.6240506172180176, 'Reentrancies ': 0.7740139365196228, 'SRL ': 0.511882483959198, 'Smatch ': 0.7310491800308228, 'Unlabeled ': 0.7995538115501404, 'max_indegree_sim': 0.65423184633255, 'max_outdegree_sim': 0.8084597587585449, 'max_degree_sim': 0.16081324219703674, 'root_sim': 0.7262200713157654, 'quant_sim': 0.8348730206489563, 'score_wlk': 0.6831331253051758, 'score_wwlk': 0.5380232930183411, 'residual': 0.5508834719657898, 'sent_a': 'You may have to experiment and find what you like.', 'sent_b': 'You have to find out what works for you.'}\n",
      "{'global': 0.26379916071891785, 'Concepts ': 0.2418302595615387, 'Frames ': 0.33232903480529785, 'Named Ent. ': 0.462554007768631, 'Negations ': 0.31553518772125244, 'Reentrancies ': 0.35742369294166565, 'SRL ': 0.33138248324394226, 'Smatch ': -0.07121429592370987, 'Unlabeled ': 0.07265884429216385, 'max_indegree_sim': 0.01730310544371605, 'max_outdegree_sim': 0.3054720163345337, 'max_degree_sim': -0.09376320242881775, 'root_sim': 0.11689534783363342, 'quant_sim': 0.30687811970710754, 'score_wlk': 0.5183352828025818, 'score_wwlk': 0.32254621386528015, 'residual': 0.2678650915622711, 'sent_a': 'My answer to your question is \"Probably Not\".', 'sent_b': 'This is a part answer to your question'}\n",
      "{'global': 0.5266047716140747, 'Concepts ': 0.48941606283187866, 'Frames ': 0.7932175397872925, 'Named Ent. ': 0.6509106159210205, 'Negations ': 0.5057197213172913, 'Reentrancies ': 0.8335936069488525, 'SRL ': 0.5893139243125916, 'Smatch ': 0.8437483310699463, 'Unlabeled ': 0.8462095856666565, 'max_indegree_sim': 0.669420599937439, 'max_outdegree_sim': 0.7268609404563904, 'max_degree_sim': 0.3647406995296478, 'root_sim': 0.41091471910476685, 'quant_sim': 0.42987778782844543, 'score_wlk': 0.6408568024635315, 'score_wwlk': 0.43790203332901, 'residual': 0.5146883130073547, 'sent_a': \"The key thing to realize here is that it's not your job to make people do their work.\", 'sent_b': \"It really isn't your problem how they are going to find a qualified person to do the job.\"}\n",
      "{'global': 0.650502622127533, 'Concepts ': 0.4600505232810974, 'Frames ': 0.7866522073745728, 'Named Ent. ': 0.5998592376708984, 'Negations ': 0.6885507106781006, 'Reentrancies ': 0.8955338001251221, 'SRL ': 0.7686814665794373, 'Smatch ': 0.5902789831161499, 'Unlabeled ': 0.7146698832511902, 'max_indegree_sim': 0.665721595287323, 'max_outdegree_sim': 0.3457590937614441, 'max_degree_sim': 0.6441957950592041, 'root_sim': 0.5914703607559204, 'quant_sim': 0.708134651184082, 'score_wlk': 0.592765748500824, 'score_wwlk': 0.7828915119171143, 'residual': 0.6522918939590454, 'sent_a': 'Not a direct answer to the question, but something to consider.', 'sent_b': 'Not a real answer to your question, but maybe it helps.'}\n",
      "{'global': 0.24423421919345856, 'Concepts ': 0.4397447407245636, 'Frames ': 0.2675166428089142, 'Named Ent. ': -0.431108683347702, 'Negations ': 0.3074232339859009, 'Reentrancies ': 0.3780040740966797, 'SRL ': 0.709104597568512, 'Smatch ': 0.5868178009986877, 'Unlabeled ': 0.6820673942565918, 'max_indegree_sim': 0.023184193298220634, 'max_outdegree_sim': 0.49738165736198425, 'max_degree_sim': 0.15635985136032104, 'root_sim': 0.46783939003944397, 'quant_sim': 0.41885069012641907, 'score_wlk': 0.08066485822200775, 'score_wwlk': 0.36081770062446594, 'residual': 0.24574673175811768, 'sent_a': 'There are two possible causes for this:', 'sent_b': 'There are two options for you - '}\n",
      "{'global': 0.965351939201355, 'Concepts ': 0.9713924527168274, 'Frames ': 0.957904040813446, 'Named Ent. ': 0.9636272192001343, 'Negations ': 0.9358769655227661, 'Reentrancies ': 0.8447844982147217, 'SRL ': 0.959341287612915, 'Smatch ': 0.9226288795471191, 'Unlabeled ': 0.9512688517570496, 'max_indegree_sim': 0.9617754817008972, 'max_outdegree_sim': 0.9605664014816284, 'max_degree_sim': 0.9619522094726562, 'root_sim': 0.9749107956886292, 'quant_sim': 0.9692466855049133, 'score_wlk': 0.9507141709327698, 'score_wwlk': 0.9581606388092041, 'residual': 0.966442346572876, 'sent_a': 'I agree with the other comments.', 'sent_b': 'I agree with the previous comments.'}\n",
      "{'global': 0.35177069902420044, 'Concepts ': 0.06693311035633087, 'Frames ': 0.21444696187973022, 'Named Ent. ': 0.39725276827812195, 'Negations ': 0.5293874144554138, 'Reentrancies ': 0.9071937203407288, 'SRL ': 0.5263577699661255, 'Smatch ': 0.5350849032402039, 'Unlabeled ': 0.17204825580120087, 'max_indegree_sim': 0.41175347566604614, 'max_outdegree_sim': 0.36828476190567017, 'max_degree_sim': 0.20852802693843842, 'root_sim': 0.13475947082042694, 'quant_sim': 0.48176124691963196, 'score_wlk': 0.22601622343063354, 'score_wwlk': 0.15763092041015625, 'residual': 0.35550162196159363, 'sent_a': 'The answer to both questions is: Yes.', 'sent_b': 'If the answer to any of these questions is yes, a PhD may be worthwhile for you.'}\n",
      "{'global': 0.935572624206543, 'Concepts ': 0.9297187924385071, 'Frames ': 0.9165916442871094, 'Named Ent. ': 0.9549862742424011, 'Negations ': 0.9390290379524231, 'Reentrancies ': 0.9648411273956299, 'SRL ': 0.9591937065124512, 'Smatch ': 0.7439221143722534, 'Unlabeled ': 0.9245806336402893, 'max_indegree_sim': 0.9083561301231384, 'max_outdegree_sim': 0.917834997177124, 'max_degree_sim': 0.9688223600387573, 'root_sim': 0.9423210620880127, 'quant_sim': 0.9574441909790039, 'score_wlk': 0.9810779094696045, 'score_wwlk': 0.9313594698905945, 'residual': 0.9353022575378418, 'sent_a': \"It rather depends on which country you're in.\", 'sent_b': 'That depends on which country you are in.'}\n",
      "{'global': 0.9421784281730652, 'Concepts ': 0.9610955119132996, 'Frames ': 0.8770940899848938, 'Named Ent. ': 0.957949697971344, 'Negations ': 0.9464544653892517, 'Reentrancies ': 0.9368573427200317, 'SRL ': 0.967170774936676, 'Smatch ': 0.9538962244987488, 'Unlabeled ': 0.9752015471458435, 'max_indegree_sim': 0.933343768119812, 'max_outdegree_sim': 0.9264349341392517, 'max_degree_sim': 0.940462052822113, 'root_sim': 0.8231738805770874, 'quant_sim': 0.966669499874115, 'score_wlk': 0.9475164413452148, 'score_wwlk': 0.846823513507843, 'residual': 0.9451962113380432, 'sent_a': 'This is a terrible idea.', 'sent_b': 'This is a bad idea.'}\n"
     ]
    }
   ],
   "source": [
    "# Print similarity scores of different features (optional)\n",
    "features = [\"global\"] + config.FEATURES[2:] + [\"residual\"]\n",
    "for i, sent_a in enumerate(sentences_a):\n",
    "    similarity = similarities[i]\n",
    "    features_text = {k:v for k,v in zip(features, similarity)}\n",
    "    features_text[\"sent_a\"] = sentences_a[i]\n",
    "    features_text[\"sent_b\"] = sentences_b[i]\n",
    "    print(features_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for unpaired (neuro) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used for neuroimaging sentence datasets, which have a list of single sentences. The code therefore computes the pairwise similarity between each unique pairing of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute similarities for neuro data\n",
    "\n",
    "# load S3BERT model\n",
    "model = SentenceTransformer(\"S3BERT_main\\s3bert_all-mpnet-base-v2\", device=\"cpu\")\n",
    "\n",
    "# define set of sentence pairs\n",
    "sim_storage = np.array([])\n",
    "sent_id_pairs = list(itertools.combinations(sentences_dict.keys(), 2))\n",
    "\n",
    "# encode with S3BERT and store results in numpy array\n",
    "for sent_id_pair in sent_id_pairs:\n",
    "    sentences_a_encoded = model.encode(sentences_dict[sent_id_pair[0]])\n",
    "    sentences_b_encoded = model.encode(sentences_dict[sent_id_pair[1]])\n",
    "    pair_sim = sep.cosine_sim(sentences_a_encoded,sentences_b_encoded)\n",
    "    sim_storage = np.append(sim_storage,pair_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save global S3BERT similarities\n",
    "dataset_name = dataset.split(' ')[1].split('\\\\')[0]\n",
    "np.savetxt(dataset_name+'_neuro_S3BERT_similarities.txt', sim_storage, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
