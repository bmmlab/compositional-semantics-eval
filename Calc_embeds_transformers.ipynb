{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56d77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script computes sentence embeddings for a list of sentences, using a wide range of models\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "#\n",
    "# This script requires the following models to be installed in order to work:\n",
    "#  1. ConceptNet: see here https://www.worldlink.com.cn/en/osdir/conceptnet-numberbatch.html\n",
    "#  2. ERNIE-base: see here https://github.com/nghuyong/ERNIE-Pytorch\n",
    "#  3. ERNIE-large: see here https://huggingface.co/nghuyong/ernie-2.0-large-en/tree/main\n",
    "#  4. SentBERT: see here https://www.sbert.net/\n",
    "#  5. InferSent: see here https://github.com/facebookresearch/InferSent\n",
    "#  6. DictBERT: see here https://huggingface.co/wyu1/DictBERT/tree/main\n",
    "#  7. Universal Sencence Encoder: install with 'conda install tensorflow'\n",
    "#  8. AMRBART: see here https://huggingface.co/xfbai/AMRBART-large\n",
    "#  9. DefSent: see here https://github.com/hppRC/defsent\n",
    "# 10. OpenAI Embeddings: see docs here https://beta.openai.com/docs/guides/embeddings\n",
    "#\n",
    "# The basic workflow for running this script is as follows:\n",
    "# 1. Download all the required models and update paths\n",
    "# 2. Prepare text file of sentences to get embeddings for\n",
    "# 3. Run the script to compute and save the embeddings\n",
    "#\n",
    "# The script generates one file per model or model variant, each line of which contains the sentence\n",
    "# embedding for one sentence from the input list of sentences.\n",
    "# \n",
    "# OpenAI Embeddings require a subscription to the openai api. \n",
    "# The key and organisation id should be stored, one value per line, in a file called 'openai_key.txt' in the same directory as this notebook.\n",
    "\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "import torch\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "# load file paths\n",
    "with open(\"file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')\n",
    "sns.set()\n",
    "\n",
    "# stop words\n",
    "stop_words = np.loadtxt(file_paths_dict['stop_words_path'], dtype='str') # list of stop words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cba17b5",
   "metadata": {},
   "source": [
    "### Define key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38a5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to compute sentence embeddings\n",
    "\n",
    "# Calculate sentence embeddings by arithmetic operation over word token embeddings\n",
    "def get_arithmetic_embedding(word_embeddings, sentences, function_name='mean', stopwords=stop_words, weights=(1,1,1)):\n",
    "    \n",
    "    # create empty array\n",
    "    sentence_embeddings = np.empty((0,len(word_embeddings['man'])), float)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # get tokens for each word\n",
    "        token_list = sep.tokenise_sentence(sentence, stop_words)\n",
    "        token_embeds_matrix = sep.get_token_embeds(word_embeddings, WordNetLemmatizer, token_list)\n",
    "        embed_dim = token_embeds_matrix.shape[1]\n",
    "        \n",
    "        # if there is only one word with embeddings\n",
    "        if token_embeds_matrix.shape[0]==1: \n",
    "            sentence_embedding = token_embeds_matrix[0]\n",
    "        \n",
    "        # elementwise addition\n",
    "        elif function_name=='mean': \n",
    "            sentence_embedding = token_embeds_matrix.mean(axis=0)\n",
    "            \n",
    "        # weighted mean; only works for three word sentences\n",
    "        elif function_name=='wgtd_mean': \n",
    "            sentence_embedding = np.average(token_embeds_matrix,axis=0,weights=weights) \n",
    "        \n",
    "        # elementwise multiplication\n",
    "        elif function_name=='mult': \n",
    "            sentence_embedding = np.ones(embed_dim) # start with all ones\n",
    "            for word_embedding in token_embeds_matrix:\n",
    "                sentence_embedding = np.multiply(sentence_embedding, word_embedding)\n",
    "            sentence_embedding=1000*sentence_embedding\n",
    "        \n",
    "        # circular convolution\n",
    "        elif function_name=='conv': \n",
    "            sentence_embedding = convolve(token_embeds_matrix[0], token_embeds_matrix[1], mode='wrap') # convolve first to embeds\n",
    "            if token_embeds_matrix.shape[0]>2:\n",
    "                for word_embedding in token_embeds_matrix[2:]: # convolve the rest of the embeds\n",
    "                    sentence_embedding = convolve(sentence_embedding, word_embedding, mode='wrap')\n",
    "\n",
    "        # store embedding in array\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "        \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using sentbert\n",
    "def get_sentbert_embedding(sentbert_model, sentences):\n",
    "    sentence_embeddings = sentbert_model.encode(sentences, convert_to_tensor=True)    \n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using ERNIE-base\n",
    "def get_ernie_embedding(ernie_tokenizer, ernie_model, layer, sentences):\n",
    "    sentence_embeddings = np.empty((0,768), float)\n",
    "    for sentence in sentences:\n",
    "        encoded_input = ernie_tokenizer(sentence, return_tensors='pt')\n",
    "        model_output = ernie_model(**encoded_input)\n",
    "        sentence_embedding = model_output.hidden_states[layer].detach().numpy()[0].mean(axis=0)\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using ERNIE-large\n",
    "def get_ernie_large_embedding(ernie_large_tokenizer, ernie_large_model, layer, sentences):\n",
    "    sentence_embeddings = np.empty((0,1024), float)\n",
    "    for sentence in sentences:\n",
    "        encoded_input = ernie_large_tokenizer(sentence, return_tensors='pt')\n",
    "        model_output = ernie_large_model(**encoded_input)\n",
    "        sentence_embedding = model_output.hidden_states[layer].detach().numpy()[0].mean(axis=0)\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using universal sentence embeddings\n",
    "def get_universal_embedding(universal_model, sentences):\n",
    "    sentence_embeddings = universal_model(sentences).numpy()\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using amrbart\n",
    "def get_amrbart_embedding(amrbart_tokenizer, amrbart_model, sentences):\n",
    "    sentence_embeddings = np.empty((0,1024), float)\n",
    "    for sentence in sentences:\n",
    "        encoded_input = amrbart_tokenizer(sentence, return_tensors='pt')\n",
    "        model_output = amrbart_model(**encoded_input)\n",
    "        sentence_embedding = np.mean(model_output.encoder_last_hidden_state.detach().numpy()[0], axis=0)\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using infersent\n",
    "def get_infersent_embedding(infersent_embeds, infersent_path, sentences):\n",
    "    infersent_embeds.set_w2v_path(infersent_path) # load word embeddings for infersent\n",
    "    infersent_embeds.build_vocab(sentences, tokenize=True) # create vocab of just words used\n",
    "    sentence_embeddings = infersent_embeds.encode(sentences, tokenize=True)\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using defsent_mean\n",
    "def get_defsent_mean_embedding(defsent_mean_model, sentence):\n",
    "    embedding_model = defsent_mean_model.encode\n",
    "    sentence_embeddings = embedding_model(sentence)\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using dictbert\n",
    "def get_dictbert_embedding(dictbert_tokenizer, dictbert_model, layer, sentences):\n",
    "    sentence_embeddings = np.empty((0,768), float)\n",
    "    for sentence in sentences:\n",
    "        encoded_input = dictbert_tokenizer(sentence, return_tensors='pt')\n",
    "        model_output = dictbert_model(**encoded_input)\n",
    "        sentence_embedding = model_output.hidden_states[layer].detach().numpy()[0].mean(axis=0)\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using defsent_mean\n",
    "def get_defsent_cls_embedding(defsent_cls_model, sentence):\n",
    "    embedding_model = defsent_cls_model.encode\n",
    "    sentence_embeddings = defsent_cls_model.encode(sentence)\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Calculate sentence embedding using open_ai embeddings\n",
    "def get_open_ai_embedding(sentences):\n",
    "    sentence_embeddings = np.empty((0,1536), float)\n",
    "    for sentence in sentences:\n",
    "        raw_sentence_embeddings = openai.Embedding.create(input=sentence, model=\"text-embedding-ada-002\") # get using API\n",
    "        sentence_embedding = np.array(raw_sentence_embeddings['data'][0]['embedding'])\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34726968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\Sentence Encoders\\ernie-2.0-base-en were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\Sentence Encoders\\dictbert were not used when initializing BertModel: ['clr.predictions.transform.dense.weight', 'clr.predictions.transform.LayerNorm.bias', 'clr.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'clr.predictions.decoder.weight', 'cls.predictions.decoder.weight', 'clr.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'clr.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\Sentence Encoders\\dictbert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Load all the needed transformer models\n",
    "\n",
    "# Load ConceptNet embeddings and nltk lemmatizer\n",
    "from nltk.data import path # need to specify the location of the nltk data\n",
    "path.append(file_paths_dict['path_root']+\"\\Frames and Structured Data\\\\FrameNet\\\\nltk_data\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "model_address = file_paths_dict['path_root']+'\\Word Embeddings\\ConceptNet Embeddings\\\\numberbatch-en.txt'\n",
    "conceptnet_embeds = sep.import_word_model(model_address)\n",
    "\n",
    "# Load ERNIE-base model\n",
    "ernie_address = file_paths_dict['path_root']+'\\Sentence Encoders\\ernie-2.0-base-en'\n",
    "config_state = AutoConfig.from_pretrained(ernie_address, output_hidden_states=True) # get hidden states\n",
    "ernie_tokenizer = AutoTokenizer.from_pretrained(ernie_address)\n",
    "ernie_model = AutoModel.from_pretrained(ernie_address, config=config_state)\n",
    "\n",
    "# Load ERNIE-large model\n",
    "ernie_large_address = file_paths_dict['path_root']+'\\Sentence Encoders\\ernie-2.0-large-en'\n",
    "config_state = AutoConfig.from_pretrained(ernie_large_address, output_hidden_states=True) # get hidden states\n",
    "ernie_large_tokenizer = AutoTokenizer.from_pretrained(ernie_large_address)\n",
    "ernie_large_model = AutoModel.from_pretrained(ernie_large_address, config=config_state)\n",
    "\n",
    "# Load SentBERT model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentbert_model_mpnet = SentenceTransformer(file_paths_dict['path_root']+'\\Sentence Encoders\\sentence-transformers-mpnet-base-v2')\n",
    "sentbert_model = SentenceTransformer(file_paths_dict['path_root']+'\\Sentence Encoders\\sentence-transformers-MiniLM-L6-v2')\n",
    "\n",
    "# Load InferSent embeddings\n",
    "module_path = file_paths_dict['path_root']+'\\Sentence Encoders\\infersent'\n",
    "sys.path.insert(0, module_path) # add model location to path\n",
    "from models import InferSent \n",
    "infersent_embeds = InferSent({'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048, 'pool_type': 'max', 'dpout_model': 0.0, 'version': 1})\n",
    "infersent_embeds.load_state_dict(torch.load(module_path+'\\infersent1.pkl'))\n",
    "infersent_path = file_paths_dict['path_root']+'\\Word Embeddings\\Glove Word Embeddings\\glove.840B.300d.txt'\n",
    "\n",
    "# Load DictBERT model\n",
    "dictbert_address = file_paths_dict['path_root']+'\\Sentence Encoders\\dictbert'\n",
    "config_state = AutoConfig.from_pretrained(dictbert_address, output_hidden_states=True) # get hidden states\n",
    "dictbert_tokenizer = AutoTokenizer.from_pretrained(dictbert_address)\n",
    "dictbert_model = AutoModel.from_pretrained(dictbert_address, config=config_state)\n",
    "\n",
    "# Load Universal Sentence Encoder embeddings\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from absl import logging # (use pip install absl-py)\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "universal_model = hub.load(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\universal-sentence-encoder')\n",
    "\n",
    "# Load AMRBART model\n",
    "from transformers import BartForConditionalGeneration\n",
    "config_state = AutoConfig.from_pretrained(file_paths_dict['path_root']+'\\Sentence Encoders\\\\amrbart-large', output_hidden_states=True) # get hidden states\n",
    "amrbart_tokenizer = AutoTokenizer.from_pretrained(file_paths_dict['path_root']+'\\Sentence Encoders\\\\amrbart-large', collapse_name_ops=False, use_pointer_tokens=True, raw_graph=False)\n",
    "amrbart_model = BartForConditionalGeneration.from_pretrained(file_paths_dict['path_root']+'\\Sentence Encoders\\\\amrbart-large', config=config_state)\n",
    "\n",
    "# Load DefSent models\n",
    "from defsent import DefSent\n",
    "defsent_mean_model = DefSent(file_paths_dict['path_root']+'\\Sentence Encoders\\\\defsent-roberta-large-mean')\n",
    "defsent_cls_model = DefSent(file_paths_dict['path_root']+'\\Sentence Encoders\\\\defsent-roberta-large-cls')\n",
    "\n",
    "# Prepare API for openai embeddings (needs a key to work)\n",
    "import openai\n",
    "api_key, api_org = sep.load_openai_key('openai_key.txt')\n",
    "openai.organization = api_org\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdc3ad93",
   "metadata": {},
   "source": [
    "### Load sentence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3280faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 2014 Wehbe\\Stimuli\\Chapter_9_sentences_final\n",
      "1 2017 Anderson\\Stimuli\\stimuli_final\n",
      "2 2018 Pereira\\Stimuli\\stimuli_243sentences\n",
      "3 2018 Pereira\\Stimuli\\stimuli_384sentences\n",
      "4 2020 Alice Dataset\\Stimuli\\stimuli_sentences_final\n",
      "5 2020 Zhang\\Stimuli\\test_sentences_final\n",
      "6 2023 Fodor Dataset\\Stimuli\\Fodor2023-final240\n",
      "7 2023 Fodor Dataset\\Stimuli\\Fodor2023-final192\n",
      "8 2023 Fodor Dataset\\Stimuli\\Fodor2023-final96\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd77e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded 2023 Fodor Dataset\\Stimuli\\Fodor2023-final96 with 96 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set (choose number from those printed above)\n",
    "dataset = datasets[8]\n",
    "sentences_dict = sep.load_set_of_sentences(dataset, file_paths_dict['data_pairs_path'], file_paths_dict['data_nonpaired_path'], pairs)\n",
    "full_dataset_name = sep.fix_sentence_dataset_name(dataset)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset,'with',n,'sentences')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b4c503e",
   "metadata": {},
   "source": [
    "### Compute arithmetic and transformer sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d998ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 275(/275) words with w2v vectors\n",
      "Vocab size : 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Study and Projects\\School Work\\Year 25 - PhD\\Data\\\\Sentence Encoders\\infersent\\models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sentences = np.array(sentences)[idx_sort]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean (96, 299)\n",
      "mult (96, 299)\n",
      "conv (96, 299)\n",
      "ernie_0 (96, 768)\n",
      "ernie_5 (96, 768)\n",
      "ernie_12 (96, 768)\n",
      "ernie_large_12 (96, 1024)\n",
      "sentbert (96, 384)\n",
      "sentbert_mpnet (96, 768)\n",
      "universal (96, 512)\n",
      "amrbart (96, 1024)\n",
      "infersent (96, 4096)\n",
      "defsent_mean (96, 1024)\n",
      "defsent_cls (96, 1024)\n",
      "dictBERT (96, 768)\n",
      "openai (96, 1536)\n"
     ]
    }
   ],
   "source": [
    "## Compute embeddings for all sentences in set\n",
    "\n",
    "# select format of sentences needed depending on the dataset type\n",
    "sentences = []\n",
    "if pairs==True: # use this for sentence similarity pair data\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,0].flatten()))\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,1].flatten()))\n",
    "else: # use this for neuroimaging data/list of sentences\n",
    "    sentences.append(list(sentences_dict.values()))\n",
    "\n",
    "# loop over all types of sentence embeddings\n",
    "set_id = 'a' # set_id specifies whether we are doing the first or second sentence in the pair\n",
    "for sentence_set in sentences:\n",
    "    \n",
    "    # specify the embeddings we want to compute\n",
    "    embeds = {}\n",
    "    embeds['mean'] = get_arithmetic_embedding(conceptnet_embeds, sentence_set, 'mean', stop_words)\n",
    "    embeds['mult'] = get_arithmetic_embedding(conceptnet_embeds, sentence_set, 'mult', stop_words)\n",
    "    embeds['conv'] = get_arithmetic_embedding(conceptnet_embeds, sentence_set, 'conv', stop_words)\n",
    "    embeds['ernie_0'] = get_ernie_embedding(ernie_tokenizer, ernie_model, 0, sentence_set)\n",
    "    embeds['ernie_5'] = get_ernie_embedding(ernie_tokenizer, ernie_model, 5, sentence_set)\n",
    "    embeds['ernie_12'] = get_ernie_embedding(ernie_tokenizer, ernie_model, 12, sentence_set)\n",
    "    embeds['ernie_large_12'] = get_ernie_large_embedding(ernie_large_tokenizer, ernie_large_model, 24, sentence_set)\n",
    "    embeds['sentbert'] = get_sentbert_embedding(sentbert_model, sentence_set)\n",
    "    embeds['sentbert_mpnet'] = get_sentbert_embedding(sentbert_model_mpnet, sentence_set)\n",
    "    embeds['universal'] = get_universal_embedding(universal_model, sentence_set)\n",
    "    embeds['amrbart'] = get_amrbart_embedding(amrbart_tokenizer, amrbart_model, sentence_set)\n",
    "    embeds['infersent'] = get_infersent_embedding(infersent_embeds, infersent_path, sentence_set)\n",
    "    embeds['defsent_mean'] = get_defsent_mean_embedding(defsent_mean_model, sentence_set)\n",
    "    embeds['defsent_cls'] = get_defsent_cls_embedding(defsent_cls_model, sentence_set)\n",
    "    embeds['dictBERT'] = get_dictbert_embedding(dictbert_tokenizer, dictbert_model, 12, sentence_set)\n",
    "    embeds['openai'] = get_open_ai_embedding(sentence_set)\n",
    "\n",
    "    # print embedding sizes as a check\n",
    "    for model in embeds.keys():\n",
    "        print(model,embeds[model].shape)\n",
    "\n",
    "    # save file\n",
    "    # for model in embeds.keys():\n",
    "    #     np.savetxt(full_dataset_name+'_'+model+'_embeddings.txt', embeds[model], fmt='%f') # string formatting\n",
    "\n",
    "    # second set of sentences in pairwise sentence sets\n",
    "    set_id = 'b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a5169",
   "metadata": {},
   "source": [
    "### Legacy code (please ignore!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ecc6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7845945320670508"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_pairs = []\n",
    "# for i in np.arange(0,34):\n",
    "#     all_pairs.append((0,i))\n",
    "# set_a = np.arange(0,18)\n",
    "# set_b = np.arange(18,35)\n",
    "# all_pairs = list(itertools.product(set_a, set_b))\n",
    "\n",
    "all_pairs = list(itertools.combinations(np.arange(34), 2))\n",
    "    \n",
    "pair_corrs = {}\n",
    "for model in models:\n",
    "    pair_corrs[model] = []\n",
    "    for pair in all_pairs:\n",
    "        sent_1 = norm_embeds[model][pair[0]]\n",
    "        sent_2 = norm_embeds[model][pair[1]]\n",
    "        pair_corrs[model].append(sep.cosine_sim(sent_1,sent_2))\n",
    "        \n",
    "np.corrcoef(pair_corrs['mean'],pair_corrs['defsent_mean'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b538be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation coefficient matrix between sentences\n",
    "# xs = np.arange(norm_embeds[model].shape[0])\n",
    "# num_string = ['{:3d}'.format(x+1) for x in xs]\n",
    "# print('      ',num_string)\n",
    "# np.corrcoef(embeds[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2572938f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08528654188993806"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_1 = ['bless']\n",
    "phrase_2 = ['care']\n",
    "embed_1 = get_arithmetic_embedding(conceptnet_embeds, phrase_1, 'mean', stop_words)\n",
    "embed_2 = get_arithmetic_embedding(conceptnet_embeds, phrase_2, 'mean', stop_words)\n",
    "sep.cosine_sim(embed_1[0],embed_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe203ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
