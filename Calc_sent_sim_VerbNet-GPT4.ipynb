{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56d77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script computes VerbNet pairwise sentence similarities\n",
    "# Requires that sentences have already been parsed using VerbNet semantic roles, as performed by 'Calc_parse_GPT4.ipynb'.\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "\n",
    "# load libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import itertools\n",
    "import sentence_embeds_processing as sep\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load file paths\n",
    "with open(\"file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)\n",
    "\n",
    "# set fixed params\n",
    "role_weights_fixed_dict = {'Agent':2, 'Verb':3, 'Patient':2, 'Theme':2, 'Time':0.5, 'Manner':0.5, 'Location':0.5, 'Trajectory':0.5}\n",
    "# these are from the test-train regression using STS3k data\n",
    "role_weights_ST3k_dict = {'Agent':0.22, 'Verb':0.19, 'Patient':0.13, 'Theme':0.14, 'Time':0.03, 'Manner':0.03, 'Location':0.12, 'Trajectory':0.10}\n",
    "role_weights_dict_of_dicts = {'fixed':role_weights_fixed_dict, 'STS3k':role_weights_ST3k_dict, 'struct':role_weights_fixed_dict}\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=4, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')\n",
    "sns.set()\n",
    "\n",
    "# turn off annoying warning\n",
    "from tqdm import TqdmExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmExperimentalWarning)\n",
    "\n",
    "# define stop words\n",
    "stop_words = np.loadtxt(file_paths_dict['stop_words_path'], dtype='str') # list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870fa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to compute VerbNet rolewise similarities \n",
    "\n",
    "# Compute embedding for phrases from the verbnet parse using conceptnet\n",
    "def get_conceptnet_embedding(conceptnet_embeds, phrase):\n",
    "    \n",
    "    # Split phrase into words and define storage\n",
    "    word_list = sep.tokenise_sentence(phrase, stop_words)\n",
    "    word_list = sep.replace_tricky_tokens(word_list)\n",
    "    embed_length = len(conceptnet_embeds['man'])\n",
    "    embeds = np.empty((0,embed_length), float)\n",
    "    \n",
    "    # Get word embeddings for each word in phrase\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            embeds = np.vstack([embeds,conceptnet_embeds[word.lower()]])\n",
    "        except KeyError:\n",
    "            continue # ignore words we can't find\n",
    "    \n",
    "    # Save final phrase embeddings\n",
    "    if len(np.ndarray.flatten(embeds)) > embed_length: # deal with phrases with multiple embeddings\n",
    "        phrase_embedding = np.mean(embeds, axis=0) # average over all saved embeddings\n",
    "    else:\n",
    "        try:\n",
    "            phrase_embedding = embeds[0] # use embeddings for single word\n",
    "        except IndexError:\n",
    "            phrase_embedding = np.zeros(embed_length) # return array of zeros if no words in phrase found\n",
    "    \n",
    "    return phrase_embedding\n",
    "\n",
    "# Calculate sentence embedding using sentbert\n",
    "def get_sentbert_embedding(sentbert_model_mpnet, sentences):\n",
    "    sentence_embeddings = sentbert_model_mpnet.encode(sentences, convert_to_tensor=True).reshape(-1)\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "# Calculate sentence embedding using defsent_cls\n",
    "def get_defsent_cls_embedding(defsent_cls_model, sentence):\n",
    "    embedding_model = defsent_cls_model.encode\n",
    "    sentence_embeddings = defsent_cls_model.encode(sentence).reshape(-1)\n",
    "    return np.array(sentence_embeddings)\n",
    "\n",
    "# Calculate sentence embedding using amrbart\n",
    "def get_amrbart_embedding(amrbart_model, amrbart_tokenizer, sentences):\n",
    "    sentence_embeddings = np.empty((0,1024), float)\n",
    "    for sentence in sentences:\n",
    "        encoded_input = amrbart_tokenizer(sentence, return_tensors='pt')\n",
    "        model_output = amrbart_model(**encoded_input)\n",
    "        sentence_embedding = np.mean(model_output.encoder_last_hidden_state.detach().numpy()[0], axis=0)\n",
    "        sentence_embeddings = np.vstack([sentence_embeddings, sentence_embedding])\n",
    "    return sentence_embeddings[0]\n",
    "\n",
    "# Get the embedding for a single phrase\n",
    "def get_phrase_embedding(phrase, embed_type, embed_model):\n",
    "    if embed_type=='conceptnet': # get embedding for phrase\n",
    "        phrase_embed = get_conceptnet_embedding(embed_model, phrase)\n",
    "    elif embed_type=='sentbert':\n",
    "        phrase_embed = get_sentbert_embedding(embed_model, phrase)\n",
    "    elif embed_type=='defsent_cls':\n",
    "        phrase_embed = get_defsent_cls_embedding(embed_model, phrase)\n",
    "    elif embed_type=='amrbart':\n",
    "        phrase_embed = get_amrbart_embedding(embed_model[0], embed_model[1], phrase)\n",
    "    return phrase_embed\n",
    "\n",
    "# Get mean and std dev for a set of embeddings to use for normalisation\n",
    "def get_all_rolewise_embeddings(sentences_parsed_dict, model_name, embed_model):\n",
    "\n",
    "    # Embeddings length\n",
    "    embed_length = get_phrase_embedding('man', model_name, embed_model).shape[0]\n",
    "\n",
    "    # Get embeddings for each semantic role of each sentence\n",
    "    all_embeds = []\n",
    "    for sent_id in sentences_parsed_dict.keys():\n",
    "        single_sentence_embed = []\n",
    "        for sem_role in sentences_parsed_dict[sent_id][0].keys():\n",
    "            sem_role_text = sentences_parsed_dict[sent_id][0][sem_role]\n",
    "            if (sem_role_text != 'NONE'): # only if the corresponding semantic role is present\n",
    "                single_sentence_embed.append(get_phrase_embedding(sem_role_text, model_name, embed_model))\n",
    "            else: # if semantic role not present\n",
    "                single_sentence_embed.append([np.NaN for x in np.arange(embed_length)]) # fill with nans\n",
    "        all_embeds.append(single_sentence_embed)\n",
    "    all_embeds = np.array(all_embeds)\n",
    "\n",
    "    # Normalise embeddings\n",
    "    all_embeds_flat = all_embeds.reshape(-1,embed_length) # combine embeddings of all roles and all sentences\n",
    "    mean_embeds = np.nanmean(all_embeds_flat, axis=0) # mean for each dimension\n",
    "    std_embeds = np.nanstd(all_embeds_flat, axis=0)+0.001 # std for each dimension\n",
    "    all_embeds_flat_norml = (all_embeds_flat-mean_embeds)/std_embeds\n",
    "    all_embeds_norml = all_embeds_flat_norml.reshape(all_embeds.shape) # put embeds back into structured form\n",
    "    \n",
    "    return all_embeds_norml\n",
    "\n",
    "# Compute rolewise similarities for a pair of sentences\n",
    "def calc_rolewise_sims(sent_id_pair, all_embeds_norml, sentences_parsed_dict, use_role_align, role_weights):\n",
    "    \n",
    "    # Define dictionary of pairs of semantic roles to align if no direct matches are present\n",
    "    role_alignment_dict = [('Location','Trajectory'), ('Trajectory','Location'), ('Theme','Patient'), ('Patient','Theme'), ('Manner','Trajectory'), ('Trajectory','Manner')]\n",
    "\n",
    "    # Get the sentence pair to compute similarity for\n",
    "    sent_1_id = int(sent_id_pair[0])\n",
    "    sent_2_id = int(sent_id_pair[1])\n",
    "    sent_1_parsed = sentences_parsed_dict[str(sent_1_id)][0]\n",
    "    sent_2_parsed = sentences_parsed_dict[str(sent_2_id)][0]\n",
    "\n",
    "    # Define role sims storage dict\n",
    "    role_sim_storage = {}\n",
    "    \n",
    "    # Loop over all semantic roles\n",
    "    for role_id,role in enumerate(list(role_weights_fixed_dict.keys())):\n",
    "        \n",
    "        # If neither sentence has the role\n",
    "        if sent_1_parsed[role]=='NONE' and sent_2_parsed[role]=='NONE':\n",
    "            role_sim_storage[role] = np.NaN\n",
    "            \n",
    "        # If both sentences have the role\n",
    "        elif sent_1_parsed[role]!='NONE' and sent_2_parsed[role]!='NONE':\n",
    "            phrase_1_embed = all_embeds_norml[sent_1_id][role_id] # get role embeddings\n",
    "            phrase_2_embed = all_embeds_norml[sent_2_id][role_id]\n",
    "            role_sim_storage[role] = float(round(sep.cosine_sim(phrase_1_embed, phrase_2_embed),4))\n",
    "            if role_weights=='struct':\n",
    "                role_sim_storage[role]+=0.5\n",
    "            \n",
    "        # If one sentence has the role but the other does not\n",
    "        else:\n",
    "            role_sim_storage[role] = 0\n",
    "            \n",
    "    # Match related but distinct roles when there is no direct match\n",
    "    if use_role_align==True:\n",
    "        for role_match_1,role_match_2 in role_alignment_dict:\n",
    "            if (sent_1_parsed[role_match_1]!='NONE' and sent_1_parsed[role_match_2]=='NONE') and (sent_2_parsed[role_match_1]=='NONE' and sent_2_parsed[role_match_2]!='NONE'):\n",
    "                phrase_1_embed = all_embeds_norml[sent_1_id][role_id] # get role embeddings\n",
    "                phrase_2_embed = all_embeds_norml[sent_2_id][role_id]\n",
    "                role_sim_storage[role_match_1] = round(sep.cosine_sim(phrase_1_embed, phrase_2_embed),4)\n",
    "                role_sim_storage[role_match_2] = np.NaN\n",
    "                \n",
    "    return role_sim_storage\n",
    "\n",
    "\n",
    "# Function to convert role-wise dictionary sim storage into role-wise list storage\n",
    "def dict_sim_to_list(role_weights_dict, sim_storage_dict):\n",
    "    sim_storage_dict_full = {}\n",
    "    for role in role_weights_dict.keys():\n",
    "        try:\n",
    "            sim_storage_dict_full[role] = sim_storage_dict[role]\n",
    "        except KeyError:\n",
    "            sim_storage_dict_full[role] = 0\n",
    "    sim_storage_list = list(sim_storage_dict_full.values())\n",
    "    \n",
    "    return(sim_storage_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730e0c",
   "metadata": {},
   "source": [
    "### Step 0: Load sentence dataset and parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f4342e",
   "metadata": {},
   "source": [
    "Load the dataset of sentences we want to compute VerbNet similarities for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3280faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Wehbe_neuro\n",
      "1 Anderson_neuro\n",
      "2 Pereira243_neuro\n",
      "3 Pereira384_neuro\n",
      "4 Alice_neuro\n",
      "5 Zhang_neuro\n",
      "6 Fodor2024-final108_neuro\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sentence_embeds_processing module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e9949c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded Zhang_neuro with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set into dictionary depending on type\n",
    "dataset_name = datasets[5]\n",
    "if pairs == True:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['data_pairs_path'], pairs)\n",
    "else:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict[dataset_name+'-stim'], pairs)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset_name,'with',n,'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c513778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse type: GPT4\n",
      "sentences parsed: 95\n"
     ]
    }
   ],
   "source": [
    "## load parse data for the dataset\n",
    "parse_type = 'GPT4' # choose parse type ('verbnet' or 'GPT4' or 'GPT4o)\n",
    "json_path = file_paths_dict['parses_path']+parse_type+' Parsing\\\\'+dataset_name+'_parse-'+parse_type+'_mod.json'\n",
    "sentences_parsed_dict = json.load(open(json_path)) # load verbnet parse data from json\n",
    "sent_id_pairs = list(itertools.combinations(sentences_parsed_dict.keys(), 2))\n",
    "n = len(sentences_parsed_dict.keys())\n",
    "print('parse type:',parse_type)\n",
    "print('sentences parsed:',n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cba17b5",
   "metadata": {},
   "source": [
    "### Step 1: Load embeddings models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "17823436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conceptnet embeds: 300\n",
      "Sentbert embeds: 768\n",
      "Defsent embeds: 1024\n"
     ]
    }
   ],
   "source": [
    "## Load embeddings and transformer models for computing similarity\n",
    "\n",
    "# Load ConceptNet embeddings\n",
    "model_address = file_paths_dict['path_root']+'\\\\Word Embeddings\\\\ConceptNet Embeddings\\\\numberbatch-en.txt'\n",
    "conceptnet_embeds = sep.import_word_model(model_address)\n",
    "embed_length = get_conceptnet_embedding(conceptnet_embeds, 'man').shape[0]\n",
    "print('Conceptnet embeds:',embed_length)\n",
    "\n",
    "# Load SentBERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentbert_model_mpnet = SentenceTransformer(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\sentence-transformers-mpnet-base-v2')\n",
    "embed_length = get_sentbert_embedding(sentbert_model_mpnet, ['man']).shape[0]\n",
    "print('Sentbert embeds:',embed_length)\n",
    "\n",
    "# Load DefSent models\n",
    "from defsent import DefSent\n",
    "defsent_cls_model = DefSent(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\defsent-roberta-large-cls')\n",
    "embed_length = get_defsent_cls_embedding(defsent_cls_model, ['man']).shape[0]\n",
    "print('Defsent embeds:',embed_length)\n",
    "\n",
    "# Load AMRBART model\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BartForConditionalGeneration\n",
    "config_state = AutoConfig.from_pretrained(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\amrbart-large', output_hidden_states=True) # get hidden states\n",
    "amrbart_tokenizer = AutoTokenizer.from_pretrained(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\amrbart-large', collapse_name_ops=False, use_pointer_tokens=True, raw_graph=False)\n",
    "amrbart_model = BartForConditionalGeneration.from_pretrained(file_paths_dict['path_root']+'\\\\Sentence Encoders\\\\amrbart-large', config=config_state)\n",
    "\n",
    "# Dictionary of all embedding models\n",
    "dict_of_models = {'conceptnet':conceptnet_embeds, 'sentbert':sentbert_model_mpnet, 'defsent_cls':defsent_cls_model, 'amrbart':[amrbart_model,amrbart_tokenizer]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "875076f2",
   "metadata": {},
   "source": [
    "### Step 2: Compute rolewise similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "821157d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute role-wise sentence similarities using VerbNet semantic role parses\n",
    "\n",
    "# Initialise storage \n",
    "all_role_sim_storage = {}\n",
    "all_sim_storage_list = []\n",
    "\n",
    "# Define parameters for calculating sentence sims\n",
    "embed_type = 'conceptnet' # conceptnet, sentbert, defsent_cls, amrbart\n",
    "role_weights = 'struct' # fixed, STS3k, struct\n",
    "role_align = False\n",
    "\n",
    "# Get embeddings for all semantic roles and all sentences\n",
    "embed_model = dict_of_models[embed_type]\n",
    "role_weights_dict = role_weights_dict_of_dicts[role_weights] # choose which set of semantic role weights to use\n",
    "all_embeds_norml = get_all_rolewise_embeddings(sentences_parsed_dict, embed_type, embed_model)\n",
    "\n",
    "# Loop over all sentence pairs to compute rolewise similarities\n",
    "for idx,sent_id_pair in enumerate(sent_id_pairs):\n",
    "   \n",
    "    # Store in dictionary with all sentence pairs\n",
    "    all_role_sim_storage[idx] = calc_rolewise_sims(sent_id_pair, all_embeds_norml, sentences_parsed_dict, role_align, role_weights)\n",
    "\n",
    "    # Convert dictionary storage of rolewise sims into list and store\n",
    "    sim_storage_list = dict_sim_to_list(role_weights_dict,all_role_sim_storage[idx])\n",
    "    all_sim_storage_list.append(sim_storage_list)\n",
    "    \n",
    "# Save in json file\n",
    "save_file_stem = dataset_name+\"_parse-\"+parse_type+\"_embed-\"+embed_type+\"_weights-\"+role_weights+\"_align-\"+str(int(role_align))+'_norml'\n",
    "with open(file_paths_dict['parses_path']+\"\\\\\"+parse_type+\" Parsing\\\\\"+save_file_stem+\"_rolesims.json\", \"w\") as file:\n",
    "    json.dump(all_role_sim_storage, file, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08228788",
   "metadata": {},
   "source": [
    "### Step 3: Compute overall similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load stored rolewise sims into list\n",
    "\n",
    "# load rolewise similarities file\n",
    "json_rolewise_path = file_paths_dict['parses_path']+\"\\\\\"+parse_type+\" Parsing\\\\\"+save_file_stem+\"_rolesims.json\"\n",
    "with open(json_rolewise_path, \"r\") as file:\n",
    "    all_role_sim_storage = json.load(file)\n",
    "    \n",
    "# loop over all sentence pairs to load rolewise similarities\n",
    "all_sim_storage_list = []\n",
    "for idx,pair_id in enumerate(sent_id_pairs):\n",
    "    # Convert dictionary storage of rolewise sims into list and store\n",
    "    sim_storage_list = dict_sim_to_list(role_weights_dict,all_role_sim_storage[str(idx)])\n",
    "    all_sim_storage_list.append(sim_storage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2b94af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overall VerbNet similarity\n",
    "\n",
    "# Compute basic version of VerbNet-based similarities with the SAME normalisation applied to each sentence pair\n",
    "all_sims_storage_np = np.array(all_sim_storage_list) \n",
    "role_weights_array = np.array(list(role_weights_dict.values())) # get role weights\n",
    "all_sim_storage_list_nonan = np.ma.array(all_sims_storage_np, mask=np.isnan(all_sims_storage_np)) # ignore nan values\n",
    "basic_similarities_non_norm = np.ma.dot(all_sim_storage_list_nonan, role_weights_array)\n",
    "basic_similarities = basic_similarities_non_norm/np.sum(list(role_weights_dict.values())) # divide by total sum of weights\n",
    "\n",
    "# Compute normalised VerbNet-based similarities, with each sentence normalised by ITS OWN length\n",
    "rolewise_weights_masked = []\n",
    "alt_similarities = []\n",
    "i = 0\n",
    "for pair_mask in all_sim_storage_list_nonan.mask:\n",
    "    role_weights_array_masked = role_weights_array[~pair_mask] # need to invert the mask for the roles we want to keep\n",
    "    total_length = np.sum(role_weights_array_masked) # get total weighted length of each sentence\n",
    "    alt_similarities.append(basic_similarities_non_norm[i]/total_length) # normalise weights by total weight length\n",
    "    i+=1\n",
    "\n",
    "# Save results to files\n",
    "save_file_name = file_paths_dict['sims_path']+save_file_stem+'_fixedparms_basic_similarities.txt'\n",
    "np.savetxt(save_file_name, basic_similarities, fmt='%f')\n",
    "save_file_name = file_paths_dict['sims_path']+save_file_stem+'_fixedparms_alt_similarities.txt'\n",
    "np.savetxt(save_file_name, alt_similarities, fmt='%f')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "367f857d",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate VerbNet Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cd308",
   "metadata": {},
   "source": [
    "Plot a histograph of VerbNet rolewise similarities for evaluation of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1e598107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot histogram of basic similarities\n",
    "sns.set()\n",
    "plt.hist(basic_similarities.data, bins=40)\n",
    "# plt.hist(alt_similarities, bins=40)\n",
    "plt.xlim(0.05,1.3)\n",
    "plt.ylim(0,300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d8d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e86f3c5951539f2722badccf95389aafd2846e6c6d91d84b623110a1b2c6697"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
