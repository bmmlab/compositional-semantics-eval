{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script for computing the split-half correlation of the STS3k dataset\n",
    "# Run with python 3.9 base\n",
    "# James Fodor 2024\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Main ID</th>\n",
       "      <th>Adversarial</th>\n",
       "      <th>Sentence pair</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NADV</td>\n",
       "      <td>A predominant concept distinguishes pain from ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.123752</td>\n",
       "      <td>0.532998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NADV</td>\n",
       "      <td>The characteristic theme accelerates the predo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.439122</td>\n",
       "      <td>0.963809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NADV</td>\n",
       "      <td>The absurdity of society intrinsically justifi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.427146</td>\n",
       "      <td>0.927072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NADV</td>\n",
       "      <td>The purpose of sincerity anticipates repeated ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.111776</td>\n",
       "      <td>0.488879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NADV</td>\n",
       "      <td>Modern capabilities opposite cultural denotati...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.241517</td>\n",
       "      <td>0.788865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>2796</td>\n",
       "      <td>NADV</td>\n",
       "      <td>Computers change quickly in the modern world.\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.095238</td>\n",
       "      <td>1.476958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>2797</td>\n",
       "      <td>NADV</td>\n",
       "      <td>The president commences talks with the prime m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>2798</td>\n",
       "      <td>NADV</td>\n",
       "      <td>The children ran out of the house.\\nThe dog ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.160577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>2799</td>\n",
       "      <td>NADV</td>\n",
       "      <td>Potatoes grow well in most climates.\\nCarrots ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.238095</td>\n",
       "      <td>0.425918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>2800</td>\n",
       "      <td>NADV</td>\n",
       "      <td>Some medications alter brain function.\\nSome d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.479865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 506 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Main ID Adversarial                                      Sentence pair  \\\n",
       "0           1        NADV  A predominant concept distinguishes pain from ...   \n",
       "1           2        NADV  The characteristic theme accelerates the predo...   \n",
       "2           3        NADV  The absurdity of society intrinsically justifi...   \n",
       "3           4        NADV  The purpose of sincerity anticipates repeated ...   \n",
       "4           5        NADV  Modern capabilities opposite cultural denotati...   \n",
       "...       ...         ...                                                ...   \n",
       "2795     2796        NADV  Computers change quickly in the modern world.\\...   \n",
       "2796     2797        NADV  The president commences talks with the prime m...   \n",
       "2797     2798        NADV  The children ran out of the house.\\nThe dog ch...   \n",
       "2798     2799        NADV  Potatoes grow well in most climates.\\nCarrots ...   \n",
       "2799     2800        NADV  Some medications alter brain function.\\nSome d...   \n",
       "\n",
       "        1    2    3    4    5    6    7  ...  494  495  496  497  498  499  \\\n",
       "0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "1     1.0  1.0  1.0  1.0  5.0  1.0  1.0  ...  1.0  1.0  1.0  4.0  1.0  1.0   \n",
       "2     1.0  1.0  1.0  2.0  1.0  1.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "3     1.0  1.0  1.0  1.0  5.0  1.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "4     2.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2795  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2796  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2797  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2798  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2799  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  1.0  NaN  NaN  NaN  1.0  NaN   \n",
       "\n",
       "      500  501      Mean       Std  \n",
       "0     1.0  1.0  1.123752  0.532998  \n",
       "1     1.0  1.0  1.439122  0.963809  \n",
       "2     1.0  1.0  1.427146  0.927072  \n",
       "3     1.0  1.0  1.111776  0.488879  \n",
       "4     1.0  1.0  1.241517  0.788865  \n",
       "...   ...  ...       ...       ...  \n",
       "2795  NaN  NaN  3.095238  1.476958  \n",
       "2796  NaN  NaN  1.100000  0.300000  \n",
       "2797  NaN  NaN  1.714286  1.160577  \n",
       "2798  NaN  NaN  1.238095  0.425918  \n",
       "2799  NaN  NaN  3.100000  1.479865  \n",
       "\n",
       "[2800 rows x 506 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "data_T_all = pd.read_excel('Data-experiment\\STS3k_all_participant_data.xlsx')\n",
    "\n",
    "# Filter rows where Adversarial is equal to 'NADV'\n",
    "data_T_NADV = data_T_all[data_T_all['Adversarial'] == 'NADV']\n",
    "data_T_ADV = data_T_all[data_T_all['Adversarial'] == 'ADV']\n",
    "\n",
    "data_T_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to actually perform the split-half computation\n",
    "def split_half_Spearman_Brown(data, num_splits, remove_ten):\n",
    "    \n",
    "    # Prepare the rating data\n",
    "    data_ratings = data.iloc[:, 3:-2].values\n",
    "    if remove_ten==True:\n",
    "        data_ratings = np.delete(data_ratings, slice(0, 10), axis=0)  # Exclude the first 10 pairs\n",
    "    data_ratings = data_ratings.astype(float)  # Convert the data from string to float\n",
    "\n",
    "    # Check the dimensions\n",
    "    num_sentences = data_ratings.shape[0]\n",
    "    print('Number of the sentences =', num_sentences)\n",
    "    num_subjects = data_ratings.shape[1]\n",
    "    print('Number of the subjects =', num_subjects)\n",
    "    \n",
    "    # Compute split-half reliability with n different splits\n",
    "    split_half_correls = []\n",
    "    cohens_kappas = []\n",
    "\n",
    "    for split_num in np.arange(num_splits):\n",
    "        # Random split of the subjects\n",
    "        idx_subjects_random = np.random.permutation(num_subjects)\n",
    "        idx_subjects_group1 = idx_subjects_random[:num_subjects // 2]\n",
    "        idx_subjects_group2 = idx_subjects_random[num_subjects // 2:]\n",
    "\n",
    "        # Average rating in group 1\n",
    "        data_tmp = data_ratings[:, idx_subjects_group1]\n",
    "        data_mean_group1 = []\n",
    "        data_mean_norm_group1 = []\n",
    "        for row in data_tmp:\n",
    "            tmp_data_mean = np.nanmean(row)\n",
    "            tmp_data_mean_norm = (tmp_data_mean - 1) / 6  # Normalization is applied\n",
    "            data_mean_group1.append(tmp_data_mean)\n",
    "            data_mean_norm_group1.append(tmp_data_mean_norm)\n",
    "        data_mean_group1 = np.array(data_mean_group1)\n",
    "        data_mean_norm_group1 = np.array(data_mean_norm_group1)\n",
    "\n",
    "        # Average rating in group 2\n",
    "        data_tmp = data_ratings[:, idx_subjects_group2]\n",
    "        data_mean_group2 = []\n",
    "        data_mean_norm_group2 = []\n",
    "        for row in data_tmp:\n",
    "            tmp_data_mean = np.nanmean(row)\n",
    "            tmp_data_mean_norm = (tmp_data_mean - 1) / 6  # Normalization is applied\n",
    "            data_mean_group2.append(tmp_data_mean)\n",
    "            data_mean_norm_group2.append(tmp_data_mean_norm)\n",
    "        data_mean_group2 = np.array(data_mean_group2)\n",
    "        data_mean_norm_group2 = np.array(data_mean_norm_group2)\n",
    "        \n",
    "        # Compute cohen's kappa\n",
    "        data_round_group1 = np.array([math.ceil(x) if math.isnan(x)==False else 0 for x in data_mean_group1])\n",
    "        data_round_group2 = np.array([math.ceil(x) if math.isnan(x)==False else 0 for x in data_mean_group2])\n",
    "        cohens_kappa = cohen_kappa_score(data_round_group1, data_round_group2, weights='linear')\n",
    "        cohens_kappas.append(cohens_kappa)\n",
    "        \n",
    "        # Correlation between groups 1 and 2\n",
    "        correl = np.corrcoef(data_mean_norm_group1, data_mean_norm_group2)[0,1]\n",
    "        split_half_correls.append(correl)\n",
    "        \n",
    "    # Apply Spearman–Brown prediction formula\n",
    "    mean_split_half_correl = np.nanmean(split_half_correls)\n",
    "    corrected_mean_split_half_correl = 2*mean_split_half_correl/(1+mean_split_half_correl)\n",
    "    print('Split-half correlation: {:.3f}'.format(corrected_mean_split_half_correl))\n",
    "    \n",
    "    mean_cohens_kappa = np.nanmean(cohens_kappas)\n",
    "    corrected_mean_cohens_kappa = 2*mean_cohens_kappa/(1+mean_cohens_kappa)\n",
    "    print('Cohens kappa: {:.3f}'.format(corrected_mean_cohens_kappa))\n",
    "    \n",
    "    return corrected_mean_split_half_correl, corrected_mean_cohens_kappa, split_half_correls, cohens_kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the sentences = 2790\n",
      "Number of the subjects = 501\n",
      "Split-half correlation: 0.953\n",
      "Cohens kappa: 0.832\n",
      "Number of the sentences = 1054\n",
      "Number of the subjects = 501\n",
      "Split-half correlation: 0.950\n",
      "Cohens kappa: 0.825\n",
      "Number of the sentences = 1736\n",
      "Number of the subjects = 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fods1\\AppData\\Local\\Temp\\ipykernel_23748\\3622918954.py:43: RuntimeWarning: Mean of empty slice\n",
      "  tmp_data_mean = np.nanmean(row)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split-half correlation: 0.939\n",
      "Cohens kappa: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Compute the value for three splits of the dataset\n",
    "all_msh, all_cohens, all_correls, all_cohens_kappas = split_half_Spearman_Brown(data_T_all, 100, True)\n",
    "NADV_msh, NADV_cohens, NADV_correls, NADV_kappas = split_half_Spearman_Brown(data_T_NADV, 100, True)\n",
    "ADV_msh, ADV_cohens, ADV_correls, ADV_kappas = split_half_Spearman_Brown(data_T_ADV, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 35.28301545387555\n",
      "p-value: 2.457508755211328e-87\n"
     ]
    }
   ],
   "source": [
    "# Compute t-statistic for differences of correlations\n",
    "t_statistic, p_value = stats.ttest_ind(NADV_correls, ADV_correls)\n",
    "print('t-statistic:', t_statistic)  \n",
    "print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18035074007553298\n",
      "t-statistic: -32.310958004698584\n",
      "p-value: 8.536387047157436e-197\n",
      "0.2192170476165174\n",
      "t-statistic: -0.49113368370383176\n",
      "p-value: 0.6233655900150772\n",
      "0.22205916583562024\n",
      "t-statistic: 1.379506868394236\n",
      "p-value: 0.16792006515286023\n",
      "0.2581748432732137\n",
      "t-statistic: 13.020268002854674\n",
      "p-value: 2.513688017178855e-37\n",
      "0.24070054247790307\n",
      "t-statistic: 17.533050198598207\n",
      "p-value: 2.9370972801420706e-67\n",
      "0.21510000237793014\n",
      "t-statistic: -3.2627919155559084\n",
      "p-value: 0.0011104707936341511\n",
      "0.245185392972449\n",
      "t-statistic: 5.5939997889199615\n",
      "p-value: 2.633516211089287e-08\n",
      "0.13620257779215839\n",
      "t-statistic: -20.14760571933381\n",
      "p-value: 5.516960271691878e-78\n"
     ]
    }
   ],
   "source": [
    "## Resampling analysis for ONLY_MOD sentence pairs\n",
    "places = [515,520,531,544,552,560,568,580,592,630,638,656,665,676,689,695,699,715,721,726,738,743,747,755,759,766,771,786,791,799]\n",
    "times = [514,516,537,543,549,565,569,573,577,581,588,604,605,608,614,641,645,666,670,675,684,701,711,716,720,725,734,739,754,762,783,785]\n",
    "manner = [555,564,599,610,619,625,636,659,681,685,710,730,742,758,773,787,805]\n",
    "IOBJ = [511,521,528,533,538,542,548,553,586,591,600,609,620,631,696,706,736,748,767,777,782]\n",
    "SUBJ_adj = [512,517,522,526,529,535,540,546,550,556,561,566,571,578,582,585,587,593,598,601,606,612,617,621,626,632,637,642,647,651,652,657,661,667,672,677,682,690,692,700,702,708,719,724,727,731,732,735,740,746,752,756,760,763,769,772,778,784,789,792,796,802,804,808]\n",
    "DOBJ_adj = [524,530,536,539,547,554,558,563,567,572,574,576,583,590,594,597,603,611,616,624,629,634,639,646,648,653,662,668,674,678,683,688,693,697,704,709,713,718,722,741,749,751,757,770,776,781,790,794,797,803,809]\n",
    "IOBJ_adj = [518,559,575,635,643,655,658,664,671,679,714,717,723,765,793]\n",
    "passive = [525,534,584,595,615,623,628,660,691,705,744,806]\n",
    "\n",
    "data_ratings = data_T_all.iloc[:, 3:-2].values\n",
    "\n",
    "num_subjects = 501\n",
    "num_resamples = 100\n",
    "\n",
    "for subset in [places, times, manner, IOBJ, SUBJ_adj, DOBJ_adj, IOBJ_adj, passive]:\n",
    "    resampled_means = []\n",
    "    for x in np.arange(num_resamples):\n",
    "        set_of_subjects = np.random.choice(np.arange(0,num_subjects), size=num_subjects, replace=True)\n",
    "        mean_subset = (np.nanmean(data_ratings[np.ix_(subset,set_of_subjects)], axis=1)-1)/6\n",
    "        resampled_means.append(mean_subset)\n",
    "        \n",
    "    # t_statistic, p_value = stats.ttest_1samp(1-np.mean(resampled_means,axis=1), 0.22)\n",
    "    t_statistic, p_value = stats.ttest_1samp(1-np.array(resampled_means).flatten(), 0.22)\n",
    "    print(1-np.mean(resampled_means))\n",
    "    print('t-statistic:', t_statistic)\n",
    "    print('p-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
