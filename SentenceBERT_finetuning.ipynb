{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuning of sentence transformer with STS3k dataset\n",
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'nli-distilroberta-base-v2' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "sourced from https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py\n",
    "\"\"\"\n",
    "\n",
    "# Extra source: https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "# https://joecummings.me/tutorials/bert\n",
    "\n",
    "# Load libraries\n",
    "import math\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_embeds_processing import load_sentences\n",
    "\n",
    "# Specify paths\n",
    "path_root = \"D:\\Study and Projects\\School Work\\Year 25 - PhD 1\\Data\\\\\"\n",
    "embeddings_path = 'Analysis Results\\Sentence Embeddings\\\\'\n",
    "sts3k_dataset_path = 'Sentence Similarity Data\\Fodor2023 - STS3k Large Dataset\\\\3 - Experimental data\\\\'\n",
    "data_pairs_path = 'Sentence Similarity Data\\\\Sentence Similarities Final\\\\'\n",
    "\n",
    "# Calculate sentence embedding using sentbert\n",
    "def get_sentbert_embedding(sentbert_model, sentences):\n",
    "    sentence_embeddings = sentbert_model.encode(sentences, convert_to_tensor=True)    \n",
    "    return np.array(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-trained sentence transformer model\n",
    "model_name = path_root+'Sentence Embeddings\\sentence-transformers-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load experimental dataset and prepare for fine-tuning\n",
    "\n",
    "# Define dataset to use for fine-tuning\n",
    "dataset_name = 'STS3k'\n",
    "# full_dataset_path = path_root+'\\Sentence Similarity Data\\STSb Dataset\\stsbenchmark.tsv'\n",
    "# full_dataset_path = path_root+sts3k_dataset_path+'STS3k_sentbert_ft_format_10waysplit.tsv'\n",
    "full_dataset_path = path_root+sts3k_dataset_path+'STS3k_sentbert_ft_format_traintestsplit.tsv'\n",
    "# full_dataset_path = path_root+sts3k_dataset_path+'STS3k_sentbert_ft_format_adversarialsplit.tsv'\n",
    "\n",
    "# Define storage lists for dataset split\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "num_epochs = 4\n",
    "\n",
    "# Specify the test and dev set names\n",
    "test_set = 'test'\n",
    "dev_set = 'dev'\n",
    "train_set = 'train'\n",
    "\n",
    "# Load dataset\n",
    "with open(full_dataset_path, 'rt', encoding='utf8') as file:\n",
    "    reader = csv.DictReader(file, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score'])\n",
    "        inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n",
    "\n",
    "        if row['split'] == test_set:\n",
    "            test_samples.append(inp_example)\n",
    "        elif row['split'] == dev_set:\n",
    "            dev_samples.append(inp_example)\n",
    "        elif row['split'] == train_set:\n",
    "            train_samples.append(inp_example)\n",
    "\n",
    "# Define dataloader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Measure correlation between cosine score and gold labels on dev set\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880a1b019a25479eafef1c0cade52763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3496086374f547a083be5bc10efeda15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e083d37ba4fbcadb9ea17a4078b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ef86efffd47759e12093f7f5cd2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954aca960c7f4b8dadaf8dd6e2f103b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9176653704079322"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform fine-tuning of model on specified dataset\n",
    "\n",
    "# Name of saved ft file\n",
    "name = full_dataset_path.split('_')[-1].split('.')[0]\n",
    "model_save_path = dataset_name+'_ft_'+name+'_4_epochs'\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "# Load the saved model and evaluate its performance on test dataset\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='STS3k_test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract sentence pair data by 10-fold set and compute embeddings by trained model\n",
    "\n",
    "# Prepare sentences data by 10-fold sets\n",
    "data_raw_array = np.loadtxt(full_dataset_path,  delimiter='\\t', dtype='str', encoding='utf-8', skiprows=1)\n",
    "index_values = np.arange(0,10)\n",
    "data_by_set_dict_a = dict(zip(index_values, [[] for x in index_values]))\n",
    "data_by_set_dict_b = dict(zip(index_values, [[] for x in index_values]))\n",
    "\n",
    "for row in data_raw_array:\n",
    "    sentence_a = row[7]\n",
    "    sentence_b = row[8]\n",
    "    test_set = int(row[1][-1]) # 10 -> 0\n",
    "    sentence_pair_id = int(row[0])\n",
    "    data_by_set_dict_a[test_set].append([sentence_pair_id,sentence_a])\n",
    "    data_by_set_dict_b[test_set].append([sentence_pair_id,sentence_b])\n",
    "    \n",
    "# Store fine-tuned sentence embeddings by 10-fold sets\n",
    "directory = path_root+'\\Sentence Embeddings\\sentence-transformers-mpnet-base-v2-ft-STS3k-10fold'\n",
    "sentbert_models_ft_a = {}\n",
    "sentbert_models_ft_b = {}\n",
    "for idx,model_folder in enumerate(os.listdir(directory)):\n",
    "    sentbert_model = SentenceTransformer(directory+'\\\\'+model_folder)\n",
    "    sentences_a = np.array(data_by_set_dict_a[idx])[:,1]\n",
    "    sentences_b = np.array(data_by_set_dict_b[idx])[:,1]\n",
    "    sentbert_models_ft_a[idx] = get_sentbert_embedding(sentbert_model, list(sentences_a))\n",
    "    sentbert_models_ft_b[idx] = get_sentbert_embedding(sentbert_model, list(sentences_b))\n",
    "    \n",
    "# Arrange all 10-fold embeddings into a single dictionary (set a)\n",
    "full_set_embeddings_a = {}\n",
    "for fold_set in data_by_set_dict_a.keys():\n",
    "    fold_set_sentence_ids = np.array(data_by_set_dict_a[fold_set])[:,0]\n",
    "    new_dict = dict(zip(fold_set_sentence_ids, sentbert_models_ft_a[fold_set]))\n",
    "    full_set_embeddings_a.update(new_dict)\n",
    "    \n",
    "# Arrange all 10-fold embeddings into a single dictionary (set b)\n",
    "full_set_embeddings_b = {}\n",
    "for fold_set in data_by_set_dict_b.keys():\n",
    "    fold_set_sentence_ids = np.array(data_by_set_dict_b[fold_set])[:,0]\n",
    "    new_dict = dict(zip(fold_set_sentence_ids, sentbert_models_ft_b[fold_set]))\n",
    "    full_set_embeddings_b.update(new_dict)\n",
    "    \n",
    "# Sort dictionaries\n",
    "keys_a = [int(x) for x in list(full_set_embeddings_a.keys())]\n",
    "keys_a.sort()\n",
    "sorted_dict = {i: full_set_embeddings_a[str(i)] for i in keys_a}\n",
    "full_set_sorted_embeddings_a = np.array(list(sorted_dict.values()))\n",
    "keys_b = [int(x) for x in list(full_set_embeddings_b.keys())]\n",
    "keys_b.sort()\n",
    "sorted_dict = {i: full_set_embeddings_b[str(i)] for i in keys_b}\n",
    "full_set_sorted_embeddings_b = np.array(list(sorted_dict.values()))\n",
    "\n",
    "# Save results\n",
    "np.savetxt(dataset_name+'_a_sentbert_ft_embeddings.txt', full_set_sorted_embeddings_a, fmt='%f')\n",
    "np.savetxt(dataset_name+'_b_sentbert_ft_embeddings.txt', full_set_sorted_embeddings_b, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract and store embeddings from a single fine-tuned model\n",
    "\n",
    "# Load pre-finetuned model\n",
    "sentbert_model = SentenceTransformer(model_save_path)\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = 'STS3k_all'\n",
    "dataset_dict = load_sentences(path_root+data_pairs_path+dataset_name+'.txt', pairs=True)\n",
    "dataset_np = np.array(list(dataset_dict.values()))\n",
    "\n",
    "# Compute embeddings\n",
    "sentbert_models_ft_a = get_sentbert_embedding(sentbert_model, list(dataset_np[:,0]))\n",
    "sentbert_models_ft_b = get_sentbert_embedding(sentbert_model, list(dataset_np[:,1]))\n",
    "\n",
    "# Save results\n",
    "np.savetxt(dataset_name+'_a_sentbert_mpnet_ft_'+name+'_'+str(num_epochs)+'_epochs_embeddings.txt', sentbert_models_ft_a, fmt='%f')\n",
    "np.savetxt(dataset_name+'_b_sentbert_mpnet_ft_'+name+'_'+str(num_epochs)+'_epochs_embeddings.txt', sentbert_models_ft_b, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
