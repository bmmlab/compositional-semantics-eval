{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script constructs the AMR parsing for a list of sentences\n",
    "# Run using 'Spring' environment with Python 3.8\n",
    "# James Fodor 2023\n",
    "# \n",
    "# Requires the amrlib package, see docs here: https://amrlib.readthedocs.io/en/latest/\n",
    "# The specific parsing model is here https://github.com/SapienzaNLP/spring\n",
    "#\n",
    "# The code takes in a list of sentences and returns .txt and .json files with the AMR parses of each sentence.\n",
    "# See the AMRlib documentation linked above for the formatting of these files.\n",
    "\n",
    "# load libraries\n",
    "import json\n",
    "import amrlib\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from amrlib.evaluate.smatch_enhanced import match_pair\n",
    "\n",
    "# file containing path directories\n",
    "with open(\"D:\\\\My Code\\\\Python\\\\2023_02 fMRI RSA Analysis\\\\file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define key functions and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load embeddings and parsing model\n",
    "\n",
    "# load ConceptNet embeddings\n",
    "model_address = file_paths_dict['path_root']+'Word Embeddings\\ConceptNet Embeddings\\\\numberbatch-en.txt'\n",
    "conceptnet_embeds = sep.import_word_model(model_address)\n",
    "\n",
    "# load AMR parse model\n",
    "model_address = file_paths_dict['path_root']+'Sentence Encoders\\\\amrlib-parsing'\n",
    "stog = amrlib.load_stog_model(model_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to perform AMR parsing and calculate AMR similarity\n",
    "\n",
    "# Function to parse sentences (sentences must end with a full stop!!)\n",
    "def AMR_parse_sent_pair(sentence_pair):\n",
    "    graphs = stog.parse_sents(sentence_pair)\n",
    "    # for graph in graphs:\n",
    "        # print(graph)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# Calculate smatch similarity\n",
    "def smatch_sim(graph_pair):\n",
    "    out = match_pair((graph_pair[0].split('.')[1],graph_pair[1].split('.')[1]))\n",
    "    return out[0]/12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 GS2011_processed\n",
      "1 KS2013_processed\n",
      "2 Fodor_pilot_2022\n",
      "3 STS131_processed\n",
      "4 SICK_relatedness\n",
      "5 STR_processed\n",
      "6 STSb_captions_test\n",
      "7 STSb_forums_test\n",
      "8 STSb_headlines_test\n",
      "9 STSb_test\n",
      "10 STS3k_all\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = True # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded STR_processed with 5500 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set \n",
    "\n",
    "# choose number from those printed above\n",
    "dataset_name = datasets[5]\n",
    "\n",
    "# load sentence set into dictionary depending on type\n",
    "if pairs == True:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['data_pairs_path'], pairs)\n",
    "else:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['neuro_root'], pairs)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset_name,'with',n,'sentences')\n",
    "\n",
    "# store in list\n",
    "sentences = []\n",
    "if pairs==True: # use this for sentence similarity pair data\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,0].flatten()))\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,1].flatten()))\n",
    "else: # use this for neuroimaging data/list of sentences\n",
    "    sentences.append(list(sentences_dict.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and save AMR parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z2', ':location', 'z1')\n",
      "ignoring epigraph data for duplicate triple: ('z2', ':location', 'z1')\n",
      "ignoring secondary node contexts for 'z2'\n",
      "ignoring secondary node contexts for 'z2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z6')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG2', 'z6')\n",
      "ignoring secondary node contexts for 'z6'\n",
      "ignoring secondary node contexts for 'z6'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing concept: ( z1 / and :op1 ( z2 / order :mod ( z3 / sell-01 ) ) :op2 ( z4 / order :mod z3 ) :op3 ( z5 / refund :mod z3 ) :op4 ( z6 / refund :mod z3 ) :op5 ( z7 / order :mod z3 ) :op6 ( z8 / order :mod ( z9 / purchase-01 ) ) :op7 ( z10 / refund :mod z9 ) :op8 ( z11 / refund :mod z9 ) :op9 ( z12 / ) ) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z14', ':mod', 'z11')\n",
      "ignoring epigraph data for duplicate triple: ('z14', ':mod', 'z11')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n",
      "2720\n",
      "2740\n",
      "2760\n",
      "2780\n",
      "2800\n",
      "2820\n",
      "2840\n",
      "2860\n",
      "2880\n",
      "2900\n",
      "2920\n",
      "2940\n",
      "2960\n",
      "2980\n",
      "3000\n",
      "3020\n",
      "3040\n",
      "3060\n",
      "3080\n",
      "3100\n",
      "3120\n",
      "3140\n",
      "3160\n",
      "3180\n",
      "3200\n",
      "3220\n",
      "3240\n",
      "3260\n",
      "3280\n",
      "3300\n",
      "3320\n",
      "3340\n",
      "3360\n",
      "3380\n",
      "3400\n",
      "3420\n",
      "3440\n",
      "3460\n",
      "3480\n",
      "3500\n",
      "3520\n",
      "3540\n",
      "3560\n",
      "3580\n",
      "3600\n",
      "3620\n",
      "3640\n",
      "3660\n",
      "3680\n",
      "3700\n",
      "3720\n",
      "3740\n",
      "3760\n",
      "3780\n",
      "3800\n",
      "3820\n",
      "3840\n",
      "3860\n",
      "3880\n",
      "3900\n",
      "3920\n",
      "3940\n",
      "3960\n",
      "3980\n",
      "4000\n",
      "4020\n",
      "4040\n",
      "4060\n",
      "4080\n",
      "4100\n",
      "4120\n",
      "4140\n",
      "4160\n",
      "4180\n",
      "4200\n",
      "4220\n",
      "4240\n",
      "4260\n",
      "4280\n",
      "4300\n",
      "4320\n",
      "4340\n",
      "4360\n",
      "4380\n",
      "4400\n",
      "4420\n",
      "4440\n",
      "4460\n",
      "4480\n",
      "4500\n",
      "4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG6', '5223')\n",
      "ignoring epigraph data for duplicate triple: ('z1', ':ARG6', '5223')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540\n",
      "4560\n",
      "4580\n",
      "4600\n",
      "4620\n",
      "4640\n",
      "4660\n",
      "4680\n",
      "4700\n",
      "4720\n",
      "4740\n",
      "4760\n",
      "4780\n",
      "4800\n",
      "4820\n",
      "4840\n",
      "4860\n",
      "4880\n",
      "4900\n",
      "4920\n",
      "4940\n",
      "4960\n",
      "4980\n",
      "5000\n",
      "5020\n",
      "5040\n",
      "5060\n",
      "5080\n",
      "5100\n",
      "5120\n",
      "5140\n",
      "5160\n",
      "5180\n",
      "5200\n",
      "5220\n",
      "5240\n",
      "5260\n",
      "5280\n",
      "5300\n",
      "5320\n",
      "5340\n",
      "5360\n",
      "5380\n",
      "5400\n",
      "5420\n",
      "5440\n",
      "5460\n",
      "5480\n",
      "5500\n"
     ]
    }
   ],
   "source": [
    "## Parse and save AMR parses using Spring parser\n",
    "# SENTENCES MUST END WITH A FULL STOP!\n",
    "\n",
    "# Parse sentence pairs\n",
    "if pairs==True:\n",
    "    AMR_graph_storage = {}\n",
    "    for pair_id in sentences_dict.keys():\n",
    "        sent_1 = sentences_dict[pair_id][0]\n",
    "        sent_2 = sentences_dict[pair_id][1]\n",
    "        try:\n",
    "            sent_parses = AMR_parse_sent_pair(sentences_dict[pair_id][0:2]) # get parses for both sentences in pair\n",
    "        except:\n",
    "            sent_parses = ['NULL','NULL'] # in case parsing fails\n",
    "        AMR_graph_storage[pair_id] = [sent_1,sent_2,sent_parses[0],sent_parses[1]]\n",
    "        if pair_id%20==0:\n",
    "            print(pair_id)\n",
    "        \n",
    "    ## Reformat AMR parse dict for saving two sets of sentences separately\n",
    "    AMR_parse_sent_1 = []\n",
    "    AMR_parse_sent_2 = []\n",
    "    for idx in AMR_graph_storage.keys():\n",
    "        new_tree_1 = AMR_graph_storage[idx][2].replace('::snt', '::snt-'+str(idx)) # need to adjust naming\n",
    "        new_tree_2 = AMR_graph_storage[idx][3].replace('::snt', '::snt-'+str(idx))\n",
    "        AMR_parse_sent_1.append(new_tree_1)\n",
    "        AMR_parse_sent_2.append(new_tree_2)\n",
    "        \n",
    "    # save first set of sentences\n",
    "    save_file = open(dataset_name+\"_a_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent_1:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "\n",
    "    # save second set of sentences\n",
    "    save_file = open(dataset_name+\"_b_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent_2:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "    \n",
    "# Parse single list of sentences (neuro data)\n",
    "elif pairs==False:\n",
    "    AMR_graph_storage = {}\n",
    "    for sent_id in sentences_dict.keys():\n",
    "        sent = sentences_dict[sent_id]\n",
    "        try:\n",
    "            sent_parse = AMR_parse_sent_pair([sent]) # inputs needs to be a list\n",
    "        except:\n",
    "            sent_parse = ['NULL'] # in case parsing fails\n",
    "        AMR_graph_storage[sent_id] = [sent,sent_parse]\n",
    "        if sent_id%20==0:\n",
    "            print(sent_id)\n",
    "        \n",
    "    ## Reformat AMR parse dict for saving two sets of sentences separately\n",
    "    AMR_parse_sent = []\n",
    "    for idx in AMR_graph_storage.keys():\n",
    "        new_tree = AMR_graph_storage[idx][1][0].replace('::snt', '::snt-'+str(idx)) # need to adjust naming\n",
    "        AMR_parse_sent.append(new_tree)\n",
    "        \n",
    "    # save first set of sentences\n",
    "    save_file = open(dataset_name+\"_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "\n",
    "# Save full set of parses of sentences to json file\n",
    "with open(dataset_name+\"_AMR_parse.json\", \"w\") as filename:\n",
    "    json.dump(AMR_graph_storage, filename, indent=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd19f865deb76414f800cd5e170dbd2fd2287196326fe3642087e61bd5d12a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
