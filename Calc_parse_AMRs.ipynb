{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script constructs the AMR parsing for a list of sentences\n",
    "# Run using 'Spring' environment with Python 3.8\n",
    "# James Fodor 2023\n",
    "# \n",
    "# Requires the amrlib package, see docs here: https://amrlib.readthedocs.io/en/latest/\n",
    "# The specific parsing model is here https://github.com/SapienzaNLP/spring\n",
    "#\n",
    "# The code takes in a list of sentences and returns .txt and .json files with the AMR parses of each sentence.\n",
    "# See the AMRlib documentation linked above for the formatting of these files.\n",
    "\n",
    "# load libraries\n",
    "import json\n",
    "import amrlib\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from amrlib.evaluate.smatch_enhanced import match_pair\n",
    "\n",
    "# load file paths\n",
    "with open(\"file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)\n",
    "\n",
    "# numpy print options\n",
    "np.set_printoptions(precision=2, threshold=2000, linewidth=200, suppress=True, floatmode='fixed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define key functions and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load embeddings and parsing model\n",
    "\n",
    "# load ConceptNet embeddings\n",
    "model_address = file_paths_dict['path_root']+'\\Word Embeddings\\ConceptNet Embeddings\\\\numberbatch-en.txt'\n",
    "conceptnet_embeds = sep.import_word_model(model_address)\n",
    "\n",
    "# load AMR parse model\n",
    "model_address = file_paths_dict['path_root']+'\\Sentence Encoders\\\\amrlib-parsing'\n",
    "stog = amrlib.load_stog_model(model_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to perform AMR parsing and calculate AMR similarity\n",
    "\n",
    "# Function to parse sentences (sentences must end with a full stop!!)\n",
    "def AMR_parse_sent_pair(sentence_pair):\n",
    "    graphs = stog.parse_sents(sentence_pair)\n",
    "    # for graph in graphs:\n",
    "        # print(graph)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# Calculate smatch similarity\n",
    "def smatch_sim(graph_pair):\n",
    "    out = match_pair((graph_pair[0].split('.')[1],graph_pair[1].split('.')[1]))\n",
    "    return out[0]/12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 2014 Wehbe\\Stimuli\\Chapter_9_sentences_final\n",
      "1 2017 Anderson\\Stimuli\\stimuli_final\n",
      "2 2018 Pereira\\Stimuli\\stimuli_243sentences\n",
      "3 2018 Pereira\\Stimuli\\stimuli_384sentences\n",
      "4 2020 Alice Dataset\\Stimuli\\stimuli_sentences_final\n",
      "5 2020 Zhang\\Stimuli\\test_sentences_final\n",
      "6 2023 Fodor Dataset\\Fodor2023-final240\n",
      "7 2023 Fodor Dataset\\Fodor2023-final192\n",
      "8 2023 Fodor Dataset\\Fodor2023-prelim\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = False # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded 2020 Zhang\\Stimuli\\test_sentences_final with 95 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set (choose number from those printed above)\n",
    "dataset = datasets[5]\n",
    "sentences_dict = sep.load_set_of_sentences(dataset, file_paths_dict['data_pairs_path'], file_paths_dict['data_nonpaired_path'], pairs)\n",
    "full_dataset_name = sep.fix_sentence_dataset_name(dataset)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset,'with',n,'sentences')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and save AMR parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "## Parse and save AMR parses using Spring parser\n",
    "# SENTENCES MUST END WITH A FULL STOP!\n",
    "\n",
    "# Parse sentence pairs\n",
    "if pairs==True:\n",
    "    AMR_graph_storage = {}\n",
    "    for pair_id in sentences_dict.keys():\n",
    "        sent_1 = sentences_dict[pair_id][0]\n",
    "        sent_2 = sentences_dict[pair_id][1]\n",
    "        try:\n",
    "            sent_parses = AMR_parse_sent_pair(sentences_dict[pair_id][0:2]) # get parses for both sentences in pair\n",
    "        except:\n",
    "            sent_parses = ['NULL','NULL'] # in case parsing fails\n",
    "        AMR_graph_storage[pair_id] = [sent_1,sent_2,sent_parses[0],sent_parses[1]]\n",
    "        if pair_id%20==0:\n",
    "            print(pair_id)\n",
    "        \n",
    "    ## Reformat AMR parse dict for saving two sets of sentences separately\n",
    "    AMR_parse_sent_1 = []\n",
    "    AMR_parse_sent_2 = []\n",
    "    for idx in AMR_graph_storage.keys():\n",
    "        new_tree_1 = AMR_graph_storage[idx][2].replace('::snt', '::snt-'+str(idx)) # need to adjust naming\n",
    "        new_tree_2 = AMR_graph_storage[idx][3].replace('::snt', '::snt-'+str(idx))\n",
    "        AMR_parse_sent_1.append(new_tree_1)\n",
    "        AMR_parse_sent_2.append(new_tree_2)\n",
    "        \n",
    "    # save first set of sentences\n",
    "    save_file = open(full_dataset_name+\"_a_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent_1:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "\n",
    "    # save second set of sentences\n",
    "    save_file = open(full_dataset_name+\"_b_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent_2:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "    \n",
    "# Parse single list of sentences (neuro data)\n",
    "elif pairs==False:\n",
    "    AMR_graph_storage = {}\n",
    "    for sent_id in sentences_dict.keys():\n",
    "        sent = sentences_dict[sent_id]\n",
    "        try:\n",
    "            sent_parse = AMR_parse_sent_pair([sent]) # inputs needs to be a list\n",
    "        except:\n",
    "            sent_parse = ['NULL'] # in case parsing fails\n",
    "        AMR_graph_storage[sent_id] = [sent,sent_parse]\n",
    "        if sent_id%20==0:\n",
    "            print(sent_id)\n",
    "        \n",
    "    ## Reformat AMR parse dict for saving two sets of sentences separately\n",
    "    AMR_parse_sent = []\n",
    "    for idx in AMR_graph_storage.keys():\n",
    "        new_tree = AMR_graph_storage[idx][1][0].replace('::snt', '::snt-'+str(idx)) # need to adjust naming\n",
    "        AMR_parse_sent.append(new_tree)\n",
    "        \n",
    "    # save first set of sentences\n",
    "    save_file = open(full_dataset_name+\"_AMR_parse.txt\", \"w\", encoding='utf-8')\n",
    "    for line in AMR_parse_sent:\n",
    "        save_file.writelines(line)\n",
    "        save_file.write('\\n\\n')\n",
    "    save_file.close()\n",
    "\n",
    "# Save full set of parses of sentences to json file\n",
    "with open(full_dataset_name+\"_AMR_parse.json\", \"w\") as filename:\n",
    "    json.dump(AMR_graph_storage, filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd19f865deb76414f800cd5e170dbd2fd2287196326fe3642087e61bd5d12a46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
