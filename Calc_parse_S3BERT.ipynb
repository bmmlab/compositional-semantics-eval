{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code computes S3BERT embeddings and saves feature-wise similarities between sentence pairs\n",
    "# Run using base python 3.9\n",
    "# James Fodor 2023\n",
    "# \n",
    "# Requires S3BERT code from https://github.com/flipz357/S3BERT\n",
    "\n",
    "# load libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import sentence_embeds_processing as sep\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# file containing path directories\n",
    "with open(\"D:\\\\My Code\\\\Python\\\\2023_02 fMRI RSA Analysis\\\\file_paths.json\", \"r\") as file:\n",
    "    file_paths_dict = json.load(file)\n",
    "\n",
    "# load S3BERT code\n",
    "sys.path.append(\"./S3BERT_main/\")\n",
    "import config\n",
    "import prediction_helpers as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "0 GS2011_processed\n",
      "1 KS2013_processed\n",
      "2 Fodor_pilot_2022\n",
      "3 STS131_processed\n",
      "4 SICK_relatedness\n",
      "5 STR_processed\n",
      "6 STSb_captions_test\n",
      "7 STSb_forums_test\n",
      "8 STSb_headlines_test\n",
      "9 STSb_test\n",
      "10 STS3k_all\n"
     ]
    }
   ],
   "source": [
    "## Show available datasets, as specified in the sep module\n",
    "pairs = True # specify if we are using paired data or list of sentences\n",
    "if pairs==True:\n",
    "    datasets = sep.available_pair_datasets\n",
    "else:\n",
    "    datasets = sep.available_nonpaired_datasets\n",
    "print('Available datasets:')\n",
    "for dataset in datasets.keys():\n",
    "    print(dataset,datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loaded STR_processed with 5500 sentences\n"
     ]
    }
   ],
   "source": [
    "## Load sentence set \n",
    "\n",
    "# choose number from those printed above\n",
    "dataset_name = datasets[5]\n",
    "\n",
    "# load sentence set into dictionary depending on type\n",
    "if pairs == True:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['data_pairs_path'], pairs)\n",
    "else:\n",
    "    sentences_dict = sep.load_set_of_sentences(dataset_name, file_paths_dict['neuro_root'], pairs)\n",
    "n = len(sentences_dict.keys()) # num sentences\n",
    "print('\\nloaded',dataset_name,'with',n,'sentences')\n",
    "\n",
    "# store in list\n",
    "sentences = []\n",
    "if pairs==True: # use this for sentence similarity pair data\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,0].flatten()))\n",
    "    sentences.append(list(np.array(list(sentences_dict.values()))[:,1].flatten()))\n",
    "else: # use this for neuroimaging data/list of sentences\n",
    "    sentences.append(list(sentences_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute S3BERT feature-wise similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare sets of sentences for computing S3BERT feature-wise similarities\n",
    "\n",
    "# load S3BERT model\n",
    "model = SentenceTransformer(\"S3BERT_main\\s3bert_all-mpnet-base-v2\", device=\"cpu\")\n",
    "\n",
    "# Get two sets of sentences for paired (behaviuoral) data\n",
    "if pairs==True:\n",
    "    \n",
    "    # sentences\n",
    "    sentences_a = np.array(list(sentences_dict.values()))[:,0]\n",
    "    sentences_b = np.array(list(sentences_dict.values()))[:,1]\n",
    "\n",
    "# Get two sets of sentences for unpaired (neuroimaging) data\n",
    "elif pairs==False:\n",
    "    \n",
    "    # define set of sentence pairs\n",
    "    sentences_storage = []\n",
    "    sent_id_pairs = list(itertools.combinations(sentences_dict.keys(), 2))\n",
    "\n",
    "    # encode with S3BERT and store results in numpy array\n",
    "    for sent_id_pair in sent_id_pairs:\n",
    "        sentence_a = sentences_dict[sent_id_pair[0]]\n",
    "        sentence_b = sentences_dict[sent_id_pair[1]]\n",
    "        sentences_storage.append([sentence_a,sentence_b])\n",
    "        \n",
    "    # sentences\n",
    "    sentences_a = np.array(sentences_storage)[:,0]\n",
    "    sentences_b = np.array(sentences_storage)[:,1]\n",
    "\n",
    "# encode with s3bert\n",
    "sentences_a_encoded = model.encode(sentences_a)\n",
    "sentences_b_encoded = model.encode(sentences_b)\n",
    "\n",
    "# get similarity scores of different features\n",
    "similarities = ph.get_preds(sentences_a_encoded, sentences_b_encoded, biases=None, n=config.N, dim=config.FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save embeddings\n",
    "np.savetxt(dataset_name+'_a_S3BERT_embeddings.txt', sentences_a_encoded, fmt='%f')\n",
    "np.savetxt(dataset_name+'_b_S3BERT_embeddings.txt', sentences_b_encoded, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract feature-wise similarities for all sentence pairs\n",
    "features = [\"global\"] + config.FEATURES[2:] + [\"residual\"]\n",
    "feature_sim_storage = []\n",
    "for i, sent_a in enumerate(sentences_a):\n",
    "    similarity = similarities[i]\n",
    "    features_text = {k:v for k,v in zip(features, similarity)}\n",
    "    features_text[\"sent_a\"] = sentences_a[i]\n",
    "    features_text[\"sent_b\"] = sentences_b[i]\n",
    "    feature_sim_storage.append(features_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store feature-wise similarities in a dictionary\n",
    "if pairs==True:\n",
    "    sent_id_pairs_str = [str(id_pair) for id_pair in list(sentences_dict.keys())]\n",
    "else:\n",
    "    sent_id_pairs_str = [str(id_pair) for id_pair in sent_id_pairs]\n",
    "feature_storage_dict = dict(zip(sent_id_pairs_str,feature_sim_storage))\n",
    "with open(dataset_name+'_S3BERT_feature_similarities.json', \"w\") as file:\n",
    "    json.dump(feature_storage_dict, file, indent=0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
